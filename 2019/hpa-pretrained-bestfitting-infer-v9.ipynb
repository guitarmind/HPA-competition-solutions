{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "[V9]\n",
    "* Ensemble pretrained 1024 model in 3 folds\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "debug = True\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/hpa-solution-2019\")\n",
    "    sys.path.insert(0, \"../input/hpa-cell-segmentation\")\n",
    "else:\n",
    "    sys.path.insert(0, \"/workspace/Github/HPA-competition-solutions/2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../input/hpa-solution-2019/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ../input/hpa-solution-2019/bestfitting/densenet121-a639ec97.pth .\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n",
    "# !pip install -q \"../input/hpapytorchzoozip/pytorch_zoo-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on e8e83ee7e5ba\n",
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from bestfitting.src.config.config import *\n",
    "from bestfitting.src.utils.common_util import *\n",
    "from bestfitting.src.networks.imageclsnet import init_network\n",
    "from bestfitting.src.datasets.protein_dataset import ProteinDataset\n",
    "from bestfitting.src.utils.augment_util import *\n",
    "from bestfitting.src.datasets.tool import *\n",
    "\n",
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import base64\n",
    "import zlib\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernel_mode:\n",
    "    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "    pretrained_folder = \"/kaggle/input/hpa-solution-2019\"\n",
    "    # pretrained_folder = \"/kaggle/input/hpa-bestfitting-solution\"\n",
    "    test_image_folder = f\"{dataset_folder}/test/\"\n",
    "    test_resize_folder = \"/kaggle/working/test_resized\"\n",
    "    cell_mask_folder = \"/kaggle/working/test_cell_masks\"\n",
    "    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "    pretrained_folder = \"/workspace/Github/HPA-competition-solutions/2019\"\n",
    "    test_image_folder = f\"{dataset_folder}/test/\"\n",
    "    test_resize_folder = f\"{dataset_folder}/test_resized\"\n",
    "    cell_mask_folder = f\"{dataset_folder}/test_cell_masks\"\n",
    "    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "\n",
    "pretrained_models = {\n",
    "    \"bestfitting_densenet121_512\": {\n",
    "        \"model_path\":\n",
    "        \"bestfitting/external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds/fold0/final.pth\",\n",
    "        \"batch_size\": 512 if kernel_mode else 256,\n",
    "        \"image_size\": 512\n",
    "    },\n",
    "    \"bestfitting_densenet121_1024\": {\n",
    "        \"model_path\":\n",
    "        \"bestfitting/external_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds/fold0/final.pth\",\n",
    "        \"batch_size\": 512 if kernel_mode else 256,\n",
    "        \"image_size\": 1024\n",
    "    },\n",
    "    \"pudae_inception-v3\": {\n",
    "        \"model_path\":\n",
    "        \"pudae/inceptionv3.attention.policy.per_image_norm.1024/checkpoint/swa.10.027.pth\",\n",
    "        \"batch_size\": 512 if kernel_mode else 256,\n",
    "        \"image_size\": 512\n",
    "    },\n",
    "    \"pudae_se-resnext50\": {\n",
    "        \"model_path\":\n",
    "        \"pudae/se_resnext50.attention.per_image_norm.1024/checkpoint/swa.10.022.pth\",\n",
    "        \"batch_size\": 512 if kernel_mode else 256,\n",
    "        \"image_size\": 512\n",
    "    },\n",
    "}\n",
    "\n",
    "num_workers = 2 if kernel_mode else 3\n",
    "\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\t       test_cell_masks\ttrain\r\n",
      "sample_submission.csv  test_resized\ttrain.csv\r\n",
      "test\t\t       test_tfrecords\ttrain_tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "!ls {dataset_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{dataset_folder}/train.csv\")\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>8|5|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>14|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60b57878-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>6|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>16|10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5b931256-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>14|0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID  Label\n",
       "0  5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0  8|5|0\n",
       "1  5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0   14|0\n",
       "2  60b57878-bb99-11e8-b2b9-ac1f6b6435d0    6|1\n",
       "3  5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0  16|10\n",
       "4  5b931256-bb99-11e8-b2b9-ac1f6b6435d0   14|0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 4) 1728 3072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ImageWidth</th>\n",
       "      <th>ImageHeight</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0040581b-f1f2-4fbe-b043-b6bfea5404bb</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004a270d-34a2-4d60-bbe4-365fca868193</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00537262-883c-4b37-a3a1-a4931b6faea5</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c9a1c9-2f06-476f-8b0d-6d01032874a2</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0173029a-161d-40ef-af28-2342915b22fb</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID  ImageWidth  ImageHeight  \\\n",
       "0  0040581b-f1f2-4fbe-b043-b6bfea5404bb        2048         2048   \n",
       "1  004a270d-34a2-4d60-bbe4-365fca868193        2048         2048   \n",
       "2  00537262-883c-4b37-a3a1-a4931b6faea5        2048         2048   \n",
       "3  00c9a1c9-2f06-476f-8b0d-6d01032874a2        2048         2048   \n",
       "4  0173029a-161d-40ef-af28-2342915b22fb        3072         3072   \n",
       "\n",
       "           PredictionString  \n",
       "0  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "1  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "2  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "3  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "4  0 1 eNoLCAgIsAQABJ4Beg==  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submit_df.shape, submit_df.ImageWidth.min(), submit_df.ImageWidth.max())\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n"
     ]
    }
   ],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\"]\n",
    "\n",
    "test_ids = submit_df[\"ID\"].values.tolist()\n",
    "print(len(test_ids))\n",
    "\n",
    "# Estimated number of private test images (RGBY): 2236 x 2.3 ~= 5143 (for 9 hours we have 6.2 secs per image)\n",
    "# Estimated number of private test images: 559 x 2.3 ~= 1286 (for 9 hours we have 25.2 secs per image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernel_mode and len(test_ids) == 559:\n",
    "    debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 17, 9: 17, 10: 17, 11: 8, 12: 9, 13: 9, 14: 10, 15: 18, 16: 18, 17: 11, 18: 18, 19: 12, 20: 17, 21: 13, 22: 18, 23: 14, 24: 15, 25: 16, 26: 17, 27: 18}\n"
     ]
    }
   ],
   "source": [
    "old_classes = {\n",
    "    0: 'Nucleoplasm',\n",
    "    1: 'Nuclear membrane',\n",
    "    2: 'Nucleoli',\n",
    "    3: 'Nucleoli fibrillar center',\n",
    "    4: 'Nuclear speckles',\n",
    "    5: 'Nuclear bodies',\n",
    "    6: 'Endoplasmic reticulum',\n",
    "    7: 'Golgi apparatus',\n",
    "    8: 'Peroxisomes',\n",
    "    9: 'Endosomes',\n",
    "    10: 'Lysosomes',\n",
    "    11: 'Intermediate filaments',\n",
    "    12: 'Actin filaments',\n",
    "    13: 'Focal adhesion sites',\n",
    "    14: 'Microtubules',\n",
    "    15: 'Microtubule ends',\n",
    "    16: 'Cytokinetic bridge',\n",
    "    17: 'Mitotic spindle',\n",
    "    18: 'Microtubule organizing center',\n",
    "    19: 'Centrosome',\n",
    "    20: 'Lipid droplets',\n",
    "    21: 'Plasma membrane',\n",
    "    22: 'Cell junctions',\n",
    "    23: 'Mitochondria',\n",
    "    24: 'Aggresome',\n",
    "    25: 'Cytosol',\n",
    "    26: 'Cytoplasmic bodies',\n",
    "    27: 'Rods & rings'\n",
    "}\n",
    "old_class_indices = {v: k for k, v in old_classes.items()}\n",
    "\n",
    "# All label names in the public HPA and their corresponding index.\n",
    "all_locations = dict({\n",
    "    \"Nucleoplasm\": 0,\n",
    "    \"Nuclear membrane\": 1,\n",
    "    \"Nucleoli\": 2,\n",
    "    \"Nucleoli fibrillar center\": 3,\n",
    "    \"Nuclear speckles\": 4,\n",
    "    \"Nuclear bodies\": 5,\n",
    "    \"Endoplasmic reticulum\": 6,\n",
    "    \"Golgi apparatus\": 7,\n",
    "    \"Intermediate filaments\": 8,\n",
    "    \"Actin filaments\": 9,\n",
    "    \"Focal adhesion sites\": 9,\n",
    "    \"Microtubules\": 10,\n",
    "    \"Mitotic spindle\": 11,\n",
    "    \"Centrosome\": 12,\n",
    "    \"Centriolar satellite\": 12,\n",
    "    \"Plasma membrane\": 13,\n",
    "    \"Cell Junctions\": 13,\n",
    "    \"Mitochondria\": 14,\n",
    "    \"Aggresome\": 15,\n",
    "    \"Cytosol\": 16,\n",
    "    \"Vesicles\": 17,\n",
    "    \"Peroxisomes\": 17,\n",
    "    \"Endosomes\": 17,\n",
    "    \"Lysosomes\": 17,\n",
    "    \"Lipid droplets\": 17,\n",
    "    \"Cytoplasmic bodies\": 17,\n",
    "    \"Rods & rings\": 18,\n",
    "    # markpeng\n",
    "    \"No staining\": 18,\n",
    "})\n",
    "\n",
    "old_class_mappings = {}\n",
    "for i, (k, v) in enumerate(old_class_indices.items()):\n",
    "    if k in all_locations:\n",
    "        old_class_mappings[v] = all_locations[k]\n",
    "    else:\n",
    "        # No staining\n",
    "        old_class_mappings[v] = 18\n",
    "assert len(old_class_mappings.values()) == len(old_classes.values())\n",
    "print(old_class_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook\n",
    "def binary_mask_to_ascii(mask, mask_val=1):\n",
    "    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "    mask = np.where(mask == mask_val, 1, 0).astype(np.bool)\n",
    "\n",
    "    # check input mask --\n",
    "    if mask.dtype != np.bool:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a binary mask, received dtype == {mask.dtype}\"\n",
    "        )\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a 2d mask, received shape == {mask.shape}\"\n",
    "        )\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str.decode()\n",
    "\n",
    "\n",
    "def rle_encoding(img, mask_val=1):\n",
    "    \"\"\"\n",
    "    Turns our masks into RLE encoding to easily store them\n",
    "    and feed them into models later on\n",
    "    https://en.wikipedia.org/wiki/Run-length_encoding\n",
    "    \n",
    "    Args:\n",
    "        img (np.array): Segmentation array\n",
    "        mask_val (int): Which value to use to create the RLE\n",
    "        \n",
    "    Returns:\n",
    "        RLE string\n",
    "    \n",
    "    \"\"\"\n",
    "    dots = np.where(img.T.flatten() == mask_val)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "\n",
    "    return ' '.join([str(x) for x in run_lengths])\n",
    "\n",
    "\n",
    "def rle_to_mask(rle_string, height, width):\n",
    "    \"\"\" Convert RLE sttring into a binary mask \n",
    "    \n",
    "    Args:\n",
    "        rle_string (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of the binary segmentation mask for a given cell\n",
    "    \"\"\"\n",
    "    rows, cols = height, width\n",
    "    rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n",
    "    rle_pairs = np.array(rle_numbers).reshape(-1, 2)\n",
    "    img = np.zeros(rows * cols, dtype=np.uint8)\n",
    "    for index, length in rle_pairs:\n",
    "        index -= 1\n",
    "        img[index:index + length] = 255\n",
    "    img = img.reshape(cols, rows)\n",
    "    img = img.T\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_segmentation_maps(list_of_image_lists, segmentator, batch_size=8):\n",
    "    \"\"\" Function to generate segmentation maps using CellSegmentator tool \n",
    "    \n",
    "    Args:\n",
    "        list_of_image_lists (list of lists):\n",
    "            - [[micro-tubules(red)], [endoplasmic-reticulum(yellow)], [nucleus(blue)]]\n",
    "        batch_size (int): Batch size to use in generating the segmentation masks\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing RLEs for all the cells in all images\n",
    "    \"\"\"\n",
    "\n",
    "    all_mask_rles = {}\n",
    "    for i in tqdm(range(0, len(list_of_image_lists[0]), batch_size),\n",
    "                  total=len(list_of_image_lists[0]) // batch_size):\n",
    "\n",
    "        # Get batch of images\n",
    "        sub_images = [\n",
    "            img_channel_list[i:i + batch_size]\n",
    "            for img_channel_list in list_of_image_lists\n",
    "        ]  # 0.000001 seconds\n",
    "\n",
    "        # Do segmentation\n",
    "        cell_segmentations = segmentator.pred_cells(sub_images)\n",
    "        nuc_segmentations = segmentator.pred_nuclei(sub_images[2])\n",
    "\n",
    "        # post-processing\n",
    "        for j, path in enumerate(sub_images[0]):\n",
    "            img_id = path.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1]\n",
    "            nuc_mask, cell_mask = label_cell(nuc_segmentations[j],\n",
    "                                             cell_segmentations[j])\n",
    "            new_name = os.path.basename(path).replace('red', 'mask')\n",
    "            all_mask_rles[img_id] = [\n",
    "                rle_encoding(cell_mask, mask_val=k)\n",
    "                for k in range(1,\n",
    "                               np.max(cell_mask) + 1)\n",
    "            ]\n",
    "    return all_mask_rles\n",
    "\n",
    "\n",
    "def get_img_list(img_dir, return_ids=False, sub_n=None):\n",
    "    \"\"\" Get image list in the format expected by the CellSegmentator tool \"\"\"\n",
    "    if sub_n is None:\n",
    "        sub_n = len(glob(img_dir + '/' + f'*_red.png'))\n",
    "    if return_ids:\n",
    "        images = [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "        return [\n",
    "            x.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1] for x in images[0]\n",
    "        ], images\n",
    "    else:\n",
    "        return [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_rle(\n",
    "    rle,\n",
    "    width,\n",
    "    height,\n",
    "    return_mask=True,\n",
    "):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        rle (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    mask = rle_to_mask(rle, height, width).copy()\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    x, y, w, h = cv2.boundingRect(cnts[0])\n",
    "\n",
    "    if return_mask:\n",
    "        return (x, y, x + w, y + h), mask\n",
    "    else:\n",
    "        return (x, y, x + w, y + h)\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_raw(raw_mask):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        raw_mask (nparray): Numpy array containing segmentation mask information\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(raw_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    xywhs = [cv2.boundingRect(cnt) for cnt in cnts]\n",
    "    xys = [(xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3])\n",
    "           for xywh in xywhs]\n",
    "    return sorted(xys, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "\n",
    "def pad_to_square(a):\n",
    "    \"\"\" Pad an array `a` evenly until it is a square \"\"\"\n",
    "    if a.shape[1] > a.shape[0]:  # pad height\n",
    "        n_to_add = a.shape[1] - a.shape[0]\n",
    "        top_pad = n_to_add // 2\n",
    "        bottom_pad = n_to_add - top_pad\n",
    "        a = np.pad(a, [(top_pad, bottom_pad), (0, 0), (0, 0)], mode='constant')\n",
    "\n",
    "    elif a.shape[0] > a.shape[1]:  # pad width\n",
    "        n_to_add = a.shape[0] - a.shape[1]\n",
    "        left_pad = n_to_add // 2\n",
    "        right_pad = n_to_add - left_pad\n",
    "        a = np.pad(a, [(0, 0), (left_pad, right_pad), (0, 0)], mode='constant')\n",
    "    else:\n",
    "        pass\n",
    "    return a\n",
    "\n",
    "\n",
    "def cut_out_cells(rgby,\n",
    "                  rles,\n",
    "                  resize_to=(256, 256),\n",
    "                  square_off=True,\n",
    "                  return_masks=False,\n",
    "                  from_raw=True):\n",
    "    \"\"\" Cut out the cells as padded square images \n",
    "    \n",
    "    Args:\n",
    "        rgby (np.array): 4 Channel image to be cut into tiles\n",
    "        rles (list of RLE strings): List of run length encoding containing \n",
    "            segmentation mask information\n",
    "        resize_to (tuple of ints, optional): The square dimension to resize the image to\n",
    "        square_off (bool, optional): Whether to pad the image to a square or not\n",
    "        \n",
    "    Returns:\n",
    "        list of square arrays representing squared off cell images\n",
    "    \"\"\"\n",
    "    w, h = rgby.shape[:2]\n",
    "    contour_bboxes = [\n",
    "        get_contour_bbox(rle, w, h, return_mask=return_masks) for rle in rles\n",
    "    ]\n",
    "    if return_masks:\n",
    "        masks = [x[-1] for x in contour_bboxes]\n",
    "        contour_bboxes = [x[:-1] for x in contour_bboxes]\n",
    "\n",
    "    arrs = [\n",
    "        rgby[bbox[1]:bbox[3], bbox[0]:bbox[2], ...] for bbox in contour_bboxes\n",
    "    ]\n",
    "    if square_off:\n",
    "        arrs = [pad_to_square(arr) for arr in arrs]\n",
    "\n",
    "    if resize_to is not None:\n",
    "        arrs = [\n",
    "            cv2.resize(pad_to_square(arr).astype(np.float32),\n",
    "                       resize_to,\n",
    "                       interpolation=cv2.INTER_CUBIC) \\\n",
    "            for arr in arrs\n",
    "        ]\n",
    "    if return_masks:\n",
    "        return arrs, masks\n",
    "    else:\n",
    "        return arrs\n",
    "\n",
    "\n",
    "def grab_contours(cnts):\n",
    "    # if the length the contours tuple returned by cv2.findContours\n",
    "    # is '2' then we are using either OpenCV v2.4, v4-beta, or\n",
    "    # v4-official\n",
    "    if len(cnts) == 2:\n",
    "        cnts = cnts[0]\n",
    "\n",
    "    # if the length of the contours tuple is '3' then we are using\n",
    "    # either OpenCV v3, v4-pre, or v4-alpha\n",
    "    elif len(cnts) == 3:\n",
    "        cnts = cnts[1]\n",
    "\n",
    "    # otherwise OpenCV has changed their cv2.findContours return\n",
    "    # signature yet again and I have no idea WTH is going on\n",
    "    else:\n",
    "        raise Exception(\n",
    "            (\"Contours tuple must have length 2 or 3, \"\n",
    "             \"otherwise OpenCV changed their cv2.findContours return \"\n",
    "             \"signature yet again. Refer to OpenCV's documentation \"\n",
    "             \"in that case\"))\n",
    "\n",
    "    # return the actual contours array\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/72534\n",
    "def generate_hash(img_dir,\n",
    "                  colors,\n",
    "                  dataset='train',\n",
    "                  imread_func=None,\n",
    "                  is_update=False):\n",
    "    meta = meta.copy()\n",
    "    hash_maps = {}\n",
    "    for color in colors:\n",
    "        hash_maps[color] = []\n",
    "        for idx in tqdm(range(len(meta)), desc='train %s' % color):\n",
    "            img = imread_func(img_dir, meta.iloc[idx][ID], color)\n",
    "            hash = imagehash.phash(img)\n",
    "            hash_maps[color].append(hash)\n",
    "\n",
    "    for color in colors:\n",
    "        meta[color] = hash_maps[color]\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def calc_hash(params):\n",
    "    color, threshold, base_test_hash1, base_test_hash2, test_ids1, test_ids2 = params\n",
    "\n",
    "    test_hash1 = base_test_hash1.reshape(1, -1)  # 1*m\n",
    "\n",
    "    test_idxes_list1 = []\n",
    "    test_idxes_list2 = []\n",
    "    hash_list = []\n",
    "\n",
    "    step = 5\n",
    "    for test_idx in tqdm(range(0, len(base_test_hash2), step), desc=color):\n",
    "        test_hash2 = base_test_hash2[test_idx:test_idx + step].reshape(\n",
    "            -1, 1)  # n*1\n",
    "        hash = test_hash2 - test_hash1  # n*m\n",
    "        test_idxes2, test_idxes1 = np.where(hash <= threshold)\n",
    "        hash = hash[test_idxes2, test_idxes1]\n",
    "\n",
    "        test_idxes2 = test_idxes2 + test_idx\n",
    "\n",
    "        test_idxes_list1.extend(test_idxes1.tolist())\n",
    "        test_idxes_list2.extend(test_idxes2.tolist())\n",
    "        hash_list.extend(hash.tolist())\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Test1': test_ids1[test_idxes_list1],\n",
    "        'Test2': test_ids2[test_idxes_list2],\n",
    "        'Sim%s' % color[:1].upper(): hash_list\n",
    "    })\n",
    "    df = df[df['Test1'] != df['Test2']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize HPA Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hpa_resize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hpa_resize.py\n",
    "\n",
    "kernel_mode = False\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import mlcrate as mlc\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def do_convert(fname_img):\n",
    "    #     img = np.array(Image.open(os.path.join(source_dir, fname_img)),\n",
    "    #                    dtype=np.float32)\n",
    "    #     img = cv2.resize(img, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "    #     cv2.imwrite(os.path.join(dest_dir, fname_img), img)\n",
    "\n",
    "    img = cv2.imread(os.path.join(source_dir, fname_img), cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.imwrite(os.path.join(dest_dir, fname_img), img)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='HPA Image Resize')\n",
    "parser.add_argument('--source', type=str, default=\"./test\", help='source')\n",
    "parser.add_argument('--dest', type=str, default=\"./resized\", help='dest')\n",
    "parser.add_argument('--size', type=int, default=512, help='size')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    size = args.size\n",
    "    source_dir = args.source\n",
    "    dest_dir = f\"{args.dest}/{size}\"\n",
    "    n_cpu = 4\n",
    "#     n_cpu = 2 if kernel_mode else 4\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    fnames = np.sort(os.listdir(source_dir))\n",
    "    #     start_num = max(0, len(os.listdir(dest_dir)) - n_cpu * 2)\n",
    "    #     fnames = np.sort(os.listdir(source_dir))[start_num:]\n",
    "    pool = mlc.SuperPool(n_cpu)\n",
    "    df_list = pool.map(do_convert, fnames, description='Resizing HPA images')\n",
    "\n",
    "    print('\\nsuccess!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !python hpa_resize.py --source {test_image_folder} --dest {test_resize_folder} --size 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !python hpa_resize.py --source {test_image_folder} --dest {test_resize_folder} --size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall time: 51.1 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !du -hs {test_resize_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Extract Cell Segmentations as Numpy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hpa_cell_segment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hpa_cell_segment.py\n",
    "\n",
    "kernel_mode = False\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/hpa-cell-segmentation\")\n",
    "\n",
    "from hpacellseg.cellsegmentator import *\n",
    "\n",
    "target_image_size = 512\n",
    "\n",
    "IMAGE_SIZES = [1728, 2048, 3072, 4096]\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "    # img_dir = f\"{dataset_folder}/test\"\n",
    "    img_dir = \"/kaggle/working/test_resized/512\"\n",
    "    output_folder = \"/kaggle/working/test_cell_masks\"\n",
    "    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "    BATCH_SIZE = {1728: 24, 2048: 22, 3072: 12, 4096: 12}\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "    # img_dir = f\"{dataset_folder}/test\"\n",
    "    img_dir = f\"{dataset_folder}/test_resized/512\"\n",
    "    output_folder = f\"{dataset_folder}/test_cell_masks\"\n",
    "    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "    #     BATCH_SIZE = {1728: 24, 2048: 24, 3072: 12, 4096: 12}\n",
    "    BATCH_SIZE = {1728: 20, 2048: 18, 3072: 8, 4096: 8}\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")\n",
    "\n",
    "predict_df_1728 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[0]]\n",
    "predict_df_2048 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[1]]\n",
    "predict_df_3072 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[2]]\n",
    "predict_df_4096 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[3]]\n",
    "\n",
    "predict_ids_1728 = predict_df_1728.ID.to_list()\n",
    "predict_ids_2048 = predict_df_2048.ID.to_list()\n",
    "predict_ids_3072 = predict_df_3072.ID.to_list()\n",
    "predict_ids_4096 = predict_df_4096.ID.to_list()\n",
    "\n",
    "\n",
    "class CellSegmentator(object):\n",
    "    \"\"\"Uses pretrained DPN-Unet models to segment cells from images.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        nuclei_model=\"../input/hpacellsegmentatormodelweights/dpn_unet_nuclei_v1.pth\",\n",
    "        cell_model=\"../input/hpacellsegmentatormodelweights/dpn_unet_cell_3ch_v1.pth\",\n",
    "        scale_factor=1.0,\n",
    "        device=\"cuda\",\n",
    "        padding=False,\n",
    "        multi_channel_model=True,\n",
    "    ):\n",
    "        \"\"\"Class for segmenting nuclei and whole cells from confocal microscopy images.\n",
    "        It takes lists of images and returns the raw output from the\n",
    "        specified segmentation model. Models can be automatically\n",
    "        downloaded if they are not already available on the system.\n",
    "        When working with images from the Huan Protein Cell atlas, the\n",
    "        outputs from this class' methods are well combined with the\n",
    "        label functions in the utils module.\n",
    "        Note that for cell segmentation, there are two possible models\n",
    "        available. One that works with 2 channeled images and one that\n",
    "        takes 3 channels.\n",
    "        Keyword arguments:\n",
    "        nuclei_model -- A loaded torch nuclei segmentation model or the\n",
    "                        path to a file which contains such a model.\n",
    "                        If the argument is a path that points to a non-existant file,\n",
    "                        a pretrained nuclei_model is going to get downloaded to the\n",
    "                        specified path (default: './nuclei_model.pth').\n",
    "        cell_model -- A loaded torch cell segmentation model or the\n",
    "                      path to a file which contains such a model.\n",
    "                      The cell_model argument can be None if only nuclei\n",
    "                      are to be segmented (default: './cell_model.pth').\n",
    "        scale_factor -- How much to scale images before they are fed to\n",
    "                        segmentation models. Segmentations will be scaled back\n",
    "                        up by 1/scale_factor to match the original image\n",
    "                        (default: 0.25).\n",
    "        device -- The device on which to run the models.\n",
    "                  This should either be 'cpu' or 'cuda' or pointed cuda\n",
    "                  device like 'cuda:0' (default: 'cuda').\n",
    "        padding -- Whether to add padding to the images before feeding the\n",
    "                   images to the network. (default: False).\n",
    "        multi_channel_model -- Control whether to use the 3-channel cell model or not.\n",
    "                               If True, use the 3-channel model, otherwise use the\n",
    "                               2-channel version (default: True).\n",
    "        \"\"\"\n",
    "        if device != \"cuda\" and device != \"cpu\" and \"cuda\" not in device:\n",
    "            raise ValueError(f\"{device} is not a valid device (cuda/cpu)\")\n",
    "        if device != \"cpu\":\n",
    "            try:\n",
    "                assert torch.cuda.is_available()\n",
    "            except AssertionError:\n",
    "                print(\"No GPU found, using CPU.\", file=sys.stderr)\n",
    "                device = \"cpu\"\n",
    "        self.device = device\n",
    "\n",
    "        if isinstance(nuclei_model, str):\n",
    "            if not os.path.exists(nuclei_model):\n",
    "                print(\n",
    "                    f\"Could not find {nuclei_model}. Downloading it now\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                download_with_url(NUCLEI_MODEL_URL, nuclei_model)\n",
    "            nuclei_model = torch.load(nuclei_model,\n",
    "                                      map_location=torch.device(self.device))\n",
    "        if isinstance(nuclei_model, torch.nn.DataParallel) and device == \"cpu\":\n",
    "            nuclei_model = nuclei_model.module\n",
    "\n",
    "        self.nuclei_model = nuclei_model.to(self.device).eval()\n",
    "\n",
    "        self.multi_channel_model = multi_channel_model\n",
    "        if isinstance(cell_model, str):\n",
    "            if not os.path.exists(cell_model):\n",
    "                print(f\"Could not find {cell_model}. Downloading it now\",\n",
    "                      file=sys.stderr)\n",
    "                if self.multi_channel_model:\n",
    "                    download_with_url(MULTI_CHANNEL_CELL_MODEL_URL, cell_model)\n",
    "                else:\n",
    "                    download_with_url(TWO_CHANNEL_CELL_MODEL_URL, cell_model)\n",
    "            cell_model = torch.load(cell_model,\n",
    "                                    map_location=torch.device(self.device))\n",
    "        self.cell_model = cell_model.to(self.device).eval()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.padding = padding\n",
    "\n",
    "    def _image_conversion(self, images):\n",
    "        \"\"\"Convert/Format images to RGB image arrays list for cell predictions.\n",
    "        Intended for internal use only.\n",
    "        Keyword arguments:\n",
    "        images -- list of lists of image paths/arrays. It should following the\n",
    "                 pattern if with er channel input,\n",
    "                 [\n",
    "                     [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                     [er_path0/image_array0, er_path1/image_array1, ...],\n",
    "                     [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                 ]\n",
    "                 or if without er input,\n",
    "                 [\n",
    "                     [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                     None,\n",
    "                     [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                 ]\n",
    "        \"\"\"\n",
    "        microtubule_imgs, er_imgs, nuclei_imgs = images\n",
    "        if self.multi_channel_model:\n",
    "            if not isinstance(er_imgs, list):\n",
    "                raise ValueError(\n",
    "                    \"Please speicify the image path(s) for er channels!\")\n",
    "        else:\n",
    "            if not er_imgs is None:\n",
    "                raise ValueError(\n",
    "                    \"second channel should be None for two channel model predition!\"\n",
    "                )\n",
    "\n",
    "        if not isinstance(microtubule_imgs, list):\n",
    "            raise ValueError(\"The microtubule images should be a list\")\n",
    "        if not isinstance(nuclei_imgs, list):\n",
    "            raise ValueError(\"The microtubule images should be a list\")\n",
    "\n",
    "        if er_imgs:\n",
    "            if not len(microtubule_imgs) == len(er_imgs) == len(nuclei_imgs):\n",
    "                raise ValueError(\n",
    "                    \"The lists of images needs to be the same length\")\n",
    "        else:\n",
    "            if not len(microtubule_imgs) == len(nuclei_imgs):\n",
    "                raise ValueError(\n",
    "                    \"The lists of images needs to be the same length\")\n",
    "\n",
    "        if not all(isinstance(item, np.ndarray) for item in microtubule_imgs):\n",
    "            microtubule_imgs = [\n",
    "                os.path.expanduser(item)\n",
    "                for _, item in enumerate(microtubule_imgs)\n",
    "            ]\n",
    "            nuclei_imgs = [\n",
    "                os.path.expanduser(item) for _, item in enumerate(nuclei_imgs)\n",
    "            ]\n",
    "\n",
    "            microtubule_imgs = list(\n",
    "                map(lambda item: imageio.imread(item), microtubule_imgs))\n",
    "            nuclei_imgs = list(\n",
    "                map(lambda item: imageio.imread(item), nuclei_imgs))\n",
    "            if er_imgs:\n",
    "                er_imgs = [\n",
    "                    os.path.expanduser(item) for _, item in enumerate(er_imgs)\n",
    "                ]\n",
    "                er_imgs = list(map(lambda item: imageio.imread(item), er_imgs))\n",
    "\n",
    "        if not er_imgs:\n",
    "            er_imgs = [\n",
    "                np.zeros(item.shape, dtype=item.dtype)\n",
    "                for _, item in enumerate(microtubule_imgs)\n",
    "            ]\n",
    "        cell_imgs = list(\n",
    "            map(\n",
    "                lambda item: np.dstack((item[0], item[1], item[2])),\n",
    "                list(zip(microtubule_imgs, er_imgs, nuclei_imgs)),\n",
    "            ))\n",
    "\n",
    "        return cell_imgs\n",
    "\n",
    "    def pred_nuclei(self, images, bs=24):\n",
    "        \"\"\"Predict the nuclei segmentation.\n",
    "        Keyword arguments:\n",
    "        images -- A list of image arrays or a list of paths to images.\n",
    "                  If as a list of image arrays, the images could be 2d images\n",
    "                  of nuclei data array only, or must have the nuclei data in\n",
    "                  the blue channel; If as a list of file paths, the images\n",
    "                  could be RGB image files or gray scale nuclei image file\n",
    "                  paths.\n",
    "        Returns:\n",
    "        predictions -- A list of predictions of nuclei segmentation for each nuclei image.\n",
    "        \"\"\"\n",
    "        def _preprocess(image):\n",
    "            if isinstance(image, str):\n",
    "                image = imageio.imread(image)\n",
    "            self.target_shape = image.shape\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.dstack((image, image, image))\n",
    "            image = transform.rescale(image,\n",
    "                                      self.scale_factor,\n",
    "                                      multichannel=True)\n",
    "            nuc_image = np.dstack((image[..., 2], image[..., 2], image[...,\n",
    "                                                                       2]))\n",
    "            if self.padding:\n",
    "                rows, cols = nuc_image.shape[:2]\n",
    "                self.scaled_shape = rows, cols\n",
    "                nuc_image = cv2.copyMakeBorder(\n",
    "                    nuc_image,\n",
    "                    32,\n",
    "                    (32 - rows % 32),\n",
    "                    32,\n",
    "                    (32 - cols % 32),\n",
    "                    cv2.BORDER_REFLECT,\n",
    "                )\n",
    "            nuc_image = nuc_image.transpose([2, 0, 1])\n",
    "            return nuc_image\n",
    "\n",
    "        def _segment_helper(imgs):\n",
    "            with torch.no_grad():\n",
    "                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n",
    "                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n",
    "                imgs = torch.tensor(imgs).float()\n",
    "                imgs = imgs.to(self.device)\n",
    "                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "\n",
    "                imgs = self.nuclei_model(imgs)\n",
    "                imgs = F.softmax(imgs, dim=1)\n",
    "                return imgs\n",
    "\n",
    "        preprocessed_imgs = list(map(_preprocess, images))\n",
    "        predictions = []\n",
    "        for i in range(0, len(preprocessed_imgs), bs):\n",
    "            start = i\n",
    "            end = min(len(preprocessed_imgs), i + bs)\n",
    "            x = preprocessed_imgs[start:end]\n",
    "            pred = _segment_helper(x).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "        predictions = list(np.concatenate(predictions, axis=0))\n",
    "        predictions = map(util.img_as_ubyte, predictions)\n",
    "        predictions = list(map(self._restore_scaling_padding, predictions))\n",
    "        return predictions\n",
    "\n",
    "    def _restore_scaling_padding(self, n_prediction):\n",
    "        \"\"\"Restore an image from scaling and padding.\n",
    "        This method is intended for internal use.\n",
    "        It takes the output from the nuclei model as input.\n",
    "        \"\"\"\n",
    "        n_prediction = n_prediction.transpose([1, 2, 0])\n",
    "        if self.padding:\n",
    "            n_prediction = n_prediction[32:32 + self.scaled_shape[0],\n",
    "                                        32:32 + self.scaled_shape[1], ...]\n",
    "        if not self.scale_factor == 1:\n",
    "            n_prediction[..., 0] = 0\n",
    "            n_prediction = cv2.resize(\n",
    "                n_prediction,\n",
    "                (self.target_shape[0], self.target_shape[1]),\n",
    "                # interpolation=cv2.INTER_AREA,\n",
    "                interpolation=cv2.INTER_NEAREST,\n",
    "            )\n",
    "        return n_prediction\n",
    "\n",
    "    def pred_cells(self, images, precombined=False, bs=24):\n",
    "        \"\"\"Predict the cell segmentation for a list of images.\n",
    "        Keyword arguments:\n",
    "        images -- list of lists of image paths/arrays. It should following the\n",
    "                  pattern if with er channel input,\n",
    "                  [\n",
    "                      [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                      [er_path0/image_array0, er_path1/image_array1, ...],\n",
    "                      [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                  ]\n",
    "                  or if without er input,\n",
    "                  [\n",
    "                      [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                      None,\n",
    "                      [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                  ]\n",
    "                  The ER channel is required when multichannel is True\n",
    "                  and required to be None when multichannel is False.\n",
    "                  The images needs to be of the same size.\n",
    "        precombined -- If precombined is True, the list of images is instead supposed to be\n",
    "                       a list of RGB numpy arrays (default: False).\n",
    "        Returns:\n",
    "        predictions -- a list of predictions of cell segmentations.\n",
    "        \"\"\"\n",
    "        def _preprocess(image):\n",
    "            self.target_shape = image.shape\n",
    "            if not len(image.shape) == 3:\n",
    "                raise ValueError(\"image should has 3 channels\")\n",
    "            cell_image = transform.rescale(image,\n",
    "                                           self.scale_factor,\n",
    "                                           multichannel=True)\n",
    "            if self.padding:\n",
    "                rows, cols = cell_image.shape[:2]\n",
    "                self.scaled_shape = rows, cols\n",
    "                cell_image = cv2.copyMakeBorder(\n",
    "                    cell_image,\n",
    "                    32,\n",
    "                    (32 - rows % 32),\n",
    "                    32,\n",
    "                    (32 - cols % 32),\n",
    "                    cv2.BORDER_REFLECT,\n",
    "                )\n",
    "            cell_image = cell_image.transpose([2, 0, 1])\n",
    "            return cell_image\n",
    "\n",
    "        def _segment_helper(imgs):\n",
    "            with torch.no_grad():\n",
    "                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n",
    "                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n",
    "                imgs = torch.tensor(imgs).float()\n",
    "                imgs = imgs.to(self.device)\n",
    "                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "\n",
    "                imgs = self.cell_model(imgs)\n",
    "                imgs = F.softmax(imgs, dim=1)\n",
    "                return imgs\n",
    "\n",
    "        if not precombined:\n",
    "            images = self._image_conversion(images)\n",
    "        preprocessed_imgs = list(map(_preprocess, images))\n",
    "        predictions = []\n",
    "        for i in range(0, len(preprocessed_imgs), bs):\n",
    "            start = i\n",
    "            end = min(len(preprocessed_imgs), i + bs)\n",
    "            x = preprocessed_imgs[start:end]\n",
    "            pred = _segment_helper(x).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "        predictions = list(np.concatenate(predictions, axis=0))\n",
    "        predictions = map(self._restore_scaling_padding, predictions)\n",
    "        predictions = list(map(util.img_as_ubyte, predictions))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "import os.path\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "from skimage import filters, measure, segmentation\n",
    "from skimage.morphology import (binary_erosion, closing, disk,\n",
    "                                remove_small_holes, remove_small_objects)\n",
    "\n",
    "HIGH_THRESHOLD = 0.4\n",
    "LOW_THRESHOLD = HIGH_THRESHOLD - 0.25\n",
    "\n",
    "\n",
    "def download_with_url(url_string, file_path, unzip=False):\n",
    "    \"\"\"Download file with a link.\"\"\"\n",
    "    with urllib.request.urlopen(url_string) as response, open(\n",
    "            file_path, \"wb\") as out_file:\n",
    "        data = response.read()  # a `bytes` object\n",
    "        out_file.write(data)\n",
    "\n",
    "    if unzip:\n",
    "        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(file_path))\n",
    "\n",
    "\n",
    "def __fill_holes(image):\n",
    "    \"\"\"Fill_holes for labelled image, with a unique number.\"\"\"\n",
    "    boundaries = segmentation.find_boundaries(image)\n",
    "    image = np.multiply(image, np.invert(boundaries))\n",
    "    image = ndi.binary_fill_holes(image > 0)\n",
    "    image = ndi.label(image)[0]\n",
    "    return image\n",
    "\n",
    "\n",
    "def label_cell(nuclei_pred, cell_pred):\n",
    "    \"\"\"Label the cells and the nuclei.\n",
    "    Keyword arguments:\n",
    "    nuclei_pred -- a 3D numpy array of a prediction from a nuclei image.\n",
    "    cell_pred -- a 3D numpy array of a prediction from a cell image.\n",
    "    Returns:\n",
    "    A tuple containing:\n",
    "    nuclei-label -- A nuclei mask data array.\n",
    "    cell-label  -- A cell mask data array.\n",
    "    0's in the data arrays indicate background while a continous\n",
    "    strech of a specific number indicates the area for a specific\n",
    "    cell.\n",
    "    The same value in cell mask and nuclei mask refers to the identical cell.\n",
    "    NOTE: The nuclei labeling from this function will be sligthly\n",
    "    different from the values in :func:`label_nuclei` as this version\n",
    "    will use information from the cell-predictions to make better\n",
    "    estimates.\n",
    "    \"\"\"\n",
    "    def __wsh(\n",
    "        mask_img,\n",
    "        threshold,\n",
    "        border_img,\n",
    "        seeds,\n",
    "        threshold_adjustment=0.35,\n",
    "        small_object_size_cutoff=10,\n",
    "    ):\n",
    "        img_copy = np.copy(mask_img)\n",
    "        m = seeds * border_img  # * dt\n",
    "        img_copy[m <= threshold + threshold_adjustment] = 0\n",
    "        img_copy[m > threshold + threshold_adjustment] = 1\n",
    "        img_copy = img_copy.astype(np.bool)\n",
    "        img_copy = remove_small_objects(\n",
    "            img_copy, small_object_size_cutoff).astype(np.uint8)\n",
    "\n",
    "        mask_img[mask_img <= threshold] = 0\n",
    "        mask_img[mask_img > threshold] = 1\n",
    "        mask_img = mask_img.astype(np.bool)\n",
    "        mask_img = remove_small_holes(mask_img, 63)\n",
    "        mask_img = remove_small_objects(mask_img, 1).astype(np.uint8)\n",
    "        markers = ndi.label(img_copy, output=np.uint32)[0]\n",
    "        labeled_array = segmentation.watershed(mask_img,\n",
    "                                               markers,\n",
    "                                               mask=mask_img,\n",
    "                                               watershed_line=True)\n",
    "        return labeled_array\n",
    "\n",
    "    nuclei_label = __wsh(\n",
    "        nuclei_pred[..., 2] / 255.0,\n",
    "        0.4,\n",
    "        1 - (nuclei_pred[..., 1] + cell_pred[..., 1]) / 255.0 > 0.05,\n",
    "        nuclei_pred[..., 2] / 255,\n",
    "        threshold_adjustment=-0.25,\n",
    "        small_object_size_cutoff=32,\n",
    "    )\n",
    "\n",
    "    # for hpa_image, to remove the small pseduo nuclei\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 157)\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    # this is to remove the cell borders' signal from cell mask.\n",
    "    # could use np.logical_and with some revision, to replace this func.\n",
    "    # Tuned for segmentation hpa images\n",
    "    threshold_value = max(\n",
    "        0.22,\n",
    "        filters.threshold_otsu(cell_pred[..., 2] / 255) * 0.5)\n",
    "    # exclude the green area first\n",
    "    cell_region = np.multiply(\n",
    "        cell_pred[..., 2] / 255 > threshold_value,\n",
    "        np.invert(np.asarray(cell_pred[..., 1] / 255 > 0.05, dtype=np.int8)),\n",
    "    )\n",
    "    sk = np.asarray(cell_region, dtype=np.int8)\n",
    "    distance = np.clip(cell_pred[..., 2], 255 * threshold_value, cell_pred[...,\n",
    "                                                                           2])\n",
    "    cell_label = segmentation.watershed(-distance, nuclei_label, mask=sk)\n",
    "    cell_label = remove_small_objects(cell_label, 344).astype(np.uint8)\n",
    "    selem = disk(2)\n",
    "    cell_label = closing(cell_label, selem)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    # this part is to use green channel, and extend cell label to green channel\n",
    "    # benefit is to exclude cells clear on border but without nucleus\n",
    "    sk = np.asarray(\n",
    "        np.add(\n",
    "            np.asarray(cell_label > 0, dtype=np.int8),\n",
    "            np.asarray(cell_pred[..., 1] / 255 > 0.05, dtype=np.int8),\n",
    "        ) > 0,\n",
    "        dtype=np.int8,\n",
    "    )\n",
    "    cell_label = segmentation.watershed(-distance, cell_label, mask=sk)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    cell_label = np.asarray(cell_label > 0, dtype=np.uint8)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    cell_label = remove_small_objects(cell_label, 344)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    cell_label = np.asarray(cell_label, dtype=np.uint16)\n",
    "    nuclei_label = np.multiply(cell_label > 0, nuclei_label) > 0\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 157)\n",
    "    nuclei_label = np.multiply(cell_label, nuclei_label > 0)\n",
    "\n",
    "    return nuclei_label, cell_label\n",
    "\n",
    "\n",
    "segmentator = CellSegmentator(NUC_MODEL,\n",
    "                              CELL_MODEL,\n",
    "                              device=\"cuda\",\n",
    "                              padding=True)\n",
    "# segmentator = CellSegmentator(\n",
    "#     NUC_MODEL,\n",
    "#     CELL_MODEL,\n",
    "#     scale_factor=0.25,\n",
    "#     device=\"cuda\",\n",
    "#     padding=True,\n",
    "#     multi_channel_model=True,\n",
    "# )\n",
    "\n",
    "\n",
    "def get_segment_mask(gray, rgb, bs=24):\n",
    "    nuc_segmentations = segmentator.pred_nuclei(gray, bs=bs)  # blue\n",
    "    cell_segmentations = segmentator.pred_cells(rgb, precombined=True, bs=bs)\n",
    "    batch_cell_masks = [\n",
    "        label_cell(nuc_seg, cell_seg)[1].astype(np.uint8)\n",
    "        for nuc_seg, cell_seg in zip(nuc_segmentations, cell_segmentations)\n",
    "    ]\n",
    "    return batch_cell_masks\n",
    "\n",
    "\n",
    "def load_images(image_ids, root=img_dir):\n",
    "    gray = []\n",
    "    rgb = []\n",
    "    for ID in tqdm(image_ids, total=len(image_ids)):\n",
    "        r = os.path.join(root, f'{ID}_red.png')\n",
    "        y = os.path.join(root, f'{ID}_yellow.png')\n",
    "        b = os.path.join(root, f'{ID}_blue.png')\n",
    "        r = cv2.imread(r, 0)\n",
    "        y = cv2.imread(y, 0)\n",
    "        b = cv2.imread(b, 0)\n",
    "        #         gray_image = cv2.resize(b, (target_image_size, target_image_size))\n",
    "        #         rgb_image = cv2.resize(np.stack((r, y, b), axis=2),\n",
    "        #                                (target_image_size, target_image_size))\n",
    "        gray_image = b\n",
    "        rgb_image = np.stack((r, y, b), axis=2)\n",
    "        gray.append(gray_image)\n",
    "        rgb.append(rgb_image)\n",
    "    return gray, rgb\n",
    "\n",
    "\n",
    "for size_idx, submission_ids in tqdm(enumerate(\n",
    "    [predict_ids_1728, predict_ids_2048, predict_ids_3072, predict_ids_4096]),\n",
    "                                     total=4):\n",
    "    size = IMAGE_SIZES[size_idx]\n",
    "    if submission_ids == []:\n",
    "        print(f\"\\n...SKIPPING SIZE {size} AS THERE ARE NO IMAGE IDS ...\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"\\n...WORKING ON IMAGE IDS FOR SIZE {size} ...\\n\")\n",
    "    for i in tqdm(range(0, len(submission_ids), BATCH_SIZE[size]),\n",
    "                  total=int(np.ceil(len(submission_ids) / BATCH_SIZE[size]))):\n",
    "\n",
    "        r, y, b = [], [], []\n",
    "        image_ids = submission_ids[i:(i + BATCH_SIZE[size])]\n",
    "        gray, rgb = load_images(image_ids)\n",
    "        batch_cell_masks = get_segment_mask(gray, rgb, bs=BATCH_SIZE[size])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        for index, img_id in enumerate(image_ids):\n",
    "            np.save(os.path.join(output_folder, f'{img_id}_cell_mask.npy'),\n",
    "                    batch_cell_masks[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !python hpa_cell_segment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Wall time: 3min 52s (using resized test images as input)\n",
    "# Wall time: 4min 45s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Std From Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mean = [0.081018, 0.052349, 0.054012, 0.08106] # rgby\n",
    "total_std = [0.133235, 0.08948, 0.143813, 0.130265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Bestfitting Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_bestfitting_densenet121(model_name,\n",
    "                                 network_path,\n",
    "                                 image_size=512,\n",
    "                                 print_model=False):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='PyTorch Protein Classification')\n",
    "    parser.add_argument(\n",
    "        '--out_dir',\n",
    "        type=str,\n",
    "        help='destination where predicted result should be saved')\n",
    "    parser.add_argument('--gpu_id',\n",
    "                        default='0',\n",
    "                        type=str,\n",
    "                        help='gpu id used for predicting (default: 0)')\n",
    "    parser.add_argument(\n",
    "        '--arch',\n",
    "        default='class_densenet121_dropout',\n",
    "        type=str,\n",
    "        help='model architecture (default: class_densenet121_dropout)')\n",
    "    parser.add_argument('--num_classes',\n",
    "                        default=28,\n",
    "                        type=int,\n",
    "                        help='number of classes (default: 28)')\n",
    "    parser.add_argument('--in_channels',\n",
    "                        default=4,\n",
    "                        type=int,\n",
    "                        help='in channels (default: 4)')\n",
    "    parser.add_argument('--img_size',\n",
    "                        default=768,\n",
    "                        type=int,\n",
    "                        help='image size (default: 768)')\n",
    "    parser.add_argument('--crop_size',\n",
    "                        default=512,\n",
    "                        type=int,\n",
    "                        help='crop size (default: 512)')\n",
    "    parser.add_argument('--batch_size',\n",
    "                        default=32,\n",
    "                        type=int,\n",
    "                        help='train mini-batch size (default: 32)')\n",
    "    parser.add_argument('--workers',\n",
    "                        default=3,\n",
    "                        type=int,\n",
    "                        help='number of data loading workers (default: 3)')\n",
    "    parser.add_argument('--fold',\n",
    "                        default=0,\n",
    "                        type=int,\n",
    "                        help='index of fold (default: 0)')\n",
    "    parser.add_argument('--augment',\n",
    "                        default='default',\n",
    "                        type=str,\n",
    "                        help='test augmentation (default: default)')\n",
    "    parser.add_argument('--seed',\n",
    "                        default=100,\n",
    "                        type=int,\n",
    "                        help='random seed (default: 100)')\n",
    "    parser.add_argument('--seeds', default=None, type=str, help='predict seed')\n",
    "    parser.add_argument('--predict_epoch',\n",
    "                        default=None,\n",
    "                        type=int,\n",
    "                        help='number epoch to predict')\n",
    "    args = parser.parse_args([\n",
    "        \"--arch\",\n",
    "        \"class_densenet121_dropout\",\n",
    "        #     \"--img_size\", str(image_size),\n",
    "        \"--crop_size\",\n",
    "        str(image_size),\n",
    "    ])\n",
    "\n",
    "    # setting up the visible GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = args.arch\n",
    "    model_params['num_classes'] = args.num_classes\n",
    "    model_params['in_channels'] = args.in_channels\n",
    "    model_params['pretrained_path'] = f\"{pretrained_folder}/bestfitting\"\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    checkpoint = torch.load(network_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # moving network to gpu and eval mode\n",
    "    # model = DataParallel(model)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    if print_model:\n",
    "        print(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pudae Inception-V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pudae_inception_v3(model_name, network_path, image_size=512, print_model=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pudae SE-ResNext50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pudae_se_resnext50(model_name, network_path, image_size=512, print_model=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images to Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crop_img(img):\n",
    "    random_crop_size = int(np.random.uniform(self.crop_size, self.img_size))\n",
    "    x = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "    y = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "    crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n",
    "    return crop_img\n",
    "\n",
    "\n",
    "def read_rgby(\n",
    "    img_dir,\n",
    "    img_id,\n",
    "    random_crop=False,\n",
    "):\n",
    "    suffix = '.png'\n",
    "    colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    rgby_img = [\n",
    "        cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n",
    "        for color in colors\n",
    "    ]\n",
    "    rgby_img = np.stack(rgby_img, axis=-1)\n",
    "    if random_crop and crop_size > 0:\n",
    "        rgby_img = read_crop_img(rgby_img)\n",
    "\n",
    "    return rgby_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_id, img_dir):\n",
    "    if img_id not in global_cache:\n",
    "        rgby_img = read_rgby(img_dir, img_id)\n",
    "        if rgby_img[0] is None:\n",
    "            print(img_dir, img_id)\n",
    "\n",
    "        h, w = rgby_img.shape[:2]\n",
    "\n",
    "        #         resized_rgby_img = rgby_img\n",
    "        #         if crop_size > 0:\n",
    "        #             if crop_size != h or crop_size != w:\n",
    "        resized_rgby_img = cv2.resize(rgby_img, (1024, 1024),\n",
    "                                      interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        full_mask = np.load(f\"{cell_mask_folder}/{img_id}_cell_mask.npy\")\n",
    "\n",
    "        #         full_mask = cv2.resize(full_mask, (crop_size, crop_size),\n",
    "        #                                interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        cell_masks = [\n",
    "            rle_encoding(full_mask, mask_val=k)\n",
    "            for k in range(1,\n",
    "                           np.max(full_mask) + 1)\n",
    "        ]\n",
    "        if len(cell_masks) == 0:\n",
    "            print(f\"No cell masks found for {img_id}\")\n",
    "\n",
    "        resized_rgby_img = resized_rgby_img / 255.0\n",
    "\n",
    "        global_cache[img_id] = (resized_rgby_img, cell_masks)\n",
    "        return resized_rgby_img, cell_masks\n",
    "    else:\n",
    "        # print(f\"Cache hit for {img_id}!\")\n",
    "        return global_cache[img_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cells_fn(x, image_size=512):\n",
    "    images = []\n",
    "    # For each full image, extract cell images\n",
    "    image = x[0]\n",
    "    masks = x[1]\n",
    "    transform = x[2]\n",
    "\n",
    "    for rle_string in masks:\n",
    "        cell_mask = rle_to_mask(rle_string, image_size, image_size)\n",
    "        # Important: set 255 to 1\n",
    "        cell_mask[cell_mask > 0] = 1\n",
    "\n",
    "        cell_image = np.copy(image)\n",
    "        #         cell_mask = np.tile(cell_mask, reps=(4, 1, 1)).transpose(1, 2, 0)\n",
    "        #         cell_image *= cell_mask\n",
    "        for i in range(4):\n",
    "            cell_image[..., i] = cell_image[..., i] * cell_mask\n",
    "\n",
    "        if transform is not None:\n",
    "            cell_image = transform(cell_image)\n",
    "\n",
    "        cell_image = image_to_tensor(cell_image)\n",
    "        images.append(cell_image.unsqueeze(0))\n",
    "\n",
    "    images = torch.cat(images)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, model, batch_size=512, image_size=512, generate_meta=False):\n",
    "    all_probs = []\n",
    "    all_meta = {}\n",
    "\n",
    "    global_processed = 0\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        id = row[\"ID\"]\n",
    "        width = row[\"ImageWidth\"]\n",
    "        height = row[\"ImageHeight\"]\n",
    "\n",
    "        # Data augmentation\n",
    "        aug_probs = None\n",
    "        for augment in augment_list:\n",
    "            transform = eval(f\"augment_{augment}\")\n",
    "            images, masks = process_image(id, f\"{test_resize_folder}/512\")\n",
    "            # images, masks = process_image(id, test_image_folder)\n",
    "            images, masks = collate_cells_fn((images, masks, transform),\n",
    "                                             image_size)\n",
    "\n",
    "            cell_probs = []\n",
    "            processed_count = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_i in range(0, len(masks), batch_size):\n",
    "                    batch_images = images[batch_i:batch_i + batch_size, ...]\n",
    "                    # batch_images = Variable(batch_images.cuda(), volatile=True)\n",
    "                    outputs = model(batch_images.cuda())\n",
    "                    logits = outputs\n",
    "\n",
    "                    probs = F.sigmoid(logits).data\n",
    "                    probs = probs.detach().cpu().numpy().tolist()\n",
    "                    cell_probs += probs\n",
    "\n",
    "                    processed_count += len(probs)\n",
    "\n",
    "            cell_probs = np.array(cell_probs).reshape(processed_count, -1)\n",
    "\n",
    "            # Take maximum probability\n",
    "            if aug_probs is None:\n",
    "                aug_probs = cell_probs\n",
    "            else:\n",
    "                aug_probs = np.max(np.stack([aug_probs, cell_probs], axis=-1),\n",
    "                                   axis=-1)\n",
    "\n",
    "        all_probs.append(aug_probs)\n",
    "\n",
    "        if generate_meta:\n",
    "            masks = np.array(masks)\n",
    "\n",
    "            if masks.shape[0] > 0:\n",
    "                # Generate RLE string for each cell mask\n",
    "                # https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook?scriptVersionId=55714434\n",
    "                submit_strings = []\n",
    "                for i in range(masks.shape[0]):\n",
    "                    mask = masks[i]\n",
    "                    mask = rle_to_mask(mask, image_size, image_size)\n",
    "                    # Important: set 255 to 1\n",
    "                    mask[mask > 0] = 1\n",
    "\n",
    "                    # Important: resize to orignal resolution to submit correct mask RLE string\n",
    "                    # https://www.kaggle.com/linshokaku/faster-hpa-cell-segmentation/comments#1251082\n",
    "                    mask = cv2.resize(mask, (width, height),\n",
    "                                      interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    rle_string = binary_mask_to_ascii(mask, mask_val=1)\n",
    "                    submit_strings.append(rle_string)\n",
    "\n",
    "                if len(submit_strings) > 0:\n",
    "                    all_meta[id] = submit_strings\n",
    "                else:\n",
    "                    all_meta[id] = []\n",
    "            else:\n",
    "                all_meta[id] = []\n",
    "\n",
    "        del images, masks, aug_probs, cell_probs, batch_images\n",
    "        gc.collect()\n",
    "\n",
    "        if debug and global_processed == batch_size - 1:\n",
    "            break\n",
    "\n",
    "        global_processed += 1\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if generate_meta:\n",
    "        return all_probs, all_meta\n",
    "    else:\n",
    "        return all_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch Processing 0]\n",
      "(4, 4)\n",
      "[Fold 0]\n",
      "[Model: bestfitting_densenet121_1024]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:28<00:09,  9.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "[Fold 1]\n",
      "[Model: bestfitting_densenet121_1024]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:28<00:09,  9.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "[Fold 2]\n",
      "[Model: bestfitting_densenet121_1024]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:28<00:09,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "CPU times: user 1min 38s, sys: 17.3 s, total: 1min 55s\n",
      "Wall time: 1min 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_probs = []\n",
    "final_meta = {}\n",
    "\n",
    "# augment_list = ['default', 'flipud', 'fliplr']\n",
    "augment_list = ['default', 'flipud', 'fliplr', 'transpose', 'flipud_lr']\n",
    "# augment_list = [\n",
    "#     'default', 'flipud', 'fliplr', 'transpose', 'flipud_lr',\n",
    "#     'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose'\n",
    "# ]\n",
    "\n",
    "model_list = [\n",
    "    #     \"bestfitting_densenet121_512\",\n",
    "    \"bestfitting_densenet121_1024\",\n",
    "    #     \"pudae_inception-v3\",\n",
    "    #     \"pudae_se-resnext50\",\n",
    "]\n",
    "seeds = [0]\n",
    "# seeds = [0, 1, 2, 3]\n",
    "\n",
    "kfolds = 3\n",
    "\n",
    "cache_size = 4 if debug else 250\n",
    "global_cache = {}\n",
    "batch_rounds = 0\n",
    "for batch_i in range(0, submit_df.shape[0], cache_size):\n",
    "    print(f\"[Batch Processing {batch_rounds}]\")\n",
    "    sub_df = submit_df.iloc[batch_i:batch_i + cache_size, :].copy()\n",
    "    print(sub_df.shape)\n",
    "\n",
    "    batch_probs = [0] * sub_df.shape[0]\n",
    "    for model_index, model_name in enumerate(model_list):\n",
    "        for fold in range(kfolds):\n",
    "            print(f\"[Fold {fold}]\")\n",
    "\n",
    "            print(f\"[Model: {model_name}]\")\n",
    "            network_path = f\"{pretrained_folder}/{pretrained_models[model_name]['model_path']}\"\n",
    "            network_path = network_path.replace(\"fold0\", f\"fold{fold}\")\n",
    "\n",
    "            image_size = pretrained_models[model_name]['image_size']\n",
    "            batch_size = 4 if debug else pretrained_models[model_name][\n",
    "                'batch_size']\n",
    "\n",
    "            if model_name in [\n",
    "                    \"bestfitting_densenet121_512\",\n",
    "                    \"bestfitting_densenet121_1024\"\n",
    "            ]:\n",
    "                model = load_bestfitting_densenet121(model_name, network_path,\n",
    "                                                     image_size)\n",
    "            elif model_name == \"pudae_inception\":\n",
    "                model = load_pudae_inception_v3(model_name, network_path,\n",
    "                                                image_size)\n",
    "            elif model_name == \"resnext50\":\n",
    "                model = load_pudae_se_resnext50(model_name, network_path,\n",
    "                                                image_size)\n",
    "\n",
    "            for i, seed in enumerate(seeds):\n",
    "                print(f\"Inferencing with seed {rand_seed+1000*seed} ......\")\n",
    "                seed_everything(rand_seed + 1000 * seed)\n",
    "\n",
    "                if model_index == 0 and i == 0:\n",
    "                    seed_probs, meta = predict(sub_df,\n",
    "                                               model,\n",
    "                                               batch_size=batch_size,\n",
    "                                               image_size=image_size,\n",
    "                                               generate_meta=True)\n",
    "                    print(len(seed_probs), len(batch_probs), sub_df.shape[0])\n",
    "                    for j in range(len(seed_probs)):\n",
    "                        batch_probs[j] = seed_probs[j] / (\n",
    "                            len(seeds) * len(model_list) * kfolds)\n",
    "                    final_meta.update(meta)\n",
    "                else:\n",
    "                    seed_probs = predict(sub_df,\n",
    "                                         model,\n",
    "                                         batch_size=batch_size,\n",
    "                                         image_size=image_size)\n",
    "                    print(len(seed_probs), len(batch_probs), sub_df.shape[0])\n",
    "                    for j in range(len(seed_probs)):\n",
    "                        batch_probs[j] += seed_probs[j] / (\n",
    "                            len(seeds) * len(model_list) * kfolds)\n",
    "\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    if batch_probs is not None:\n",
    "        final_probs.extend(batch_probs)\n",
    "\n",
    "    if debug:\n",
    "        break\n",
    "\n",
    "    # Reset cache\n",
    "    del global_cache\n",
    "    gc.collect()\n",
    "    global_cache = {}\n",
    "    batch_rounds += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/559 [00:00<00:00, 913.79it/s]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "for index, row in tqdm(submit_df.iterrows(), total=submit_df.shape[0]):\n",
    "    id = row[\"ID\"]\n",
    "    width = row[\"ImageWidth\"]\n",
    "    height = row[\"ImageHeight\"]\n",
    "\n",
    "    cell_probs = final_probs[index]\n",
    "    rle_strings = final_meta[id]\n",
    "\n",
    "    new_preds = np.zeros((cell_probs.shape[0], 19))\n",
    "    for i in range(cell_probs.shape[0]):\n",
    "        for j in range(28):\n",
    "            new_class_i = old_class_mappings[j]\n",
    "            # Take maximum prob.\n",
    "            if cell_probs[i, j] > new_preds[i, new_class_i]:\n",
    "                new_preds[i, new_class_i] = cell_probs[i, j]\n",
    "\n",
    "    submit_strings = []\n",
    "    for i in range(new_preds.shape[0]):\n",
    "        confidence = new_preds[i, ...]\n",
    "        rle_string = rle_strings[i]\n",
    "        for l in range(19):\n",
    "            submit_strings.append(f\"{l} {confidence[l]:.6f} {rle_string}\")\n",
    "\n",
    "    if len(submit_strings) > 0:\n",
    "        all_predictions.append(\" \".join(submit_strings))\n",
    "    else:\n",
    "        all_predictions.append(\"\")\n",
    "\n",
    "    if debug and index == batch_size - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ImageWidth</th>\n",
       "      <th>ImageHeight</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0040581b-f1f2-4fbe-b043-b6bfea5404bb</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.126614 eNqFUsFuwyAM/SXHZRx3rhAYFKKqqiKGsin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004a270d-34a2-4d60-bbe4-365fca868193</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.144963 eNqFkEEKgEAIRa80mUhHCBFxMYvoALXs/rs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00537262-883c-4b37-a3a1-a4931b6faea5</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.131212 eNqFj0ELwjAMhf/SM4ZQZAwZQ0oIqQdBEJE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c9a1c9-2f06-476f-8b0d-6d01032874a2</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.175907 eNp9UU1vgzAM/UuvJoo49DxVkWWmME1oBzS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0173029a-161d-40ef-af28-2342915b22fb</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>fea47298-266a-4cf4-93bd-55d1bcc2fc7d</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>0 1 eNoLCAjJNgIABNkBkg==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>feb955db-6c07-4717-a98b-92236c8e01d8</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>fefb9bb7-934a-40d1-8d2f-210265857388</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>ff069fa2-d948-408e-91b3-034cfea428d1</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>ff23eea9-4bbe-42af-a8da-9ae16321fc6d</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>559 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ID  ImageWidth  ImageHeight  \\\n",
       "0    0040581b-f1f2-4fbe-b043-b6bfea5404bb        2048         2048   \n",
       "1    004a270d-34a2-4d60-bbe4-365fca868193        2048         2048   \n",
       "2    00537262-883c-4b37-a3a1-a4931b6faea5        2048         2048   \n",
       "3    00c9a1c9-2f06-476f-8b0d-6d01032874a2        2048         2048   \n",
       "4    0173029a-161d-40ef-af28-2342915b22fb        3072         3072   \n",
       "..                                    ...         ...          ...   \n",
       "554  fea47298-266a-4cf4-93bd-55d1bcc2fc7d        1728         1728   \n",
       "555  feb955db-6c07-4717-a98b-92236c8e01d8        2048         2048   \n",
       "556  fefb9bb7-934a-40d1-8d2f-210265857388        2048         2048   \n",
       "557  ff069fa2-d948-408e-91b3-034cfea428d1        3072         3072   \n",
       "558  ff23eea9-4bbe-42af-a8da-9ae16321fc6d        2048         2048   \n",
       "\n",
       "                                      PredictionString  \n",
       "0    0 0.126614 eNqFUsFuwyAM/SXHZRx3rhAYFKKqqiKGsin...  \n",
       "1    0 0.144963 eNqFkEEKgEAIRa80mUhHCBFxMYvoALXs/rs...  \n",
       "2    0 0.131212 eNqFj0ELwjAMhf/SM4ZQZAwZQ0oIqQdBEJE...  \n",
       "3    0 0.175907 eNp9UU1vgzAM/UuvJoo49DxVkWWmME1oBzS...  \n",
       "4                             0 1 eNoLCAgIsAQABJ4Beg==  \n",
       "..                                                 ...  \n",
       "554                           0 1 eNoLCAjJNgIABNkBkg==  \n",
       "555                           0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "556                           0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "557                           0 1 eNoLCAgIsAQABJ4Beg==  \n",
       "558                           0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "\n",
       "[559 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug:\n",
    "    submit_df.iloc[:batch_size, :][\"PredictionString\"] = all_predictions\n",
    "else:\n",
    "    submit_df[\"PredictionString\"] = all_predictions\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_result(probs, img_ids, th=0.5):\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    predicted_probs = probs.copy()\n",
    "    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n",
    "\n",
    "    pred_list = []\n",
    "    pred_list_new = []\n",
    "    for line in probs:\n",
    "        # Map old classes to new ones\n",
    "        predicted_old_classes = sorted(\n",
    "            list(set([i for i in np.nonzero(line > th)[0]])))\n",
    "        predicted_new_classes = sorted(\n",
    "            list(set([old_class_mappings[i]\n",
    "                      for i in np.nonzero(line > th)[0]])))\n",
    "        # print(predicted_classes)\n",
    "        s = '|'.join([str(i) for i in predicted_old_classes])\n",
    "        s_new = '|'.join([str(i) for i in predicted_new_classes])\n",
    "        pred_list.append(s)\n",
    "        pred_list_new.append(s_new)\n",
    "    result_df = pd.DataFrame({\n",
    "        # \"ID\": img_ids,\n",
    "        \"Predicted\": pred_list,\n",
    "        \"Predicted_New\": pred_list_new\n",
    "    })\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Predicted_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Predicted Predicted_New\n",
       "0         7             7\n",
       "1        25            16\n",
       "2        25            16\n",
       "3        25            16\n",
       "4         0             0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = prob_to_result(final_probs, None, th=confidence_threshold)\n",
    "result_df.to_csv(\"result_comparison.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Predicted_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Predicted_New\n",
       "0          7             7\n",
       "1         25            16\n",
       "2         25            16\n",
       "3         25            16\n",
       "4          0             0\n",
       "5         25            16\n",
       "6         25            16\n",
       "7         25            16\n",
       "8         25            16\n",
       "9          0             0\n",
       "10        25            16\n",
       "11        25            16\n",
       "12        25            16\n",
       "13        25            16\n",
       "14        25            16\n",
       "15        25            16\n",
       "16        25            16\n",
       "17        25            16\n",
       "18        25            16\n",
       "19        25            16\n",
       "20        25            16\n",
       "21        25            16\n",
       "22        25            16\n",
       "23        25            16\n",
       "24        25            16\n",
       "25        25            16\n",
       "26        25            16\n",
       "27        25            16\n",
       "28        25            16\n",
       "29        23            14\n",
       "30        25            16\n",
       "31        23            14\n",
       "32        23            14\n",
       "33        25            16\n",
       "34         0             0\n",
       "35        25            16\n",
       "36        23            14\n",
       "37        25            16\n",
       "38        23            14\n",
       "39        25            16\n",
       "40        25            16\n",
       "41        23            14\n",
       "42        25            16\n",
       "43         0             0\n",
       "44        23            14\n",
       "45        25            16\n",
       "46        23            14\n",
       "47        23            14\n",
       "48        23            14\n",
       "49        23            14\n",
       "50        23            14\n",
       "51        23            14\n",
       "52        25            16\n",
       "53        25            16\n",
       "54        23            14\n",
       "55        23            14\n",
       "56        23            14\n",
       "57        23            14\n",
       "58        23            14\n",
       "59        23            14\n",
       "60         7             7\n",
       "61        23            14\n",
       "62        25            16\n",
       "63        25            16\n",
       "64         0             0\n",
       "65         7             7\n",
       "66        25            16\n",
       "67        25            16\n",
       "68        11             8\n",
       "69        25            16\n",
       "70        25            16\n",
       "71        25            16\n",
       "72        25            16\n",
       "73        25            16\n",
       "74        25            16\n",
       "75        25            16\n",
       "76        25            16\n",
       "77        25            16\n",
       "78        25            16\n",
       "79         7             7\n",
       "80        25            16\n",
       "81        25            16\n",
       "82         0             0\n",
       "83         0             0\n",
       "84        25            16\n",
       "85        25            16"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16    54\n",
       "14    20\n",
       "0      7\n",
       "7      4\n",
       "8      1\n",
       "Name: Predicted_New, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"Predicted_New\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset.release_gpu()\n",
    "# del model, test_dataset, test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm densenet121-a639ec97.pth hpa_resize.py hpa_cell_segment.py\n",
    "# !rm -rf inference test_cell_masks test_resized\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

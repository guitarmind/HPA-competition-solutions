{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[V1]\n",
    "* Resolution: Resized to 512x512 from 768x768\n",
    "* Extract cell masks and create individual cell images (512x512)\n",
    "* No random crop\n",
    "* No TTA\n",
    "* Update normalization mean and std with 2021 training and test sets\n",
    "\n",
    "[V2]\n",
    "* Use INTER_AREA for resize\n",
    "\n",
    "[V3]\n",
    "* Refactor to use less memory\n",
    "\n",
    "[V4]\n",
    "* Run batch cell segmentator from script\n",
    "\n",
    "[V5]\n",
    "* Add data augmentation\n",
    "\n",
    "\n",
    "Note: HPA-Cell-Segmentatior assume that all input images are of the same shape!\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "debug = True\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/hpa-bestfitting-solution/src\")\n",
    "    sys.path.insert(0, \"../input/hpa-cell-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../input/hpa-bestfitting-solution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ../input/hpa-bestfitting-solution/densenet121-a639ec97.pth .\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n",
    "# !pip install -q \"../input/hpapytorchzoozip/pytorch_zoo-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from config.config import *\n",
    "from utils.common_util import *\n",
    "from networks.imageclsnet import init_network\n",
    "from datasets.protein_dataset import ProteinDataset\n",
    "from utils.augment_util import *\n",
    "from datasets.tool import *\n",
    "\n",
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import base64\n",
    "import zlib\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernel_mode:\n",
    "    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "    bestfitting_folder = \"/kaggle/input/hpa-bestfitting-solution\"\n",
    "    test_image_folder = f\"{dataset_folder}/test/\"\n",
    "    cell_mask_folder = \"/kaggle/working/test_cell_masks\"\n",
    "    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "    bestfitting_folder = \"/workspace/Github/HPA-competition-solutions/bestfitting\"\n",
    "    test_image_folder = f\"{dataset_folder}/test/\"\n",
    "    cell_mask_folder = f\"{dataset_folder}/test_cell_masks\"\n",
    "    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "\n",
    "model_folder = \"external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds\"\n",
    "\n",
    "# image_size = 2048\n",
    "# image_size = 768\n",
    "crop_size = 512\n",
    "\n",
    "batch_size = 512 if kernel_mode else 256\n",
    "# batch_size = 6 if kernel_mode else 4\n",
    "# batch_size = 4\n",
    "num_workers = 2 if kernel_mode else 3\n",
    "\n",
    "# scale_factor = 1.0\n",
    "# scale_factor = 0.1\n",
    "scale_factor = 0.25\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_classes = {\n",
    "    0: 'Nucleoplasm',\n",
    "    1: 'Nuclear membrane',\n",
    "    2: 'Nucleoli',\n",
    "    3: 'Nucleoli fibrillar center',\n",
    "    4: 'Nuclear speckles',\n",
    "    5: 'Nuclear bodies',\n",
    "    6: 'Endoplasmic reticulum',\n",
    "    7: 'Golgi apparatus',\n",
    "    8: 'Peroxisomes',\n",
    "    9: 'Endosomes',\n",
    "    10: 'Lysosomes',\n",
    "    11: 'Intermediate filaments',\n",
    "    12: 'Actin filaments',\n",
    "    13: 'Focal adhesion sites',\n",
    "    14: 'Microtubules',\n",
    "    15: 'Microtubule ends',\n",
    "    16: 'Cytokinetic bridge',\n",
    "    17: 'Mitotic spindle',\n",
    "    18: 'Microtubule organizing center',\n",
    "    19: 'Centrosome',\n",
    "    20: 'Lipid droplets',\n",
    "    21: 'Plasma membrane',\n",
    "    22: 'Cell junctions',\n",
    "    23: 'Mitochondria',\n",
    "    24: 'Aggresome',\n",
    "    25: 'Cytosol',\n",
    "    26: 'Cytoplasmic bodies',\n",
    "    27: 'Rods & rings'\n",
    "}\n",
    "old_class_indices = {v: k for k, v in old_classes.items()}\n",
    "\n",
    "# All label names in the public HPA and their corresponding index.\n",
    "all_locations = dict({\n",
    "    \"Nucleoplasm\": 0,\n",
    "    \"Nuclear membrane\": 1,\n",
    "    \"Nucleoli\": 2,\n",
    "    \"Nucleoli fibrillar center\": 3,\n",
    "    \"Nuclear speckles\": 4,\n",
    "    \"Nuclear bodies\": 5,\n",
    "    \"Endoplasmic reticulum\": 6,\n",
    "    \"Golgi apparatus\": 7,\n",
    "    \"Intermediate filaments\": 8,\n",
    "    \"Actin filaments\": 9,\n",
    "    \"Focal adhesion sites\": 9,\n",
    "    \"Microtubules\": 10,\n",
    "    \"Mitotic spindle\": 11,\n",
    "    \"Centrosome\": 12,\n",
    "    \"Centriolar satellite\": 12,\n",
    "    \"Plasma membrane\": 13,\n",
    "    \"Cell Junctions\": 13,\n",
    "    \"Mitochondria\": 14,\n",
    "    \"Aggresome\": 15,\n",
    "    \"Cytosol\": 16,\n",
    "    \"Vesicles\": 17,\n",
    "    \"Peroxisomes\": 17,\n",
    "    \"Endosomes\": 17,\n",
    "    \"Lysosomes\": 17,\n",
    "    \"Lipid droplets\": 17,\n",
    "    \"Cytoplasmic bodies\": 17,\n",
    "    \"Rods & rings\": 18,\n",
    "    # markpeng\n",
    "    \"No staining\": 18,\n",
    "})\n",
    "\n",
    "old_class_mappings = {}\n",
    "for i, (k, v) in enumerate(old_class_indices.items()):\n",
    "    if k in all_locations:\n",
    "        old_class_mappings[v] = all_locations[k]\n",
    "    else:\n",
    "        # No staining\n",
    "        old_class_mappings[v] = 18\n",
    "assert len(old_class_mappings.values()) == len(old_classes.values())\n",
    "print(old_class_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {dataset_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{dataset_folder}/train.csv\")\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submit_df.shape, submit_df.ImageWidth.min(), submit_df.ImageWidth.max())\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\"]\n",
    "\n",
    "test_ids = submit_df[\"ID\"].values.tolist()\n",
    "print(len(test_ids))\n",
    "\n",
    "# Estimated number of private test images (RGBY): 2236 x 2.3 ~= 5143 (for 9 hours we have 6.2 secs per image)\n",
    "# Estimated number of private test images: 559 x 2.3 ~= 1286 (for 9 hours we have 25.2 secs per image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook\n",
    "def binary_mask_to_ascii(mask, mask_val=1):\n",
    "    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "    mask = np.where(mask == mask_val, 1, 0).astype(np.bool)\n",
    "\n",
    "    # check input mask --\n",
    "    if mask.dtype != np.bool:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a binary mask, received dtype == {mask.dtype}\"\n",
    "        )\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a 2d mask, received shape == {mask.shape}\"\n",
    "        )\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str.decode()\n",
    "\n",
    "\n",
    "def rle_encoding(img, mask_val=1):\n",
    "    \"\"\"\n",
    "    Turns our masks into RLE encoding to easily store them\n",
    "    and feed them into models later on\n",
    "    https://en.wikipedia.org/wiki/Run-length_encoding\n",
    "    \n",
    "    Args:\n",
    "        img (np.array): Segmentation array\n",
    "        mask_val (int): Which value to use to create the RLE\n",
    "        \n",
    "    Returns:\n",
    "        RLE string\n",
    "    \n",
    "    \"\"\"\n",
    "    dots = np.where(img.T.flatten() == mask_val)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "\n",
    "    return ' '.join([str(x) for x in run_lengths])\n",
    "\n",
    "\n",
    "def rle_to_mask(rle_string, height, width):\n",
    "    \"\"\" Convert RLE sttring into a binary mask \n",
    "    \n",
    "    Args:\n",
    "        rle_string (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of the binary segmentation mask for a given cell\n",
    "    \"\"\"\n",
    "    rows, cols = height, width\n",
    "    rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n",
    "    rle_pairs = np.array(rle_numbers).reshape(-1, 2)\n",
    "    img = np.zeros(rows * cols, dtype=np.uint8)\n",
    "    for index, length in rle_pairs:\n",
    "        index -= 1\n",
    "        img[index:index + length] = 255\n",
    "    img = img.reshape(cols, rows)\n",
    "    img = img.T\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_segmentation_maps(list_of_image_lists, segmentator, batch_size=8):\n",
    "    \"\"\" Function to generate segmentation maps using CellSegmentator tool \n",
    "    \n",
    "    Args:\n",
    "        list_of_image_lists (list of lists):\n",
    "            - [[micro-tubules(red)], [endoplasmic-reticulum(yellow)], [nucleus(blue)]]\n",
    "        batch_size (int): Batch size to use in generating the segmentation masks\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing RLEs for all the cells in all images\n",
    "    \"\"\"\n",
    "\n",
    "    all_mask_rles = {}\n",
    "    for i in tqdm(range(0, len(list_of_image_lists[0]), batch_size),\n",
    "                  total=len(list_of_image_lists[0]) // batch_size):\n",
    "\n",
    "        # Get batch of images\n",
    "        sub_images = [\n",
    "            img_channel_list[i:i + batch_size]\n",
    "            for img_channel_list in list_of_image_lists\n",
    "        ]  # 0.000001 seconds\n",
    "\n",
    "        # Do segmentation\n",
    "        cell_segmentations = segmentator.pred_cells(sub_images)\n",
    "        nuc_segmentations = segmentator.pred_nuclei(sub_images[2])\n",
    "\n",
    "        # post-processing\n",
    "        for j, path in enumerate(sub_images[0]):\n",
    "            img_id = path.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1]\n",
    "            nuc_mask, cell_mask = label_cell(nuc_segmentations[j],\n",
    "                                             cell_segmentations[j])\n",
    "            new_name = os.path.basename(path).replace('red', 'mask')\n",
    "            all_mask_rles[img_id] = [\n",
    "                rle_encoding(cell_mask, mask_val=k)\n",
    "                for k in range(1,\n",
    "                               np.max(cell_mask) + 1)\n",
    "            ]\n",
    "    return all_mask_rles\n",
    "\n",
    "\n",
    "def get_img_list(img_dir, return_ids=False, sub_n=None):\n",
    "    \"\"\" Get image list in the format expected by the CellSegmentator tool \"\"\"\n",
    "    if sub_n is None:\n",
    "        sub_n = len(glob(img_dir + '/' + f'*_red.png'))\n",
    "    if return_ids:\n",
    "        images = [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "        return [\n",
    "            x.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1] for x in images[0]\n",
    "        ], images\n",
    "    else:\n",
    "        return [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_rle(\n",
    "    rle,\n",
    "    width,\n",
    "    height,\n",
    "    return_mask=True,\n",
    "):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        rle (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    mask = rle_to_mask(rle, height, width).copy()\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    x, y, w, h = cv2.boundingRect(cnts[0])\n",
    "\n",
    "    if return_mask:\n",
    "        return (x, y, x + w, y + h), mask\n",
    "    else:\n",
    "        return (x, y, x + w, y + h)\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_raw(raw_mask):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        raw_mask (nparray): Numpy array containing segmentation mask information\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(raw_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    xywhs = [cv2.boundingRect(cnt) for cnt in cnts]\n",
    "    xys = [(xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3])\n",
    "           for xywh in xywhs]\n",
    "    return sorted(xys, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "\n",
    "def pad_to_square(a):\n",
    "    \"\"\" Pad an array `a` evenly until it is a square \"\"\"\n",
    "    if a.shape[1] > a.shape[0]:  # pad height\n",
    "        n_to_add = a.shape[1] - a.shape[0]\n",
    "        top_pad = n_to_add // 2\n",
    "        bottom_pad = n_to_add - top_pad\n",
    "        a = np.pad(a, [(top_pad, bottom_pad), (0, 0), (0, 0)], mode='constant')\n",
    "\n",
    "    elif a.shape[0] > a.shape[1]:  # pad width\n",
    "        n_to_add = a.shape[0] - a.shape[1]\n",
    "        left_pad = n_to_add // 2\n",
    "        right_pad = n_to_add - left_pad\n",
    "        a = np.pad(a, [(0, 0), (left_pad, right_pad), (0, 0)], mode='constant')\n",
    "    else:\n",
    "        pass\n",
    "    return a\n",
    "\n",
    "\n",
    "def cut_out_cells(rgby,\n",
    "                  rles,\n",
    "                  resize_to=(256, 256),\n",
    "                  square_off=True,\n",
    "                  return_masks=False,\n",
    "                  from_raw=True):\n",
    "    \"\"\" Cut out the cells as padded square images \n",
    "    \n",
    "    Args:\n",
    "        rgby (np.array): 4 Channel image to be cut into tiles\n",
    "        rles (list of RLE strings): List of run length encoding containing \n",
    "            segmentation mask information\n",
    "        resize_to (tuple of ints, optional): The square dimension to resize the image to\n",
    "        square_off (bool, optional): Whether to pad the image to a square or not\n",
    "        \n",
    "    Returns:\n",
    "        list of square arrays representing squared off cell images\n",
    "    \"\"\"\n",
    "    w, h = rgby.shape[:2]\n",
    "    contour_bboxes = [\n",
    "        get_contour_bbox(rle, w, h, return_mask=return_masks) for rle in rles\n",
    "    ]\n",
    "    if return_masks:\n",
    "        masks = [x[-1] for x in contour_bboxes]\n",
    "        contour_bboxes = [x[:-1] for x in contour_bboxes]\n",
    "\n",
    "    arrs = [\n",
    "        rgby[bbox[1]:bbox[3], bbox[0]:bbox[2], ...] for bbox in contour_bboxes\n",
    "    ]\n",
    "    if square_off:\n",
    "        arrs = [pad_to_square(arr) for arr in arrs]\n",
    "\n",
    "    if resize_to is not None:\n",
    "        arrs = [\n",
    "            cv2.resize(pad_to_square(arr).astype(np.float32),\n",
    "                       resize_to,\n",
    "                       interpolation=cv2.INTER_CUBIC) \\\n",
    "            for arr in arrs\n",
    "        ]\n",
    "    if return_masks:\n",
    "        return arrs, masks\n",
    "    else:\n",
    "        return arrs\n",
    "\n",
    "\n",
    "def grab_contours(cnts):\n",
    "    # if the length the contours tuple returned by cv2.findContours\n",
    "    # is '2' then we are using either OpenCV v2.4, v4-beta, or\n",
    "    # v4-official\n",
    "    if len(cnts) == 2:\n",
    "        cnts = cnts[0]\n",
    "\n",
    "    # if the length of the contours tuple is '3' then we are using\n",
    "    # either OpenCV v3, v4-pre, or v4-alpha\n",
    "    elif len(cnts) == 3:\n",
    "        cnts = cnts[1]\n",
    "\n",
    "    # otherwise OpenCV has changed their cv2.findContours return\n",
    "    # signature yet again and I have no idea WTH is going on\n",
    "    else:\n",
    "        raise Exception(\n",
    "            (\"Contours tuple must have length 2 or 3, \"\n",
    "             \"otherwise OpenCV changed their cv2.findContours return \"\n",
    "             \"signature yet again. Refer to OpenCV's documentation \"\n",
    "             \"in that case\"))\n",
    "\n",
    "    # return the actual contours array\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/72534\n",
    "def generate_hash(img_dir,\n",
    "                  colors,\n",
    "                  dataset='train',\n",
    "                  imread_func=None,\n",
    "                  is_update=False):\n",
    "    meta = meta.copy()\n",
    "    hash_maps = {}\n",
    "    for color in colors:\n",
    "        hash_maps[color] = []\n",
    "        for idx in tqdm(range(len(meta)), desc='train %s' % color):\n",
    "            img = imread_func(img_dir, meta.iloc[idx][ID], color)\n",
    "            hash = imagehash.phash(img)\n",
    "            hash_maps[color].append(hash)\n",
    "\n",
    "    for color in colors:\n",
    "        meta[color] = hash_maps[color]\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def calc_hash(params):\n",
    "    color, threshold, base_test_hash1, base_test_hash2, test_ids1, test_ids2 = params\n",
    "\n",
    "    test_hash1 = base_test_hash1.reshape(1, -1)  # 1*m\n",
    "\n",
    "    test_idxes_list1 = []\n",
    "    test_idxes_list2 = []\n",
    "    hash_list = []\n",
    "\n",
    "    step = 5\n",
    "    for test_idx in tqdm(range(0, len(base_test_hash2), step), desc=color):\n",
    "        test_hash2 = base_test_hash2[test_idx:test_idx + step].reshape(\n",
    "            -1, 1)  # n*1\n",
    "        hash = test_hash2 - test_hash1  # n*m\n",
    "        test_idxes2, test_idxes1 = np.where(hash <= threshold)\n",
    "        hash = hash[test_idxes2, test_idxes1]\n",
    "\n",
    "        test_idxes2 = test_idxes2 + test_idx\n",
    "\n",
    "        test_idxes_list1.extend(test_idxes1.tolist())\n",
    "        test_idxes_list2.extend(test_idxes2.tolist())\n",
    "        hash_list.extend(hash.tolist())\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Test1': test_ids1[test_idxes_list1],\n",
    "        'Test2': test_ids2[test_idxes_list2],\n",
    "        'Sim%s' % color[:1].upper(): hash_list\n",
    "    })\n",
    "    df = df[df['Test1'] != df['Test2']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Cell Segmentations as Numpy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hpa_cell_segment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hpa_cell_segment.py\n",
    "\n",
    "kernel_mode = False\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "IMAGE_SIZES = [1728, 2048, 3072, 4096]\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "    img_dir = f\"{dataset_folder}/test\"\n",
    "    output_folder = \"/kaggle/working/test_cell_masks\"\n",
    "    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "    BATCH_SIZE = {1728: 24, 2048: 24, 3072: 12, 4096: 12}\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "    img_dir = f\"{dataset_folder}/test\"\n",
    "    output_folder = f\"{dataset_folder}/test_cell_masks\"\n",
    "    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "    #     BATCH_SIZE = {1728: 24, 2048: 24, 3072: 12, 4096: 12}\n",
    "    BATCH_SIZE = {1728: 24, 2048: 18, 3072: 8, 4096: 8}\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")\n",
    "\n",
    "predict_df_1728 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[0]]\n",
    "predict_df_2048 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[1]]\n",
    "predict_df_3072 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[2]]\n",
    "predict_df_4096 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[3]]\n",
    "\n",
    "predict_ids_1728 = predict_df_1728.ID.to_list()\n",
    "predict_ids_2048 = predict_df_2048.ID.to_list()\n",
    "predict_ids_3072 = predict_df_3072.ID.to_list()\n",
    "predict_ids_4096 = predict_df_4096.ID.to_list()\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skimage import transform, util\n",
    "\"\"\"Shared constants for the HPA Cell Segmentation package.\"\"\"\n",
    "\n",
    "NUCLEI_MODEL_URL = (\n",
    "    \"https://zenodo.org/record/4665863/files/dpn_unet_nuclei_v1.pth\")\n",
    "\n",
    "MULTI_CHANNEL_CELL_MODEL_URL = (\n",
    "    \"https://zenodo.org/record/4665863/files/dpn_unet_cell_3ch_v1.pth\")\n",
    "\n",
    "TWO_CHANNEL_CELL_MODEL_URL = (\n",
    "    \"https://zenodo.org/record/4665863/files/dpn_unet_cell_v2.pth\")\n",
    "\"\"\"Utility functions for the HPA Cell Segmentation package.\"\"\"\n",
    "import os.path\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "from skimage import filters, measure, segmentation\n",
    "from skimage.morphology import (binary_erosion, closing, disk,\n",
    "                                remove_small_holes, remove_small_objects)\n",
    "\n",
    "HIGH_THRESHOLD = 0.4\n",
    "LOW_THRESHOLD = HIGH_THRESHOLD - 0.25\n",
    "\n",
    "\n",
    "def download_with_url(url_string, file_path, unzip=False):\n",
    "    \"\"\"Download file with a link.\"\"\"\n",
    "    with urllib.request.urlopen(url_string) as response, open(\n",
    "            file_path, \"wb\") as out_file:\n",
    "        data = response.read()  # a `bytes` object\n",
    "        out_file.write(data)\n",
    "\n",
    "    if unzip:\n",
    "        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(file_path))\n",
    "\n",
    "\n",
    "def __fill_holes(image):\n",
    "    \"\"\"Fill_holes for labelled image, with a unique number.\"\"\"\n",
    "    boundaries = segmentation.find_boundaries(image)\n",
    "    image = np.multiply(image, np.invert(boundaries))\n",
    "    image = ndi.binary_fill_holes(image > 0)\n",
    "    image = ndi.label(image)[0]\n",
    "    return image\n",
    "\n",
    "\n",
    "def label_nuclei(nuclei_pred):\n",
    "    \"\"\"Return the labeled nuclei mask data array.\n",
    "    This function works best for Human Protein Atlas cell images with\n",
    "    predictions from the CellSegmentator class.\n",
    "    Keyword arguments:\n",
    "    nuclei_pred -- a 3D numpy array of a prediction from a nuclei image.\n",
    "    Returns:\n",
    "    nuclei-label -- An array with unique numbers for each found nuclei\n",
    "                    in the nuclei_pred. A value of 0 in the array is\n",
    "                    considered background, and the values 1-n is the\n",
    "                    areas of the cells 1-n.\n",
    "    \"\"\"\n",
    "    img_copy = np.copy(nuclei_pred[..., 2])\n",
    "    borders = (nuclei_pred[..., 1] > 0.05).astype(np.uint8)\n",
    "    m = img_copy * (1 - borders)\n",
    "\n",
    "    img_copy[m <= LOW_THRESHOLD] = 0\n",
    "    img_copy[m > LOW_THRESHOLD] = 1\n",
    "    img_copy = img_copy.astype(np.bool)\n",
    "    img_copy = binary_erosion(img_copy)\n",
    "    # TODO: Add parameter for remove small object size for\n",
    "    #       differently scaled images.\n",
    "    # img_copy = remove_small_objects(img_copy, 500)\n",
    "    img_copy = img_copy.astype(np.uint8)\n",
    "    markers = measure.label(img_copy).astype(np.uint32)\n",
    "\n",
    "    mask_img = np.copy(nuclei_pred[..., 2])\n",
    "    mask_img[mask_img <= HIGH_THRESHOLD] = 0\n",
    "    mask_img[mask_img > HIGH_THRESHOLD] = 1\n",
    "    mask_img = mask_img.astype(np.bool)\n",
    "    mask_img = remove_small_holes(mask_img, 1000)\n",
    "    # TODO: Figure out good value for remove small objects.\n",
    "    # mask_img = remove_small_objects(mask_img, 8)\n",
    "    mask_img = mask_img.astype(np.uint8)\n",
    "    nuclei_label = segmentation.watershed(mask_img,\n",
    "                                          markers,\n",
    "                                          mask=mask_img,\n",
    "                                          watershed_line=True)\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 2500)\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    return nuclei_label\n",
    "\n",
    "\n",
    "def label_cell(nuclei_pred, cell_pred):\n",
    "    \"\"\"Label the cells and the nuclei.\n",
    "    Keyword arguments:\n",
    "    nuclei_pred -- a 3D numpy array of a prediction from a nuclei image.\n",
    "    cell_pred -- a 3D numpy array of a prediction from a cell image.\n",
    "    Returns:\n",
    "    A tuple containing:\n",
    "    nuclei-label -- A nuclei mask data array.\n",
    "    cell-label  -- A cell mask data array.\n",
    "    0's in the data arrays indicate background while a continous\n",
    "    strech of a specific number indicates the area for a specific\n",
    "    cell.\n",
    "    The same value in cell mask and nuclei mask refers to the identical cell.\n",
    "    NOTE: The nuclei labeling from this function will be sligthly\n",
    "    different from the values in :func:`label_nuclei` as this version\n",
    "    will use information from the cell-predictions to make better\n",
    "    estimates.\n",
    "    \"\"\"\n",
    "    def __wsh(\n",
    "        mask_img,\n",
    "        threshold,\n",
    "        border_img,\n",
    "        seeds,\n",
    "        threshold_adjustment=0.35,\n",
    "        small_object_size_cutoff=10,\n",
    "    ):\n",
    "        img_copy = np.copy(mask_img)\n",
    "        m = seeds * border_img  # * dt\n",
    "        img_copy[m <= threshold + threshold_adjustment] = 0\n",
    "        img_copy[m > threshold + threshold_adjustment] = 1\n",
    "        img_copy = img_copy.astype(np.bool)\n",
    "        img_copy = remove_small_objects(\n",
    "            img_copy, small_object_size_cutoff).astype(np.uint8)\n",
    "\n",
    "        mask_img[mask_img <= threshold] = 0\n",
    "        mask_img[mask_img > threshold] = 1\n",
    "        mask_img = mask_img.astype(np.bool)\n",
    "        mask_img = remove_small_holes(mask_img, 1000)\n",
    "        mask_img = remove_small_objects(mask_img, 8).astype(np.uint8)\n",
    "        markers = ndi.label(img_copy, output=np.uint32)[0]\n",
    "        labeled_array = segmentation.watershed(mask_img,\n",
    "                                               markers,\n",
    "                                               mask=mask_img,\n",
    "                                               watershed_line=True)\n",
    "        return labeled_array\n",
    "\n",
    "    nuclei_label = __wsh(\n",
    "        nuclei_pred[..., 2] / 255.0,\n",
    "        0.4,\n",
    "        1 - (nuclei_pred[..., 1] + cell_pred[..., 1]) / 255.0 > 0.05,\n",
    "        nuclei_pred[..., 2] / 255,\n",
    "        threshold_adjustment=-0.25,\n",
    "        small_object_size_cutoff=500,\n",
    "    )\n",
    "\n",
    "    # for hpa_image, to remove the small pseduo nuclei\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 2500)\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    # this is to remove the cell borders' signal from cell mask.\n",
    "    # could use np.logical_and with some revision, to replace this func.\n",
    "    # Tuned for segmentation hpa images\n",
    "    threshold_value = max(\n",
    "        0.22,\n",
    "        filters.threshold_otsu(cell_pred[..., 2] / 255) * 0.5)\n",
    "    # exclude the green area first\n",
    "    cell_region = np.multiply(\n",
    "        cell_pred[..., 2] / 255 > threshold_value,\n",
    "        np.invert(np.asarray(cell_pred[..., 1] / 255 > 0.05, dtype=np.int8)),\n",
    "    )\n",
    "    sk = np.asarray(cell_region, dtype=np.int8)\n",
    "    distance = np.clip(cell_pred[..., 2], 255 * threshold_value, cell_pred[...,\n",
    "                                                                           2])\n",
    "    cell_label = segmentation.watershed(-distance, nuclei_label, mask=sk)\n",
    "    cell_label = remove_small_objects(cell_label, 5500).astype(np.uint8)\n",
    "    selem = disk(6)\n",
    "    cell_label = closing(cell_label, selem)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    # this part is to use green channel, and extend cell label to green channel\n",
    "    # benefit is to exclude cells clear on border but without nucleus\n",
    "    sk = np.asarray(\n",
    "        np.add(\n",
    "            np.asarray(cell_label > 0, dtype=np.int8),\n",
    "            np.asarray(cell_pred[..., 1] / 255 > 0.05, dtype=np.int8),\n",
    "        ) > 0,\n",
    "        dtype=np.int8,\n",
    "    )\n",
    "    cell_label = segmentation.watershed(-distance, cell_label, mask=sk)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    cell_label = np.asarray(cell_label > 0, dtype=np.uint8)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    cell_label = remove_small_objects(cell_label, 5500)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    cell_label = np.asarray(cell_label, dtype=np.uint16)\n",
    "    nuclei_label = np.multiply(cell_label > 0, nuclei_label) > 0\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 2500)\n",
    "    nuclei_label = np.multiply(cell_label, nuclei_label > 0)\n",
    "\n",
    "    return nuclei_label, cell_label\n",
    "\n",
    "\n",
    "def label_cell2(cell_pred):\n",
    "    \"\"\"label cell with only cell predition\"\"\"\n",
    "    cell_pred = cell_pred / cell_pred.max()\n",
    "    size = cell_pred.shape[0]\n",
    "    img = cell_pred.copy()\n",
    "    cell_pred[..., 2] = filters.gaussian(cell_pred[..., 2], sigma=8)\n",
    "    threshold_value = max(0.22, filters.threshold_otsu(cell_pred[..., 2]))\n",
    "    threshold_value1 = max(0.6, filters.threshold_otsu(img[..., 2]))\n",
    "    # exclude the green area first\n",
    "    cell_region = np.multiply(\n",
    "        cell_pred[..., 2],\n",
    "        np.logical_and(np.invert(np.asarray(cell_pred[..., 1] > 0.01)),\n",
    "                       cell_pred[..., 2] > threshold_value))\n",
    "    cell_region1 = np.multiply(\n",
    "        img[..., 2] > threshold_value1,\n",
    "        np.invert(np.asarray(cell_pred[..., 1] > 0.01)),\n",
    "    )\n",
    "    cell_region_eroded = morphology.erosion(cell_region1,\n",
    "                                            morphology.square(25))\n",
    "    cell_region_eroded = np.asarray(cell_region_eroded, dtype=np.uint8)\n",
    "    cell_region_eroded = ndi.label(cell_region_eroded)[0]\n",
    "    remove_size_ratio = int((size / 512)**2)\n",
    "    cell_region_eroded = remove_small_objects(cell_region_eroded,\n",
    "                                              10 * remove_size_ratio)\n",
    "    cell_region_eroded = np.asarray(cell_region_eroded > 0, dtype=np.uint8)\n",
    "    distance = np.clip(cell_pred[..., 2], threshold_value, cell_pred[..., 2])\n",
    "    local_maxi = feature.peak_local_max(cell_region_eroded,\n",
    "                                        indices=False,\n",
    "                                        footprint=np.ones((1, 1)))\n",
    "    markers = ndi.label(local_maxi)[0]\n",
    "    cell_label = segmentation.watershed(-distance, markers, mask=cell_region)\n",
    "    cell_label = remove_small_objects(cell_label, 1000 *\n",
    "                                      remove_size_ratio).astype(np.uint8)\n",
    "    selem = disk(6)\n",
    "    cell_label = closing(cell_label, selem)\n",
    "    # this part is to use green channel, and extend cell label to green channel\n",
    "    # benefit is to exclude cells clear on border but without nucleus\n",
    "    sk = np.logical_or(\n",
    "        cell_label > 0,\n",
    "        cell_pred[..., 1] > 0.1,\n",
    "    )\n",
    "    sk = np.asarray(sk, dtype=np.uint8)\n",
    "    cell_label = segmentation.watershed(-sk, cell_label, mask=sk)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    #cell_label = np.asarray(cell_label, dtype=np.uint16)\n",
    "\n",
    "    return cell_label\n",
    "\n",
    "\n",
    "NORMALIZE = {\n",
    "    \"mean\": [124 / 255, 117 / 255, 104 / 255],\n",
    "    \"std\": [1 / (0.0167 * 255)] * 3\n",
    "}\n",
    "\n",
    "\n",
    "class CellSegmentator(object):\n",
    "    \"\"\"Uses pretrained DPN-Unet models to segment cells from images.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        nuclei_model=\"./nuclei_model.pth\",\n",
    "        cell_model=\"./cell_model.pth\",\n",
    "        scale_factor=0.25,\n",
    "        device=\"cuda\",\n",
    "        padding=False,\n",
    "        multi_channel_model=True,\n",
    "    ):\n",
    "        \"\"\"Class for segmenting nuclei and whole cells from confocal microscopy images.\n",
    "        It takes lists of images and returns the raw output from the\n",
    "        specified segmentation model. Models can be automatically\n",
    "        downloaded if they are not already available on the system.\n",
    "        When working with images from the Huan Protein Cell atlas, the\n",
    "        outputs from this class' methods are well combined with the\n",
    "        label functions in the utils module.\n",
    "        Note that for cell segmentation, there are two possible models\n",
    "        available. One that works with 2 channeled images and one that\n",
    "        takes 3 channels.\n",
    "        Keyword arguments:\n",
    "        nuclei_model -- A loaded torch nuclei segmentation model or the\n",
    "                        path to a file which contains such a model.\n",
    "                        If the argument is a path that points to a non-existant file,\n",
    "                        a pretrained nuclei_model is going to get downloaded to the\n",
    "                        specified path (default: './nuclei_model.pth').\n",
    "        cell_model -- A loaded torch cell segmentation model or the\n",
    "                      path to a file which contains such a model.\n",
    "                      The cell_model argument can be None if only nuclei\n",
    "                      are to be segmented (default: './cell_model.pth').\n",
    "        scale_factor -- How much to scale images before they are fed to\n",
    "                        segmentation models. Segmentations will be scaled back\n",
    "                        up by 1/scale_factor to match the original image\n",
    "                        (default: 0.25).\n",
    "        device -- The device on which to run the models.\n",
    "                  This should either be 'cpu' or 'cuda' or pointed cuda\n",
    "                  device like 'cuda:0' (default: 'cuda').\n",
    "        padding -- Whether to add padding to the images before feeding the\n",
    "                   images to the network. (default: False).\n",
    "        multi_channel_model -- Control whether to use the 3-channel cell model or not.\n",
    "                               If True, use the 3-channel model, otherwise use the\n",
    "                               2-channel version (default: True).\n",
    "        \"\"\"\n",
    "        if device != \"cuda\" and device != \"cpu\" and \"cuda\" not in device:\n",
    "            raise ValueError(f\"{device} is not a valid device (cuda/cpu)\")\n",
    "        if device != \"cpu\":\n",
    "            try:\n",
    "                assert torch.cuda.is_available()\n",
    "            except AssertionError:\n",
    "                print(\"No GPU found, using CPU.\", file=sys.stderr)\n",
    "                device = \"cpu\"\n",
    "        self.device = device\n",
    "\n",
    "        if isinstance(nuclei_model, str):\n",
    "            if not os.path.exists(nuclei_model):\n",
    "                print(\n",
    "                    f\"Could not find {nuclei_model}. Downloading it now\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                download_with_url(NUCLEI_MODEL_URL, nuclei_model)\n",
    "            nuclei_model = torch.load(nuclei_model,\n",
    "                                      map_location=torch.device(self.device))\n",
    "        if isinstance(nuclei_model, torch.nn.DataParallel) and device == \"cpu\":\n",
    "            nuclei_model = nuclei_model.module\n",
    "\n",
    "        self.nuclei_model = nuclei_model.to(self.device)\n",
    "\n",
    "        self.multi_channel_model = multi_channel_model\n",
    "        if isinstance(cell_model, str):\n",
    "            if not os.path.exists(cell_model):\n",
    "                print(f\"Could not find {cell_model}. Downloading it now\",\n",
    "                      file=sys.stderr)\n",
    "                if self.multi_channel_model:\n",
    "                    download_with_url(MULTI_CHANNEL_CELL_MODEL_URL, cell_model)\n",
    "                else:\n",
    "                    download_with_url(TWO_CHANNEL_CELL_MODEL_URL, cell_model)\n",
    "            cell_model = torch.load(cell_model,\n",
    "                                    map_location=torch.device(self.device))\n",
    "        self.cell_model = cell_model.to(self.device)\n",
    "        self.scale_factor = scale_factor\n",
    "        self.padding = padding\n",
    "\n",
    "        # markpeng\n",
    "        self.nuclei_model.eval()\n",
    "        self.cell_model.eval()\n",
    "\n",
    "    def _image_conversion(self, images):\n",
    "        \"\"\"Convert/Format images to RGB image arrays list for cell predictions.\n",
    "        Intended for internal use only.\n",
    "        Keyword arguments:\n",
    "        images -- list of lists of image paths/arrays. It should following the\n",
    "                 pattern if with er channel input,\n",
    "                 [\n",
    "                     [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                     [er_path0/image_array0, er_path1/image_array1, ...],\n",
    "                     [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                 ]\n",
    "                 or if without er input,\n",
    "                 [\n",
    "                     [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                     None,\n",
    "                     [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                 ]\n",
    "        \"\"\"\n",
    "        microtubule_imgs, er_imgs, nuclei_imgs = images\n",
    "        if self.multi_channel_model:\n",
    "            if not isinstance(er_imgs, list):\n",
    "                raise ValueError(\n",
    "                    \"Please speicify the image path(s) for er channels!\")\n",
    "        else:\n",
    "            if not er_imgs is None:\n",
    "                raise ValueError(\n",
    "                    \"second channel should be None for two channel model predition!\"\n",
    "                )\n",
    "\n",
    "        if not isinstance(microtubule_imgs, list):\n",
    "            raise ValueError(\"The microtubule images should be a list\")\n",
    "        if not isinstance(nuclei_imgs, list):\n",
    "            raise ValueError(\"The microtubule images should be a list\")\n",
    "\n",
    "        if er_imgs:\n",
    "            if not len(microtubule_imgs) == len(er_imgs) == len(nuclei_imgs):\n",
    "                raise ValueError(\n",
    "                    \"The lists of images needs to be the same length\")\n",
    "        else:\n",
    "            if not len(microtubule_imgs) == len(nuclei_imgs):\n",
    "                raise ValueError(\n",
    "                    \"The lists of images needs to be the same length\")\n",
    "\n",
    "        if not all(isinstance(item, np.ndarray) for item in microtubule_imgs):\n",
    "            microtubule_imgs = [\n",
    "                os.path.expanduser(item)\n",
    "                for _, item in enumerate(microtubule_imgs)\n",
    "            ]\n",
    "            nuclei_imgs = [\n",
    "                os.path.expanduser(item) for _, item in enumerate(nuclei_imgs)\n",
    "            ]\n",
    "\n",
    "            microtubule_imgs = list(\n",
    "                map(lambda item: imageio.imread(item), microtubule_imgs))\n",
    "            nuclei_imgs = list(\n",
    "                map(lambda item: imageio.imread(item), nuclei_imgs))\n",
    "            if er_imgs:\n",
    "                er_imgs = [\n",
    "                    os.path.expanduser(item) for _, item in enumerate(er_imgs)\n",
    "                ]\n",
    "                er_imgs = list(map(lambda item: imageio.imread(item), er_imgs))\n",
    "\n",
    "        if not er_imgs:\n",
    "            er_imgs = [\n",
    "                np.zeros(item.shape, dtype=item.dtype)\n",
    "                for _, item in enumerate(microtubule_imgs)\n",
    "            ]\n",
    "        cell_imgs = list(\n",
    "            map(\n",
    "                lambda item: np.dstack((item[0], item[1], item[2])),\n",
    "                list(zip(microtubule_imgs, er_imgs, nuclei_imgs)),\n",
    "            ))\n",
    "\n",
    "        return cell_imgs\n",
    "\n",
    "    def pred_nuclei(self, images, bs=24):\n",
    "        \"\"\"Predict the nuclei segmentation.\n",
    "        Keyword arguments:\n",
    "        images -- A list of image arrays or a list of paths to images.\n",
    "                  If as a list of image arrays, the images could be 2d images\n",
    "                  of nuclei data array only, or must have the nuclei data in\n",
    "                  the blue channel; If as a list of file paths, the images\n",
    "                  could be RGB image files or gray scale nuclei image file\n",
    "                  paths.\n",
    "        Returns:\n",
    "        predictions -- A list of predictions of nuclei segmentation for each nuclei image.\n",
    "        \"\"\"\n",
    "        def _preprocess(image):\n",
    "            if isinstance(image, str):\n",
    "                image = imageio.imread(image)\n",
    "            self.target_shape = image.shape\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.dstack((image, image, image))\n",
    "            image = transform.rescale(image,\n",
    "                                      self.scale_factor,\n",
    "                                      multichannel=True)\n",
    "            nuc_image = np.dstack((image[..., 2], image[..., 2], image[...,\n",
    "                                                                       2]))\n",
    "            if self.padding:\n",
    "                rows, cols = nuc_image.shape[:2]\n",
    "                self.scaled_shape = rows, cols\n",
    "                nuc_image = cv2.copyMakeBorder(\n",
    "                    nuc_image,\n",
    "                    32,\n",
    "                    (32 - rows % 32),\n",
    "                    32,\n",
    "                    (32 - cols % 32),\n",
    "                    cv2.BORDER_REFLECT,\n",
    "                )\n",
    "            nuc_image = nuc_image.transpose([2, 0, 1])\n",
    "            return nuc_image\n",
    "\n",
    "        def _segment_helper(imgs):\n",
    "            with torch.no_grad():\n",
    "                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n",
    "                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n",
    "                imgs = torch.tensor(imgs).float()\n",
    "                imgs = imgs.to(self.device)\n",
    "                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "\n",
    "                imgs = self.nuclei_model(imgs)\n",
    "                imgs = F.softmax(imgs, dim=1)\n",
    "                return imgs\n",
    "\n",
    "        # preprocessed_imgs = map(_preprocess, images)\n",
    "        # predictions = map(lambda x: _segment_helper([x]), preprocessed_imgs)\n",
    "        # predictions = map(lambda x: x.to(\"cpu\").numpy()[0], predictions)\n",
    "        # predictions = map(util.img_as_ubyte, predictions)\n",
    "        # predictions = list(map(self._restore_scaling_padding, predictions))\n",
    "        preprocessed_imgs = list(map(_preprocess, images))\n",
    "        predictions = []\n",
    "        for i in range(0, len(preprocessed_imgs), bs):\n",
    "            start = i\n",
    "            end = min(len(preprocessed_imgs), i + bs)\n",
    "            x = preprocessed_imgs[start:end]\n",
    "            pred = _segment_helper(x).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "        predictions = list(np.concatenate(predictions, axis=0))\n",
    "        predictions = map(util.img_as_ubyte, predictions)\n",
    "        predictions = list(map(self._restore_scaling_padding, predictions))\n",
    "        return predictions\n",
    "\n",
    "    def _restore_scaling_padding(self, n_prediction):\n",
    "        \"\"\"Restore an image from scaling and padding.\n",
    "        This method is intended for internal use.\n",
    "        It takes the output from the nuclei model as input.\n",
    "        \"\"\"\n",
    "        n_prediction = n_prediction.transpose([1, 2, 0])\n",
    "        if self.padding:\n",
    "            n_prediction = n_prediction[32:32 + self.scaled_shape[0],\n",
    "                                        32:32 + self.scaled_shape[1], ...]\n",
    "        n_prediction[..., 0] = 0\n",
    "        if not self.scale_factor == 1:\n",
    "            n_prediction = cv2.resize(\n",
    "                n_prediction,\n",
    "                (self.target_shape[0], self.target_shape[1]),\n",
    "                interpolation=cv2.INTER_AREA,\n",
    "            )\n",
    "        return n_prediction\n",
    "\n",
    "    def pred_cells(self, images, precombined=False, bs=24):\n",
    "        \"\"\"Predict the cell segmentation for a list of images.\n",
    "        Keyword arguments:\n",
    "        images -- list of lists of image paths/arrays. It should following the\n",
    "                  pattern if with er channel input,\n",
    "                  [\n",
    "                      [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                      [er_path0/image_array0, er_path1/image_array1, ...],\n",
    "                      [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                  ]\n",
    "                  or if without er input,\n",
    "                  [\n",
    "                      [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                      None,\n",
    "                      [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                  ]\n",
    "                  The ER channel is required when multichannel is True\n",
    "                  and required to be None when multichannel is False.\n",
    "                  The images needs to be of the same size.\n",
    "        precombined -- If precombined is True, the list of images is instead supposed to be\n",
    "                       a list of RGB numpy arrays (default: False).\n",
    "        Returns:\n",
    "        predictions -- a list of predictions of cell segmentations.\n",
    "        \"\"\"\n",
    "        def _preprocess(image):\n",
    "            self.target_shape = image.shape\n",
    "            if not len(image.shape) == 3:\n",
    "                raise ValueError(\"image should has 3 channels\")\n",
    "            cell_image = transform.rescale(image,\n",
    "                                           self.scale_factor,\n",
    "                                           multichannel=True)\n",
    "            if self.padding:\n",
    "                rows, cols = cell_image.shape[:2]\n",
    "                self.scaled_shape = rows, cols\n",
    "                cell_image = cv2.copyMakeBorder(\n",
    "                    cell_image,\n",
    "                    32,\n",
    "                    (32 - rows % 32),\n",
    "                    32,\n",
    "                    (32 - cols % 32),\n",
    "                    cv2.BORDER_REFLECT,\n",
    "                )\n",
    "            cell_image = cell_image.transpose([2, 0, 1])\n",
    "            return cell_image\n",
    "\n",
    "        def _segment_helper(imgs):\n",
    "            with torch.no_grad():\n",
    "                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n",
    "                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n",
    "                imgs = torch.tensor(imgs).float()\n",
    "                imgs = imgs.to(self.device)\n",
    "                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "\n",
    "                imgs = self.cell_model(imgs)\n",
    "                imgs = F.softmax(imgs, dim=1)\n",
    "                return imgs\n",
    "\n",
    "        if not precombined:\n",
    "            images = self._image_conversion(images)\n",
    "        # preprocessed_imgs = map(_preprocess, images)\n",
    "        # predictions = map(lambda x: _segment_helper([x]), preprocessed_imgs)\n",
    "        # predictions = map(lambda x: x.to(\"cpu\").numpy()[0], predictions)\n",
    "        # predictions = map(self._restore_scaling_padding, predictions)\n",
    "        # predictions = list(map(util.img_as_ubyte, predictions))\n",
    "        preprocessed_imgs = list(map(_preprocess, images))\n",
    "        predictions = []\n",
    "        for i in range(0, len(preprocessed_imgs), bs):\n",
    "            start = i\n",
    "            end = min(len(preprocessed_imgs), i + bs)\n",
    "            x = preprocessed_imgs[start:end]\n",
    "            pred = _segment_helper(x).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "        predictions = list(np.concatenate(predictions, axis=0))\n",
    "        predictions = map(self._restore_scaling_padding, predictions)\n",
    "        predictions = list(map(util.img_as_ubyte, predictions))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "segmentator = CellSegmentator(\n",
    "    NUC_MODEL,\n",
    "    CELL_MODEL,\n",
    "    scale_factor=0.25,\n",
    "    device=\"cuda\",\n",
    "    padding=True,\n",
    "    multi_channel_model=True,\n",
    ")\n",
    "\n",
    "def get_segment_mask(batch_image_paths, bs=24):\n",
    "    nuc_segmentations = segmentator.pred_nuclei(batch_image_paths[2],\n",
    "                                                bs=bs)  # blue\n",
    "    cell_segmentations = segmentator.pred_cells(batch_image_paths,\n",
    "                                                bs=bs)\n",
    "    batch_cell_masks = [\n",
    "        label_cell(nuc_seg, cell_seg)[1].astype(np.uint8)\n",
    "        for nuc_seg, cell_seg in zip(nuc_segmentations, cell_segmentations)\n",
    "    ]\n",
    "    return batch_cell_masks\n",
    "\n",
    "def load_images(df: pd.DataFrame, root=img_dir):\n",
    "    gray = []\n",
    "    rgb = []\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        r = os.path.join(root, f'{row.ID}_red.png')\n",
    "        y = os.path.join(root, f'{row.ID}_yellow.png')\n",
    "        b = os.path.join(root, f'{row.ID}_blue.png')\n",
    "        r = cv2.imread(r, 0)\n",
    "        y = cv2.imread(y, 0)\n",
    "        b = cv2.imread(b, 0)\n",
    "        gray_image = cv2.resize(b, (512, 512))\n",
    "        rgb_image = cv2.resize(np.stack((r, y, b), axis=2), (512, 512))\n",
    "        gray.append(gray_image)\n",
    "        rgb.append(rgb_image)\n",
    "    return gray, rgb\n",
    "\n",
    "for size_idx, submission_ids in tqdm(enumerate(\n",
    "    [predict_ids_1728, predict_ids_2048, predict_ids_3072, predict_ids_4096]),\n",
    "                                     total=4):\n",
    "    size = IMAGE_SIZES[size_idx]\n",
    "    if submission_ids == []:\n",
    "        print(f\"\\n...SKIPPING SIZE {size} AS THERE ARE NO IMAGE IDS ...\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"\\n...WORKING ON IMAGE IDS FOR SIZE {size} ...\\n\")\n",
    "    for i in tqdm(range(0, len(submission_ids), BATCH_SIZE[size]),\n",
    "                  total=int(np.ceil(len(submission_ids) / BATCH_SIZE[size]))):\n",
    "\n",
    "        r, y, b = [], [], []\n",
    "        image_ids = submission_ids[i:(i + BATCH_SIZE[size])]\n",
    "        for img_id in image_ids:\n",
    "            r.append(os.path.join(img_dir, f'{img_id}_red.png'))\n",
    "            y.append(os.path.join(img_dir, f'{img_id}_yellow.png'))\n",
    "            b.append(os.path.join(img_dir, f'{img_id}_blue.png'))\n",
    "        batch_image_paths = [r, y, b]\n",
    "        batch_cell_masks = get_segment_mask(batch_image_paths,\n",
    "                                            bs=BATCH_SIZE[size])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        for index, img_id in enumerate(image_ids):\n",
    "            np.save(os.path.join(output_folder, f'{img_id}_cell_mask.npy'),\n",
    "                    batch_cell_masks[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please compile abn\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'pytorch_zoo.unet.DPNUnet' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.upsampling.Upsample' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/serialization.py:649: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\n",
      "...WORKING ON IMAGE IDS FOR SIZE 1728 ...\n",
      "\n",
      "\n",
      "  0%|                                                     | 0/3 [00:06<?, ?it/s]\u001b[A\n",
      "  0%|                                                     | 0/4 [00:06<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"hpa_cell_segment.py\", line 666, in <module>\n",
      "    bs=BATCH_SIZE[size])\n",
      "  File \"hpa_cell_segment.py\", line 638, in get_segment_mask\n",
      "    cell_segmentations = segmentator.pred_cells(batch_image_paths, precombined=True, bs=bs)\n",
      "  File \"hpa_cell_segment.py\", line 610, in pred_cells\n",
      "    preprocessed_imgs = list(map(_preprocess, images))\n",
      "  File \"hpa_cell_segment.py\", line 571, in _preprocess\n",
      "    self.target_shape = image.shape\n",
      "AttributeError: 'list' object has no attribute 'shape'\n",
      "CPU times: user 103 ms, sys: 20 ms, total: 123 ms\n",
      "Wall time: 9.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python hpa_cell_segment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Std From Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mean = [0.081018, 0.052349, 0.054012, 0.08106] # rgby\n",
    "total_std = [0.133235, 0.08948, 0.143813, 0.130265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Pretrained Model from Bestfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Protein Classification')\n",
    "parser.add_argument('--out_dir', type=str, help='destination where predicted result should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for predicting (default: 0)')\n",
    "parser.add_argument('--arch', default='class_densenet121_dropout', type=str,\n",
    "                    help='model architecture (default: class_densenet121_dropout)')\n",
    "parser.add_argument('--num_classes', default=28, type=int, help='number of classes (default: 28)')\n",
    "parser.add_argument('--in_channels', default=4, type=int, help='in channels (default: 4)')\n",
    "parser.add_argument('--img_size', default=768, type=int, help='image size (default: 768)')\n",
    "parser.add_argument('--crop_size', default=512, type=int, help='crop size (default: 512)')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='train mini-batch size (default: 32)')\n",
    "parser.add_argument('--workers', default=3, type=int, help='number of data loading workers (default: 3)')\n",
    "parser.add_argument('--fold', default=0, type=int, help='index of fold (default: 0)')\n",
    "parser.add_argument('--augment', default='default', type=str, help='test augmentation (default: default)')\n",
    "parser.add_argument('--seed', default=100, type=int, help='random seed (default: 100)')\n",
    "parser.add_argument('--seeds', default=None, type=str, help='predict seed')\n",
    "parser.add_argument('--predict_epoch', default=None, type=int, help='number epoch to predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args = parser.parse_args([\n",
    "    \"--arch\",\n",
    "    \"class_densenet121_dropout\",\n",
    "    #     \"--img_size\", str(image_size),\n",
    "    \"--crop_size\",\n",
    "    str(crop_size),\n",
    "])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_model(network_path, args, print_model=False):\n",
    "    # setting up the visible GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = args.arch\n",
    "    model_params['num_classes'] = args.num_classes\n",
    "    model_params['in_channels'] = args.in_channels\n",
    "    model_params['pretrained_path'] = f\"{bestfitting_folder}\"\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    checkpoint = torch.load(network_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # moving network to gpu and eval mode\n",
    "    # model = DataParallel(model)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    if print_model:\n",
    "        print(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crop_img(img):\n",
    "    random_crop_size = int(np.random.uniform(self.crop_size, self.img_size))\n",
    "    x = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "    y = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "    crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n",
    "    return crop_img\n",
    "\n",
    "\n",
    "def read_rgby(\n",
    "    img_dir,\n",
    "    img_id,\n",
    "    random_crop=False,\n",
    "):\n",
    "    suffix = '.png'\n",
    "    colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    rgby_img = [\n",
    "        cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n",
    "        for color in colors\n",
    "    ]\n",
    "    rgby_img = np.stack(rgby_img, axis=-1)\n",
    "    if random_crop and crop_size > 0:\n",
    "        rgby_img = read_crop_img(rgby_img)\n",
    "\n",
    "    return rgby_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_id, img_dir):\n",
    "    if img_id not in global_cache:\n",
    "        rgby_img = read_rgby(img_dir, img_id)\n",
    "        if rgby_img[0] is None:\n",
    "            print(self.img_dir, img_id)\n",
    "\n",
    "        h, w = rgby_img.shape[:2]\n",
    "\n",
    "        if crop_size > 0:\n",
    "            if crop_size != h or crop_size != w:\n",
    "                resized_rgby_img = cv2.resize(rgby_img, (crop_size, crop_size),\n",
    "                                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        full_mask = np.load(f\"{cell_mask_folder}/{img_id}_cell_mask.npy\")\n",
    "\n",
    "        full_mask = cv2.resize(full_mask, (crop_size, crop_size),\n",
    "                               interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        cell_masks = [\n",
    "            rle_encoding(full_mask, mask_val=k)\n",
    "            for k in range(1,\n",
    "                           np.max(full_mask) + 1)\n",
    "        ]\n",
    "        if len(cell_masks) == 0:\n",
    "            print(f\"No cell masks found for {img_id}\")\n",
    "\n",
    "        resized_rgby_img = resized_rgby_img / 255.0\n",
    "\n",
    "        global_cache[img_id] = (resized_rgby_img, cell_masks)\n",
    "        return resized_rgby_img, cell_masks\n",
    "    else:\n",
    "        # print(f\"Cache hit for {img_id}!\")\n",
    "        return global_cache[img_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cells_fn(x):\n",
    "    images = []\n",
    "    # For each full image, extract cell images\n",
    "    image = x[0]\n",
    "    masks = x[1]\n",
    "    transform = x[2]\n",
    "\n",
    "    for rle_string in masks:\n",
    "        cell_mask = rle_to_mask(rle_string, crop_size, crop_size)\n",
    "        # Important: set 255 to 1\n",
    "        cell_mask[cell_mask > 0] = 1\n",
    "\n",
    "        cell_image = np.copy(image)\n",
    "        for i in range(4):\n",
    "            cell_image[..., i] = cell_image[..., i] * cell_mask\n",
    "\n",
    "        if transform is not None:\n",
    "            cell_image = transform(cell_image)\n",
    "\n",
    "        cell_image = image_to_tensor(cell_image)\n",
    "        images.append(cell_image.unsqueeze(0))\n",
    "\n",
    "    images = torch.cat(images)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, seed, fold=0, generate_meta=False):\n",
    "    all_probs = []\n",
    "    all_meta = {}\n",
    "\n",
    "    seed_everything(rand_seed + 1000 * seed)\n",
    "    network_path = f\"{bestfitting_folder}/{model_folder}/fold{fold}/final.pth\"\n",
    "    model = load_model(network_path, args)\n",
    "\n",
    "    global_processed = 0\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        id = row[\"ID\"]\n",
    "        width = row[\"ImageWidth\"]\n",
    "        height = row[\"ImageHeight\"]\n",
    "\n",
    "        # Data augmentation\n",
    "        aug_probs = None\n",
    "        for augment in augment_list:\n",
    "            transform = eval(f\"augment_{augment}\")\n",
    "            images, masks = process_image(id, test_image_folder)\n",
    "            images, masks = collate_cells_fn((images, masks, transform))\n",
    "\n",
    "            cell_probs = []\n",
    "            processed_count = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_i in range(0, len(masks), batch_size):\n",
    "                    batch_images = images[batch_i:batch_i + batch_size, ...]\n",
    "                    # batch_images = Variable(batch_images.cuda(), volatile=True)\n",
    "                    outputs = model(batch_images.cuda())\n",
    "                    logits = outputs\n",
    "\n",
    "                    probs = F.sigmoid(logits).data\n",
    "                    probs = probs.detach().cpu().numpy().tolist()\n",
    "                    cell_probs += probs\n",
    "\n",
    "                    processed_count += len(probs)\n",
    "\n",
    "            cell_probs = np.array(cell_probs).reshape(processed_count, -1)\n",
    "\n",
    "            if aug_probs is None:\n",
    "                aug_probs = cell_probs / len(augment_list)\n",
    "            else:\n",
    "                aug_probs += cell_probs / len(augment_list)\n",
    "\n",
    "        all_probs.append(aug_probs)\n",
    "\n",
    "        if generate_meta:\n",
    "            masks = np.array(masks)\n",
    "\n",
    "            if masks.shape[0] > 0:\n",
    "                # Generate RLE string for each cell mask\n",
    "                # https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook?scriptVersionId=55714434\n",
    "                submit_strings = []\n",
    "                for i in range(masks.shape[0]):\n",
    "                    mask = masks[i]\n",
    "                    mask = rle_to_mask(mask, crop_size, crop_size)\n",
    "                    # Important: set 255 to 1\n",
    "                    mask[mask > 0] = 1\n",
    "\n",
    "                    # Important: resize to orignal resolution to submit correct mask RLE string\n",
    "                    # https://www.kaggle.com/linshokaku/faster-hpa-cell-segmentation/comments#1251082\n",
    "                    mask = cv2.resize(mask, (width, height),\n",
    "                                      interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    rle_string = binary_mask_to_ascii(mask, mask_val=1)\n",
    "                    submit_strings.append(rle_string)\n",
    "\n",
    "                if len(submit_strings) > 0:\n",
    "                    all_meta[id] = submit_strings\n",
    "                else:\n",
    "                    all_meta[id] = []\n",
    "            else:\n",
    "                all_meta[id] = []\n",
    "\n",
    "        del images, masks, aug_probs, cell_probs, batch_images\n",
    "        gc.collect()\n",
    "\n",
    "        if debug and global_processed == batch_size - 1:\n",
    "            break\n",
    "\n",
    "        global_processed += 1\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if generate_meta:\n",
    "        return all_probs, all_meta\n",
    "    else:\n",
    "        return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "final_probs = []\n",
    "final_meta = {}\n",
    "\n",
    "augment_list = [\n",
    "    'default', 'flipud', 'fliplr', 'transpose', 'flipud_lr',\n",
    "    'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose'\n",
    "]\n",
    "seeds = [0, 1, 2, 3]\n",
    "fold = 0\n",
    "\n",
    "cache_size = batch_size if debug else 250\n",
    "global_cache = {}\n",
    "batch_rounds = 0\n",
    "for batch_i in range(0, submit_df.shape[0], cache_size):\n",
    "    print(f\"[Batch Processing {batch_rounds}]\")\n",
    "    sub_df = submit_df.iloc[batch_i:batch_i + cache_size, :].copy()\n",
    "    print(sub_df.shape)\n",
    "\n",
    "    batch_probs = [0] * sub_df.shape[0]\n",
    "    for i, s in enumerate(seeds):\n",
    "        print(f\"Inferencing with seed {rand_seed+1000*s} ......\")\n",
    "\n",
    "        if i == 0:\n",
    "            seed_probs, meta = predict(sub_df,\n",
    "                                       s,\n",
    "                                       fold=fold,\n",
    "                                       generate_meta=True)\n",
    "            print(len(seed_probs), len(batch_probs), sub_df.shape[0])\n",
    "            for j in range(len(seed_probs)):\n",
    "                batch_probs[j] = seed_probs[j] / len(seeds)\n",
    "            final_meta.update(meta)\n",
    "        else:\n",
    "            seed_probs = predict(sub_df, s, fold=fold)\n",
    "            print(len(seed_probs), len(batch_probs), sub_df.shape[0])\n",
    "            for j in range(len(seed_probs)):\n",
    "                batch_probs[j] += seed_probs[j] / len(seeds)\n",
    "\n",
    "    if batch_probs is not None:\n",
    "        final_probs.extend(batch_probs)\n",
    "\n",
    "    if debug:\n",
    "        break\n",
    "\n",
    "    # Reset cache\n",
    "    del global_cache\n",
    "    gc.collect()\n",
    "    global_cache = {}\n",
    "    batch_rounds += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment_list = [\n",
    "#     'default', 'flipud', 'fliplr', 'transpose', 'flipud_lr',\n",
    "#     'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose'\n",
    "# ]\n",
    "# seeds = [0, 1, 2, 3]\n",
    "# for seed in seeds:\n",
    "#     seed_everything(seed)\n",
    "\n",
    "#     for augment in augment_list:\n",
    "#         transform = eval(f\"augment_{augment}\")\n",
    "#         if args.crop_size > 0:\n",
    "#             sub_submit_out_dir = opj(submit_out_dir,\n",
    "#                                      '%s_seed%d' % (augment, seed))\n",
    "#         else:\n",
    "#             sub_submit_out_dir = opj(submit_out_dir, augment)\n",
    "#         if not ope(sub_submit_out_dir):\n",
    "#             os.makedirs(sub_submit_out_dir)\n",
    "#         with torch.no_grad():\n",
    "#             predict(test_loader, model, sub_submit_out_dir, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "for index, row in tqdm(submit_df.iterrows(), total=submit_df.shape[0]):\n",
    "    id = row[\"ID\"]\n",
    "    width = row[\"ImageWidth\"]\n",
    "    height = row[\"ImageHeight\"]\n",
    "\n",
    "    cell_probs = final_probs[index]\n",
    "    rle_strings = final_meta[id]\n",
    "\n",
    "    new_preds = np.zeros((cell_probs.shape[0], 19))\n",
    "    for i in range(cell_probs.shape[0]):\n",
    "        for j in range(28):\n",
    "            new_class_i = old_class_mappings[j]\n",
    "            # Take maximum prob.\n",
    "            if cell_probs[i, j] > new_preds[i, new_class_i]:\n",
    "                new_preds[i, new_class_i] = cell_probs[i, j]\n",
    "\n",
    "    submit_strings = []\n",
    "    for i in range(new_preds.shape[0]):\n",
    "        confidence = new_preds[i, ...]\n",
    "        rle_string = rle_strings[i]\n",
    "        for l in range(19):\n",
    "            submit_strings.append(f\"{l} {confidence[l]:.6f} {rle_string}\")\n",
    "\n",
    "    if len(submit_strings) > 0:\n",
    "        all_predictions.append(\" \".join(submit_strings))\n",
    "    else:\n",
    "        all_predictions.append(\"\")\n",
    "\n",
    "    if debug and index == batch_size - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    submit_df.iloc[:batch_size, :][\"PredictionString\"] = all_predictions\n",
    "else:\n",
    "    submit_df[\"PredictionString\"] = all_predictions\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_result(probs, img_ids, th=0.5):\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    predicted_probs = probs.copy()\n",
    "    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n",
    "\n",
    "    pred_list = []\n",
    "    pred_list_new = []\n",
    "    for line in probs:\n",
    "        # Map old classes to new ones\n",
    "        predicted_old_classes = sorted(\n",
    "            list(set([i for i in np.nonzero(line > th)[0]])))\n",
    "        predicted_new_classes = sorted(\n",
    "            list(set([old_class_mappings[i]\n",
    "                      for i in np.nonzero(line > th)[0]])))\n",
    "        # print(predicted_classes)\n",
    "        s = '|'.join([str(i) for i in predicted_old_classes])\n",
    "        s_new = '|'.join([str(i) for i in predicted_new_classes])\n",
    "        pred_list.append(s)\n",
    "        pred_list_new.append(s_new)\n",
    "    result_df = pd.DataFrame({\n",
    "        # \"ID\": img_ids,\n",
    "        \"Predicted\": pred_list,\n",
    "        \"Predicted_New\": pred_list_new\n",
    "    })\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = prob_to_result(final_probs, None, th=confidence_threshold)\n",
    "result_df.to_csv(\"result_comparison.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df[\"Predicted_New\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset.release_gpu()\n",
    "# del model, test_dataset, test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm densenet121-a639ec97.pth hpa_cell_segment.py\n",
    "# !rm -rf inference test_cell_masks\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

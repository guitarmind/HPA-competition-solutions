{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[V1]\n",
    "* Resolution: Resized to 512x512 from 768x768\n",
    "* Extract cell masks and create individual cell images (512x512)\n",
    "* No random crop\n",
    "* No TTA\n",
    "* Update normalization mean and std with 2021 training and test sets\n",
    "\n",
    "[V2]\n",
    "* Use INTER_AREA for resize\n",
    "\n",
    "[V3]\n",
    "* Refactor to use less memory\n",
    "\n",
    "[V4]\n",
    "* Run batch cell segmentator from script\n",
    "\n",
    "[V5]\n",
    "* Add data augmentation\n",
    "\n",
    "[V6]\n",
    "* Faster HPA-Cell-Segmentatior (resize to 512x512)\n",
    "\n",
    "[V7]\n",
    "* 5 folds ensemble with 1 seed (avg prob.)\n",
    "\n",
    "Note: HPA-Cell-Segmentatior assume that all input images are of the same shape!\n",
    "\"\"\"\n",
    "\n",
    "kernel_mode = False\n",
    "debug = True\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/hpa-bestfitting-solution/src\")\n",
    "    sys.path.insert(0, \"../input/hpa-cell-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../input/hpa-bestfitting-solution/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ../input/hpa-bestfitting-solution/densenet121-a639ec97.pth .\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n",
    "# !pip install -q \"../input/hpapytorchzoozip/pytorch_zoo-master\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on 2fb1688be1a5\n",
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from config.config import *\n",
    "from utils.common_util import *\n",
    "from networks.imageclsnet import init_network\n",
    "from datasets.protein_dataset import ProteinDataset\n",
    "from utils.augment_util import *\n",
    "from datasets.tool import *\n",
    "\n",
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "from pycocotools import _mask as coco_mask\n",
    "import typing as t\n",
    "import base64\n",
    "import zlib\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if kernel_mode:\n",
    "    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "    bestfitting_folder = \"/kaggle/input/hpa-bestfitting-solution\"\n",
    "    test_image_folder = f\"{dataset_folder}/test/\"\n",
    "    cell_mask_folder = \"/kaggle/working/test_cell_masks\"\n",
    "    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "    bestfitting_folder = \"/workspace/Github/HPA-competition-solutions/bestfitting\"\n",
    "    test_image_folder = f\"{dataset_folder}/test/\"\n",
    "    cell_mask_folder = f\"{dataset_folder}/test_cell_masks\"\n",
    "    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "\n",
    "model_folder = \"external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds\"\n",
    "\n",
    "# image_size = 2048\n",
    "# image_size = 768\n",
    "crop_size = 512\n",
    "\n",
    "batch_size = 512 if kernel_mode else 256\n",
    "if debug:\n",
    "    batch_size = 4\n",
    "# batch_size = 6 if kernel_mode else 4\n",
    "# batch_size = 4\n",
    "num_workers = 2 if kernel_mode else 3\n",
    "\n",
    "# scale_factor = 1.0\n",
    "# scale_factor = 0.1\n",
    "scale_factor = 0.25\n",
    "confidence_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 17, 9: 17, 10: 17, 11: 8, 12: 9, 13: 9, 14: 10, 15: 18, 16: 18, 17: 11, 18: 18, 19: 12, 20: 17, 21: 13, 22: 18, 23: 14, 24: 15, 25: 16, 26: 17, 27: 18}\n"
     ]
    }
   ],
   "source": [
    "old_classes = {\n",
    "    0: 'Nucleoplasm',\n",
    "    1: 'Nuclear membrane',\n",
    "    2: 'Nucleoli',\n",
    "    3: 'Nucleoli fibrillar center',\n",
    "    4: 'Nuclear speckles',\n",
    "    5: 'Nuclear bodies',\n",
    "    6: 'Endoplasmic reticulum',\n",
    "    7: 'Golgi apparatus',\n",
    "    8: 'Peroxisomes',\n",
    "    9: 'Endosomes',\n",
    "    10: 'Lysosomes',\n",
    "    11: 'Intermediate filaments',\n",
    "    12: 'Actin filaments',\n",
    "    13: 'Focal adhesion sites',\n",
    "    14: 'Microtubules',\n",
    "    15: 'Microtubule ends',\n",
    "    16: 'Cytokinetic bridge',\n",
    "    17: 'Mitotic spindle',\n",
    "    18: 'Microtubule organizing center',\n",
    "    19: 'Centrosome',\n",
    "    20: 'Lipid droplets',\n",
    "    21: 'Plasma membrane',\n",
    "    22: 'Cell junctions',\n",
    "    23: 'Mitochondria',\n",
    "    24: 'Aggresome',\n",
    "    25: 'Cytosol',\n",
    "    26: 'Cytoplasmic bodies',\n",
    "    27: 'Rods & rings'\n",
    "}\n",
    "old_class_indices = {v: k for k, v in old_classes.items()}\n",
    "\n",
    "# All label names in the public HPA and their corresponding index.\n",
    "all_locations = dict({\n",
    "    \"Nucleoplasm\": 0,\n",
    "    \"Nuclear membrane\": 1,\n",
    "    \"Nucleoli\": 2,\n",
    "    \"Nucleoli fibrillar center\": 3,\n",
    "    \"Nuclear speckles\": 4,\n",
    "    \"Nuclear bodies\": 5,\n",
    "    \"Endoplasmic reticulum\": 6,\n",
    "    \"Golgi apparatus\": 7,\n",
    "    \"Intermediate filaments\": 8,\n",
    "    \"Actin filaments\": 9,\n",
    "    \"Focal adhesion sites\": 9,\n",
    "    \"Microtubules\": 10,\n",
    "    \"Mitotic spindle\": 11,\n",
    "    \"Centrosome\": 12,\n",
    "    \"Centriolar satellite\": 12,\n",
    "    \"Plasma membrane\": 13,\n",
    "    \"Cell Junctions\": 13,\n",
    "    \"Mitochondria\": 14,\n",
    "    \"Aggresome\": 15,\n",
    "    \"Cytosol\": 16,\n",
    "    \"Vesicles\": 17,\n",
    "    \"Peroxisomes\": 17,\n",
    "    \"Endosomes\": 17,\n",
    "    \"Lysosomes\": 17,\n",
    "    \"Lipid droplets\": 17,\n",
    "    \"Cytoplasmic bodies\": 17,\n",
    "    \"Rods & rings\": 18,\n",
    "    # markpeng\n",
    "    \"No staining\": 18,\n",
    "})\n",
    "\n",
    "old_class_mappings = {}\n",
    "for i, (k, v) in enumerate(old_class_indices.items()):\n",
    "    if k in all_locations:\n",
    "        old_class_mappings[v] = all_locations[k]\n",
    "    else:\n",
    "        # No staining\n",
    "        old_class_mappings[v] = 18\n",
    "assert len(old_class_mappings.values()) == len(old_classes.values())\n",
    "print(old_class_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\t       test\t\ttest_tfrecords\ttrain.csv\r\n",
      "sample_submission.csv  test_cell_masks\ttrain\t\ttrain_tfrecords\r\n"
     ]
    }
   ],
   "source": [
    "!ls {dataset_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{dataset_folder}/train.csv\")\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>8|5|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>14|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60b57878-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>6|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>16|10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5b931256-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>14|0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID  Label\n",
       "0  5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0  8|5|0\n",
       "1  5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0   14|0\n",
       "2  60b57878-bb99-11e8-b2b9-ac1f6b6435d0    6|1\n",
       "3  5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0  16|10\n",
       "4  5b931256-bb99-11e8-b2b9-ac1f6b6435d0   14|0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 4) 1728 3072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ImageWidth</th>\n",
       "      <th>ImageHeight</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0040581b-f1f2-4fbe-b043-b6bfea5404bb</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004a270d-34a2-4d60-bbe4-365fca868193</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00537262-883c-4b37-a3a1-a4931b6faea5</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c9a1c9-2f06-476f-8b0d-6d01032874a2</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0173029a-161d-40ef-af28-2342915b22fb</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID  ImageWidth  ImageHeight  \\\n",
       "0  0040581b-f1f2-4fbe-b043-b6bfea5404bb        2048         2048   \n",
       "1  004a270d-34a2-4d60-bbe4-365fca868193        2048         2048   \n",
       "2  00537262-883c-4b37-a3a1-a4931b6faea5        2048         2048   \n",
       "3  00c9a1c9-2f06-476f-8b0d-6d01032874a2        2048         2048   \n",
       "4  0173029a-161d-40ef-af28-2342915b22fb        3072         3072   \n",
       "\n",
       "           PredictionString  \n",
       "0  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "1  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "2  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "3  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "4  0 1 eNoLCAgIsAQABJ4Beg==  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submit_df.shape, submit_df.ImageWidth.min(), submit_df.ImageWidth.max())\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n"
     ]
    }
   ],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\"]\n",
    "\n",
    "test_ids = submit_df[\"ID\"].values.tolist()\n",
    "print(len(test_ids))\n",
    "\n",
    "# Estimated number of private test images (RGBY): 2236 x 2.3 ~= 5143 (for 9 hours we have 6.2 secs per image)\n",
    "# Estimated number of private test images: 559 x 2.3 ~= 1286 (for 9 hours we have 25.2 secs per image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook\n",
    "def binary_mask_to_ascii(mask, mask_val=1):\n",
    "    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "    mask = np.where(mask == mask_val, 1, 0).astype(np.bool)\n",
    "\n",
    "    # check input mask --\n",
    "    if mask.dtype != np.bool:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a binary mask, received dtype == {mask.dtype}\"\n",
    "        )\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a 2d mask, received shape == {mask.shape}\"\n",
    "        )\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str.decode()\n",
    "\n",
    "\n",
    "def rle_encoding(img, mask_val=1):\n",
    "    \"\"\"\n",
    "    Turns our masks into RLE encoding to easily store them\n",
    "    and feed them into models later on\n",
    "    https://en.wikipedia.org/wiki/Run-length_encoding\n",
    "    \n",
    "    Args:\n",
    "        img (np.array): Segmentation array\n",
    "        mask_val (int): Which value to use to create the RLE\n",
    "        \n",
    "    Returns:\n",
    "        RLE string\n",
    "    \n",
    "    \"\"\"\n",
    "    dots = np.where(img.T.flatten() == mask_val)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "\n",
    "    return ' '.join([str(x) for x in run_lengths])\n",
    "\n",
    "\n",
    "def rle_to_mask(rle_string, height, width):\n",
    "    \"\"\" Convert RLE sttring into a binary mask \n",
    "    \n",
    "    Args:\n",
    "        rle_string (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of the binary segmentation mask for a given cell\n",
    "    \"\"\"\n",
    "    rows, cols = height, width\n",
    "    rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n",
    "    rle_pairs = np.array(rle_numbers).reshape(-1, 2)\n",
    "    img = np.zeros(rows * cols, dtype=np.uint8)\n",
    "    for index, length in rle_pairs:\n",
    "        index -= 1\n",
    "        img[index:index + length] = 255\n",
    "    img = img.reshape(cols, rows)\n",
    "    img = img.T\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_segmentation_maps(list_of_image_lists, segmentator, batch_size=8):\n",
    "    \"\"\" Function to generate segmentation maps using CellSegmentator tool \n",
    "    \n",
    "    Args:\n",
    "        list_of_image_lists (list of lists):\n",
    "            - [[micro-tubules(red)], [endoplasmic-reticulum(yellow)], [nucleus(blue)]]\n",
    "        batch_size (int): Batch size to use in generating the segmentation masks\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing RLEs for all the cells in all images\n",
    "    \"\"\"\n",
    "\n",
    "    all_mask_rles = {}\n",
    "    for i in tqdm(range(0, len(list_of_image_lists[0]), batch_size),\n",
    "                  total=len(list_of_image_lists[0]) // batch_size):\n",
    "\n",
    "        # Get batch of images\n",
    "        sub_images = [\n",
    "            img_channel_list[i:i + batch_size]\n",
    "            for img_channel_list in list_of_image_lists\n",
    "        ]  # 0.000001 seconds\n",
    "\n",
    "        # Do segmentation\n",
    "        cell_segmentations = segmentator.pred_cells(sub_images)\n",
    "        nuc_segmentations = segmentator.pred_nuclei(sub_images[2])\n",
    "\n",
    "        # post-processing\n",
    "        for j, path in enumerate(sub_images[0]):\n",
    "            img_id = path.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1]\n",
    "            nuc_mask, cell_mask = label_cell(nuc_segmentations[j],\n",
    "                                             cell_segmentations[j])\n",
    "            new_name = os.path.basename(path).replace('red', 'mask')\n",
    "            all_mask_rles[img_id] = [\n",
    "                rle_encoding(cell_mask, mask_val=k)\n",
    "                for k in range(1,\n",
    "                               np.max(cell_mask) + 1)\n",
    "            ]\n",
    "    return all_mask_rles\n",
    "\n",
    "\n",
    "def get_img_list(img_dir, return_ids=False, sub_n=None):\n",
    "    \"\"\" Get image list in the format expected by the CellSegmentator tool \"\"\"\n",
    "    if sub_n is None:\n",
    "        sub_n = len(glob(img_dir + '/' + f'*_red.png'))\n",
    "    if return_ids:\n",
    "        images = [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "        return [\n",
    "            x.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1] for x in images[0]\n",
    "        ], images\n",
    "    else:\n",
    "        return [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_rle(\n",
    "    rle,\n",
    "    width,\n",
    "    height,\n",
    "    return_mask=True,\n",
    "):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        rle (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    mask = rle_to_mask(rle, height, width).copy()\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    x, y, w, h = cv2.boundingRect(cnts[0])\n",
    "\n",
    "    if return_mask:\n",
    "        return (x, y, x + w, y + h), mask\n",
    "    else:\n",
    "        return (x, y, x + w, y + h)\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_raw(raw_mask):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        raw_mask (nparray): Numpy array containing segmentation mask information\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(raw_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    xywhs = [cv2.boundingRect(cnt) for cnt in cnts]\n",
    "    xys = [(xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3])\n",
    "           for xywh in xywhs]\n",
    "    return sorted(xys, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "\n",
    "def pad_to_square(a):\n",
    "    \"\"\" Pad an array `a` evenly until it is a square \"\"\"\n",
    "    if a.shape[1] > a.shape[0]:  # pad height\n",
    "        n_to_add = a.shape[1] - a.shape[0]\n",
    "        top_pad = n_to_add // 2\n",
    "        bottom_pad = n_to_add - top_pad\n",
    "        a = np.pad(a, [(top_pad, bottom_pad), (0, 0), (0, 0)], mode='constant')\n",
    "\n",
    "    elif a.shape[0] > a.shape[1]:  # pad width\n",
    "        n_to_add = a.shape[0] - a.shape[1]\n",
    "        left_pad = n_to_add // 2\n",
    "        right_pad = n_to_add - left_pad\n",
    "        a = np.pad(a, [(0, 0), (left_pad, right_pad), (0, 0)], mode='constant')\n",
    "    else:\n",
    "        pass\n",
    "    return a\n",
    "\n",
    "\n",
    "def cut_out_cells(rgby,\n",
    "                  rles,\n",
    "                  resize_to=(256, 256),\n",
    "                  square_off=True,\n",
    "                  return_masks=False,\n",
    "                  from_raw=True):\n",
    "    \"\"\" Cut out the cells as padded square images \n",
    "    \n",
    "    Args:\n",
    "        rgby (np.array): 4 Channel image to be cut into tiles\n",
    "        rles (list of RLE strings): List of run length encoding containing \n",
    "            segmentation mask information\n",
    "        resize_to (tuple of ints, optional): The square dimension to resize the image to\n",
    "        square_off (bool, optional): Whether to pad the image to a square or not\n",
    "        \n",
    "    Returns:\n",
    "        list of square arrays representing squared off cell images\n",
    "    \"\"\"\n",
    "    w, h = rgby.shape[:2]\n",
    "    contour_bboxes = [\n",
    "        get_contour_bbox(rle, w, h, return_mask=return_masks) for rle in rles\n",
    "    ]\n",
    "    if return_masks:\n",
    "        masks = [x[-1] for x in contour_bboxes]\n",
    "        contour_bboxes = [x[:-1] for x in contour_bboxes]\n",
    "\n",
    "    arrs = [\n",
    "        rgby[bbox[1]:bbox[3], bbox[0]:bbox[2], ...] for bbox in contour_bboxes\n",
    "    ]\n",
    "    if square_off:\n",
    "        arrs = [pad_to_square(arr) for arr in arrs]\n",
    "\n",
    "    if resize_to is not None:\n",
    "        arrs = [\n",
    "            cv2.resize(pad_to_square(arr).astype(np.float32),\n",
    "                       resize_to,\n",
    "                       interpolation=cv2.INTER_CUBIC) \\\n",
    "            for arr in arrs\n",
    "        ]\n",
    "    if return_masks:\n",
    "        return arrs, masks\n",
    "    else:\n",
    "        return arrs\n",
    "\n",
    "\n",
    "def grab_contours(cnts):\n",
    "    # if the length the contours tuple returned by cv2.findContours\n",
    "    # is '2' then we are using either OpenCV v2.4, v4-beta, or\n",
    "    # v4-official\n",
    "    if len(cnts) == 2:\n",
    "        cnts = cnts[0]\n",
    "\n",
    "    # if the length of the contours tuple is '3' then we are using\n",
    "    # either OpenCV v3, v4-pre, or v4-alpha\n",
    "    elif len(cnts) == 3:\n",
    "        cnts = cnts[1]\n",
    "\n",
    "    # otherwise OpenCV has changed their cv2.findContours return\n",
    "    # signature yet again and I have no idea WTH is going on\n",
    "    else:\n",
    "        raise Exception(\n",
    "            (\"Contours tuple must have length 2 or 3, \"\n",
    "             \"otherwise OpenCV changed their cv2.findContours return \"\n",
    "             \"signature yet again. Refer to OpenCV's documentation \"\n",
    "             \"in that case\"))\n",
    "\n",
    "    # return the actual contours array\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/72534\n",
    "def generate_hash(img_dir,\n",
    "                  colors,\n",
    "                  dataset='train',\n",
    "                  imread_func=None,\n",
    "                  is_update=False):\n",
    "    meta = meta.copy()\n",
    "    hash_maps = {}\n",
    "    for color in colors:\n",
    "        hash_maps[color] = []\n",
    "        for idx in tqdm(range(len(meta)), desc='train %s' % color):\n",
    "            img = imread_func(img_dir, meta.iloc[idx][ID], color)\n",
    "            hash = imagehash.phash(img)\n",
    "            hash_maps[color].append(hash)\n",
    "\n",
    "    for color in colors:\n",
    "        meta[color] = hash_maps[color]\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def calc_hash(params):\n",
    "    color, threshold, base_test_hash1, base_test_hash2, test_ids1, test_ids2 = params\n",
    "\n",
    "    test_hash1 = base_test_hash1.reshape(1, -1)  # 1*m\n",
    "\n",
    "    test_idxes_list1 = []\n",
    "    test_idxes_list2 = []\n",
    "    hash_list = []\n",
    "\n",
    "    step = 5\n",
    "    for test_idx in tqdm(range(0, len(base_test_hash2), step), desc=color):\n",
    "        test_hash2 = base_test_hash2[test_idx:test_idx + step].reshape(\n",
    "            -1, 1)  # n*1\n",
    "        hash = test_hash2 - test_hash1  # n*m\n",
    "        test_idxes2, test_idxes1 = np.where(hash <= threshold)\n",
    "        hash = hash[test_idxes2, test_idxes1]\n",
    "\n",
    "        test_idxes2 = test_idxes2 + test_idx\n",
    "\n",
    "        test_idxes_list1.extend(test_idxes1.tolist())\n",
    "        test_idxes_list2.extend(test_idxes2.tolist())\n",
    "        hash_list.extend(hash.tolist())\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Test1': test_ids1[test_idxes_list1],\n",
    "        'Test2': test_ids2[test_idxes_list2],\n",
    "        'Sim%s' % color[:1].upper(): hash_list\n",
    "    })\n",
    "    df = df[df['Test1'] != df['Test2']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Cell Segmentations as Numpy Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hpa_cell_segment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hpa_cell_segment.py\n",
    "\n",
    "kernel_mode = False\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/hpa-bestfitting-solution/src\")\n",
    "    sys.path.insert(0, \"../input/hpa-cell-segmentation\")\n",
    "\n",
    "from hpacellseg.cellsegmentator import *\n",
    "\n",
    "target_image_size = 512\n",
    "\n",
    "IMAGE_SIZES = [1728, 2048, 3072, 4096]\n",
    "if kernel_mode:\n",
    "    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n",
    "    img_dir = f\"{dataset_folder}/test\"\n",
    "    output_folder = \"/kaggle/working/test_cell_masks\"\n",
    "    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "    BATCH_SIZE = {1728: 24, 2048: 22, 3072: 12, 4096: 12}\n",
    "else:\n",
    "    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "    img_dir = f\"{dataset_folder}/test\"\n",
    "    output_folder = f\"{dataset_folder}/test_cell_masks\"\n",
    "    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "    #     BATCH_SIZE = {1728: 24, 2048: 24, 3072: 12, 4096: 12}\n",
    "    BATCH_SIZE = {1728: 20, 2048: 18, 3072: 8, 4096: 8}\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")\n",
    "\n",
    "predict_df_1728 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[0]]\n",
    "predict_df_2048 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[1]]\n",
    "predict_df_3072 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[2]]\n",
    "predict_df_4096 = submit_df[submit_df.ImageWidth == IMAGE_SIZES[3]]\n",
    "\n",
    "predict_ids_1728 = predict_df_1728.ID.to_list()\n",
    "predict_ids_2048 = predict_df_2048.ID.to_list()\n",
    "predict_ids_3072 = predict_df_3072.ID.to_list()\n",
    "predict_ids_4096 = predict_df_4096.ID.to_list()\n",
    "\n",
    "\n",
    "class CellSegmentator(object):\n",
    "    \"\"\"Uses pretrained DPN-Unet models to segment cells from images.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        nuclei_model=\"../input/hpacellsegmentatormodelweights/dpn_unet_nuclei_v1.pth\",\n",
    "        cell_model=\"../input/hpacellsegmentatormodelweights/dpn_unet_cell_3ch_v1.pth\",\n",
    "        scale_factor=1.0,\n",
    "        device=\"cuda\",\n",
    "        padding=False,\n",
    "        multi_channel_model=True,\n",
    "    ):\n",
    "        \"\"\"Class for segmenting nuclei and whole cells from confocal microscopy images.\n",
    "        It takes lists of images and returns the raw output from the\n",
    "        specified segmentation model. Models can be automatically\n",
    "        downloaded if they are not already available on the system.\n",
    "        When working with images from the Huan Protein Cell atlas, the\n",
    "        outputs from this class' methods are well combined with the\n",
    "        label functions in the utils module.\n",
    "        Note that for cell segmentation, there are two possible models\n",
    "        available. One that works with 2 channeled images and one that\n",
    "        takes 3 channels.\n",
    "        Keyword arguments:\n",
    "        nuclei_model -- A loaded torch nuclei segmentation model or the\n",
    "                        path to a file which contains such a model.\n",
    "                        If the argument is a path that points to a non-existant file,\n",
    "                        a pretrained nuclei_model is going to get downloaded to the\n",
    "                        specified path (default: './nuclei_model.pth').\n",
    "        cell_model -- A loaded torch cell segmentation model or the\n",
    "                      path to a file which contains such a model.\n",
    "                      The cell_model argument can be None if only nuclei\n",
    "                      are to be segmented (default: './cell_model.pth').\n",
    "        scale_factor -- How much to scale images before they are fed to\n",
    "                        segmentation models. Segmentations will be scaled back\n",
    "                        up by 1/scale_factor to match the original image\n",
    "                        (default: 0.25).\n",
    "        device -- The device on which to run the models.\n",
    "                  This should either be 'cpu' or 'cuda' or pointed cuda\n",
    "                  device like 'cuda:0' (default: 'cuda').\n",
    "        padding -- Whether to add padding to the images before feeding the\n",
    "                   images to the network. (default: False).\n",
    "        multi_channel_model -- Control whether to use the 3-channel cell model or not.\n",
    "                               If True, use the 3-channel model, otherwise use the\n",
    "                               2-channel version (default: True).\n",
    "        \"\"\"\n",
    "        if device != \"cuda\" and device != \"cpu\" and \"cuda\" not in device:\n",
    "            raise ValueError(f\"{device} is not a valid device (cuda/cpu)\")\n",
    "        if device != \"cpu\":\n",
    "            try:\n",
    "                assert torch.cuda.is_available()\n",
    "            except AssertionError:\n",
    "                print(\"No GPU found, using CPU.\", file=sys.stderr)\n",
    "                device = \"cpu\"\n",
    "        self.device = device\n",
    "\n",
    "        if isinstance(nuclei_model, str):\n",
    "            if not os.path.exists(nuclei_model):\n",
    "                print(\n",
    "                    f\"Could not find {nuclei_model}. Downloading it now\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "                download_with_url(NUCLEI_MODEL_URL, nuclei_model)\n",
    "            nuclei_model = torch.load(nuclei_model,\n",
    "                                      map_location=torch.device(self.device))\n",
    "        if isinstance(nuclei_model, torch.nn.DataParallel) and device == \"cpu\":\n",
    "            nuclei_model = nuclei_model.module\n",
    "\n",
    "        self.nuclei_model = nuclei_model.to(self.device).eval()\n",
    "\n",
    "        self.multi_channel_model = multi_channel_model\n",
    "        if isinstance(cell_model, str):\n",
    "            if not os.path.exists(cell_model):\n",
    "                print(f\"Could not find {cell_model}. Downloading it now\",\n",
    "                      file=sys.stderr)\n",
    "                if self.multi_channel_model:\n",
    "                    download_with_url(MULTI_CHANNEL_CELL_MODEL_URL, cell_model)\n",
    "                else:\n",
    "                    download_with_url(TWO_CHANNEL_CELL_MODEL_URL, cell_model)\n",
    "            cell_model = torch.load(cell_model,\n",
    "                                    map_location=torch.device(self.device))\n",
    "        self.cell_model = cell_model.to(self.device).eval()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.padding = padding\n",
    "\n",
    "    def _image_conversion(self, images):\n",
    "        \"\"\"Convert/Format images to RGB image arrays list for cell predictions.\n",
    "        Intended for internal use only.\n",
    "        Keyword arguments:\n",
    "        images -- list of lists of image paths/arrays. It should following the\n",
    "                 pattern if with er channel input,\n",
    "                 [\n",
    "                     [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                     [er_path0/image_array0, er_path1/image_array1, ...],\n",
    "                     [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                 ]\n",
    "                 or if without er input,\n",
    "                 [\n",
    "                     [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                     None,\n",
    "                     [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                 ]\n",
    "        \"\"\"\n",
    "        microtubule_imgs, er_imgs, nuclei_imgs = images\n",
    "        if self.multi_channel_model:\n",
    "            if not isinstance(er_imgs, list):\n",
    "                raise ValueError(\n",
    "                    \"Please speicify the image path(s) for er channels!\")\n",
    "        else:\n",
    "            if not er_imgs is None:\n",
    "                raise ValueError(\n",
    "                    \"second channel should be None for two channel model predition!\"\n",
    "                )\n",
    "\n",
    "        if not isinstance(microtubule_imgs, list):\n",
    "            raise ValueError(\"The microtubule images should be a list\")\n",
    "        if not isinstance(nuclei_imgs, list):\n",
    "            raise ValueError(\"The microtubule images should be a list\")\n",
    "\n",
    "        if er_imgs:\n",
    "            if not len(microtubule_imgs) == len(er_imgs) == len(nuclei_imgs):\n",
    "                raise ValueError(\n",
    "                    \"The lists of images needs to be the same length\")\n",
    "        else:\n",
    "            if not len(microtubule_imgs) == len(nuclei_imgs):\n",
    "                raise ValueError(\n",
    "                    \"The lists of images needs to be the same length\")\n",
    "\n",
    "        if not all(isinstance(item, np.ndarray) for item in microtubule_imgs):\n",
    "            microtubule_imgs = [\n",
    "                os.path.expanduser(item)\n",
    "                for _, item in enumerate(microtubule_imgs)\n",
    "            ]\n",
    "            nuclei_imgs = [\n",
    "                os.path.expanduser(item) for _, item in enumerate(nuclei_imgs)\n",
    "            ]\n",
    "\n",
    "            microtubule_imgs = list(\n",
    "                map(lambda item: imageio.imread(item), microtubule_imgs))\n",
    "            nuclei_imgs = list(\n",
    "                map(lambda item: imageio.imread(item), nuclei_imgs))\n",
    "            if er_imgs:\n",
    "                er_imgs = [\n",
    "                    os.path.expanduser(item) for _, item in enumerate(er_imgs)\n",
    "                ]\n",
    "                er_imgs = list(map(lambda item: imageio.imread(item), er_imgs))\n",
    "\n",
    "        if not er_imgs:\n",
    "            er_imgs = [\n",
    "                np.zeros(item.shape, dtype=item.dtype)\n",
    "                for _, item in enumerate(microtubule_imgs)\n",
    "            ]\n",
    "        cell_imgs = list(\n",
    "            map(\n",
    "                lambda item: np.dstack((item[0], item[1], item[2])),\n",
    "                list(zip(microtubule_imgs, er_imgs, nuclei_imgs)),\n",
    "            ))\n",
    "\n",
    "        return cell_imgs\n",
    "\n",
    "    def pred_nuclei(self, images, bs=24):\n",
    "        \"\"\"Predict the nuclei segmentation.\n",
    "        Keyword arguments:\n",
    "        images -- A list of image arrays or a list of paths to images.\n",
    "                  If as a list of image arrays, the images could be 2d images\n",
    "                  of nuclei data array only, or must have the nuclei data in\n",
    "                  the blue channel; If as a list of file paths, the images\n",
    "                  could be RGB image files or gray scale nuclei image file\n",
    "                  paths.\n",
    "        Returns:\n",
    "        predictions -- A list of predictions of nuclei segmentation for each nuclei image.\n",
    "        \"\"\"\n",
    "        def _preprocess(image):\n",
    "            if isinstance(image, str):\n",
    "                image = imageio.imread(image)\n",
    "            self.target_shape = image.shape\n",
    "            if len(image.shape) == 2:\n",
    "                image = np.dstack((image, image, image))\n",
    "            image = transform.rescale(image,\n",
    "                                      self.scale_factor,\n",
    "                                      multichannel=True)\n",
    "            nuc_image = np.dstack((image[..., 2], image[..., 2], image[...,\n",
    "                                                                       2]))\n",
    "            if self.padding:\n",
    "                rows, cols = nuc_image.shape[:2]\n",
    "                self.scaled_shape = rows, cols\n",
    "                nuc_image = cv2.copyMakeBorder(\n",
    "                    nuc_image,\n",
    "                    32,\n",
    "                    (32 - rows % 32),\n",
    "                    32,\n",
    "                    (32 - cols % 32),\n",
    "                    cv2.BORDER_REFLECT,\n",
    "                )\n",
    "            nuc_image = nuc_image.transpose([2, 0, 1])\n",
    "            return nuc_image\n",
    "\n",
    "        def _segment_helper(imgs):\n",
    "            with torch.no_grad():\n",
    "                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n",
    "                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n",
    "                imgs = torch.tensor(imgs).float()\n",
    "                imgs = imgs.to(self.device)\n",
    "                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "\n",
    "                imgs = self.nuclei_model(imgs)\n",
    "                imgs = F.softmax(imgs, dim=1)\n",
    "                return imgs\n",
    "\n",
    "        preprocessed_imgs = list(map(_preprocess, images))\n",
    "        predictions = []\n",
    "        for i in range(0, len(preprocessed_imgs), bs):\n",
    "            start = i\n",
    "            end = min(len(preprocessed_imgs), i + bs)\n",
    "            x = preprocessed_imgs[start:end]\n",
    "            pred = _segment_helper(x).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "        predictions = list(np.concatenate(predictions, axis=0))\n",
    "        predictions = map(util.img_as_ubyte, predictions)\n",
    "        predictions = list(map(self._restore_scaling_padding, predictions))\n",
    "        return predictions\n",
    "\n",
    "    def _restore_scaling_padding(self, n_prediction):\n",
    "        \"\"\"Restore an image from scaling and padding.\n",
    "        This method is intended for internal use.\n",
    "        It takes the output from the nuclei model as input.\n",
    "        \"\"\"\n",
    "        n_prediction = n_prediction.transpose([1, 2, 0])\n",
    "        if self.padding:\n",
    "            n_prediction = n_prediction[32:32 + self.scaled_shape[0],\n",
    "                                        32:32 + self.scaled_shape[1], ...]\n",
    "        if not self.scale_factor == 1:\n",
    "            n_prediction[..., 0] = 0\n",
    "            n_prediction = cv2.resize(\n",
    "                n_prediction,\n",
    "                (self.target_shape[0], self.target_shape[1]),\n",
    "                # interpolation=cv2.INTER_AREA,\n",
    "                interpolation=cv2.INTER_NEAREST,\n",
    "            )\n",
    "        return n_prediction\n",
    "\n",
    "    def pred_cells(self, images, precombined=False, bs=24):\n",
    "        \"\"\"Predict the cell segmentation for a list of images.\n",
    "        Keyword arguments:\n",
    "        images -- list of lists of image paths/arrays. It should following the\n",
    "                  pattern if with er channel input,\n",
    "                  [\n",
    "                      [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                      [er_path0/image_array0, er_path1/image_array1, ...],\n",
    "                      [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                  ]\n",
    "                  or if without er input,\n",
    "                  [\n",
    "                      [microtubule_path0/image_array0, microtubule_path1/image_array1, ...],\n",
    "                      None,\n",
    "                      [nuclei_path0/image_array0, nuclei_path1/image_array1, ...]\n",
    "                  ]\n",
    "                  The ER channel is required when multichannel is True\n",
    "                  and required to be None when multichannel is False.\n",
    "                  The images needs to be of the same size.\n",
    "        precombined -- If precombined is True, the list of images is instead supposed to be\n",
    "                       a list of RGB numpy arrays (default: False).\n",
    "        Returns:\n",
    "        predictions -- a list of predictions of cell segmentations.\n",
    "        \"\"\"\n",
    "        def _preprocess(image):\n",
    "            self.target_shape = image.shape\n",
    "            if not len(image.shape) == 3:\n",
    "                raise ValueError(\"image should has 3 channels\")\n",
    "            cell_image = transform.rescale(image,\n",
    "                                           self.scale_factor,\n",
    "                                           multichannel=True)\n",
    "            if self.padding:\n",
    "                rows, cols = cell_image.shape[:2]\n",
    "                self.scaled_shape = rows, cols\n",
    "                cell_image = cv2.copyMakeBorder(\n",
    "                    cell_image,\n",
    "                    32,\n",
    "                    (32 - rows % 32),\n",
    "                    32,\n",
    "                    (32 - cols % 32),\n",
    "                    cv2.BORDER_REFLECT,\n",
    "                )\n",
    "            cell_image = cell_image.transpose([2, 0, 1])\n",
    "            return cell_image\n",
    "\n",
    "        def _segment_helper(imgs):\n",
    "            with torch.no_grad():\n",
    "                mean = torch.as_tensor(NORMALIZE[\"mean\"], device=self.device)\n",
    "                std = torch.as_tensor(NORMALIZE[\"std\"], device=self.device)\n",
    "                imgs = torch.tensor(imgs).float()\n",
    "                imgs = imgs.to(self.device)\n",
    "                imgs = imgs.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "\n",
    "                imgs = self.cell_model(imgs)\n",
    "                imgs = F.softmax(imgs, dim=1)\n",
    "                return imgs\n",
    "\n",
    "        if not precombined:\n",
    "            images = self._image_conversion(images)\n",
    "        preprocessed_imgs = list(map(_preprocess, images))\n",
    "        predictions = []\n",
    "        for i in range(0, len(preprocessed_imgs), bs):\n",
    "            start = i\n",
    "            end = min(len(preprocessed_imgs), i + bs)\n",
    "            x = preprocessed_imgs[start:end]\n",
    "            pred = _segment_helper(x).cpu().numpy()\n",
    "            predictions.append(pred)\n",
    "        predictions = list(np.concatenate(predictions, axis=0))\n",
    "        predictions = map(self._restore_scaling_padding, predictions)\n",
    "        predictions = list(map(util.img_as_ubyte, predictions))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "import os.path\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndi\n",
    "from skimage import filters, measure, segmentation\n",
    "from skimage.morphology import (binary_erosion, closing, disk,\n",
    "                                remove_small_holes, remove_small_objects)\n",
    "\n",
    "HIGH_THRESHOLD = 0.4\n",
    "LOW_THRESHOLD = HIGH_THRESHOLD - 0.25\n",
    "\n",
    "\n",
    "def download_with_url(url_string, file_path, unzip=False):\n",
    "    \"\"\"Download file with a link.\"\"\"\n",
    "    with urllib.request.urlopen(url_string) as response, open(\n",
    "            file_path, \"wb\") as out_file:\n",
    "        data = response.read()  # a `bytes` object\n",
    "        out_file.write(data)\n",
    "\n",
    "    if unzip:\n",
    "        with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(file_path))\n",
    "\n",
    "\n",
    "def __fill_holes(image):\n",
    "    \"\"\"Fill_holes for labelled image, with a unique number.\"\"\"\n",
    "    boundaries = segmentation.find_boundaries(image)\n",
    "    image = np.multiply(image, np.invert(boundaries))\n",
    "    image = ndi.binary_fill_holes(image > 0)\n",
    "    image = ndi.label(image)[0]\n",
    "    return image\n",
    "\n",
    "\n",
    "def label_cell(nuclei_pred, cell_pred):\n",
    "    \"\"\"Label the cells and the nuclei.\n",
    "    Keyword arguments:\n",
    "    nuclei_pred -- a 3D numpy array of a prediction from a nuclei image.\n",
    "    cell_pred -- a 3D numpy array of a prediction from a cell image.\n",
    "    Returns:\n",
    "    A tuple containing:\n",
    "    nuclei-label -- A nuclei mask data array.\n",
    "    cell-label  -- A cell mask data array.\n",
    "    0's in the data arrays indicate background while a continous\n",
    "    strech of a specific number indicates the area for a specific\n",
    "    cell.\n",
    "    The same value in cell mask and nuclei mask refers to the identical cell.\n",
    "    NOTE: The nuclei labeling from this function will be sligthly\n",
    "    different from the values in :func:`label_nuclei` as this version\n",
    "    will use information from the cell-predictions to make better\n",
    "    estimates.\n",
    "    \"\"\"\n",
    "    def __wsh(\n",
    "        mask_img,\n",
    "        threshold,\n",
    "        border_img,\n",
    "        seeds,\n",
    "        threshold_adjustment=0.35,\n",
    "        small_object_size_cutoff=10,\n",
    "    ):\n",
    "        img_copy = np.copy(mask_img)\n",
    "        m = seeds * border_img  # * dt\n",
    "        img_copy[m <= threshold + threshold_adjustment] = 0\n",
    "        img_copy[m > threshold + threshold_adjustment] = 1\n",
    "        img_copy = img_copy.astype(np.bool)\n",
    "        img_copy = remove_small_objects(\n",
    "            img_copy, small_object_size_cutoff).astype(np.uint8)\n",
    "\n",
    "        mask_img[mask_img <= threshold] = 0\n",
    "        mask_img[mask_img > threshold] = 1\n",
    "        mask_img = mask_img.astype(np.bool)\n",
    "        mask_img = remove_small_holes(mask_img, 63)\n",
    "        mask_img = remove_small_objects(mask_img, 1).astype(np.uint8)\n",
    "        markers = ndi.label(img_copy, output=np.uint32)[0]\n",
    "        labeled_array = segmentation.watershed(mask_img,\n",
    "                                               markers,\n",
    "                                               mask=mask_img,\n",
    "                                               watershed_line=True)\n",
    "        return labeled_array\n",
    "\n",
    "    nuclei_label = __wsh(\n",
    "        nuclei_pred[..., 2] / 255.0,\n",
    "        0.4,\n",
    "        1 - (nuclei_pred[..., 1] + cell_pred[..., 1]) / 255.0 > 0.05,\n",
    "        nuclei_pred[..., 2] / 255,\n",
    "        threshold_adjustment=-0.25,\n",
    "        small_object_size_cutoff=32,\n",
    "    )\n",
    "\n",
    "    # for hpa_image, to remove the small pseduo nuclei\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 157)\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    # this is to remove the cell borders' signal from cell mask.\n",
    "    # could use np.logical_and with some revision, to replace this func.\n",
    "    # Tuned for segmentation hpa images\n",
    "    threshold_value = max(\n",
    "        0.22,\n",
    "        filters.threshold_otsu(cell_pred[..., 2] / 255) * 0.5)\n",
    "    # exclude the green area first\n",
    "    cell_region = np.multiply(\n",
    "        cell_pred[..., 2] / 255 > threshold_value,\n",
    "        np.invert(np.asarray(cell_pred[..., 1] / 255 > 0.05, dtype=np.int8)),\n",
    "    )\n",
    "    sk = np.asarray(cell_region, dtype=np.int8)\n",
    "    distance = np.clip(cell_pred[..., 2], 255 * threshold_value, cell_pred[...,\n",
    "                                                                           2])\n",
    "    cell_label = segmentation.watershed(-distance, nuclei_label, mask=sk)\n",
    "    cell_label = remove_small_objects(cell_label, 344).astype(np.uint8)\n",
    "    selem = disk(2)\n",
    "    cell_label = closing(cell_label, selem)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    # this part is to use green channel, and extend cell label to green channel\n",
    "    # benefit is to exclude cells clear on border but without nucleus\n",
    "    sk = np.asarray(\n",
    "        np.add(\n",
    "            np.asarray(cell_label > 0, dtype=np.int8),\n",
    "            np.asarray(cell_pred[..., 1] / 255 > 0.05, dtype=np.int8),\n",
    "        ) > 0,\n",
    "        dtype=np.int8,\n",
    "    )\n",
    "    cell_label = segmentation.watershed(-distance, cell_label, mask=sk)\n",
    "    cell_label = __fill_holes(cell_label)\n",
    "    cell_label = np.asarray(cell_label > 0, dtype=np.uint8)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    cell_label = remove_small_objects(cell_label, 344)\n",
    "    cell_label = measure.label(cell_label)\n",
    "    cell_label = np.asarray(cell_label, dtype=np.uint16)\n",
    "    nuclei_label = np.multiply(cell_label > 0, nuclei_label) > 0\n",
    "    nuclei_label = measure.label(nuclei_label)\n",
    "    nuclei_label = remove_small_objects(nuclei_label, 157)\n",
    "    nuclei_label = np.multiply(cell_label, nuclei_label > 0)\n",
    "\n",
    "    return nuclei_label, cell_label\n",
    "\n",
    "\n",
    "segmentator = CellSegmentator(NUC_MODEL, CELL_MODEL, padding=True)\n",
    "# segmentator = CellSegmentator(\n",
    "#     NUC_MODEL,\n",
    "#     CELL_MODEL,\n",
    "#     scale_factor=0.25,\n",
    "#     device=\"cuda\",\n",
    "#     padding=True,\n",
    "#     multi_channel_model=True,\n",
    "# )\n",
    "\n",
    "\n",
    "def get_segment_mask(gray, rgb, bs=24):\n",
    "    nuc_segmentations = segmentator.pred_nuclei(gray, bs=bs)  # blue\n",
    "    cell_segmentations = segmentator.pred_cells(rgb, precombined=True, bs=bs)\n",
    "    batch_cell_masks = [\n",
    "        label_cell(nuc_seg, cell_seg)[1].astype(np.uint8)\n",
    "        for nuc_seg, cell_seg in zip(nuc_segmentations, cell_segmentations)\n",
    "    ]\n",
    "    return batch_cell_masks\n",
    "\n",
    "\n",
    "def load_images(image_ids, root=img_dir):\n",
    "    gray = []\n",
    "    rgb = []\n",
    "    for ID in tqdm(image_ids, total=len(image_ids)):\n",
    "        r = os.path.join(root, f'{ID}_red.png')\n",
    "        y = os.path.join(root, f'{ID}_yellow.png')\n",
    "        b = os.path.join(root, f'{ID}_blue.png')\n",
    "        r = cv2.imread(r, 0)\n",
    "        y = cv2.imread(y, 0)\n",
    "        b = cv2.imread(b, 0)\n",
    "        gray_image = cv2.resize(b, (target_image_size, target_image_size))\n",
    "        rgb_image = cv2.resize(np.stack((r, y, b), axis=2),\n",
    "                               (target_image_size, target_image_size))\n",
    "        gray.append(gray_image)\n",
    "        rgb.append(rgb_image)\n",
    "    return gray, rgb\n",
    "\n",
    "\n",
    "for size_idx, submission_ids in tqdm(enumerate(\n",
    "    [predict_ids_1728, predict_ids_2048, predict_ids_3072, predict_ids_4096]),\n",
    "                                     total=4):\n",
    "    size = IMAGE_SIZES[size_idx]\n",
    "    if submission_ids == []:\n",
    "        print(f\"\\n...SKIPPING SIZE {size} AS THERE ARE NO IMAGE IDS ...\\n\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"\\n...WORKING ON IMAGE IDS FOR SIZE {size} ...\\n\")\n",
    "    for i in tqdm(range(0, len(submission_ids), BATCH_SIZE[size]),\n",
    "                  total=int(np.ceil(len(submission_ids) / BATCH_SIZE[size]))):\n",
    "\n",
    "        r, y, b = [], [], []\n",
    "        image_ids = submission_ids[i:(i + BATCH_SIZE[size])]\n",
    "        gray, rgb = load_images(image_ids)\n",
    "        batch_cell_masks = get_segment_mask(gray, rgb, bs=BATCH_SIZE[size])\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        for index, img_id in enumerate(image_ids):\n",
    "            np.save(os.path.join(output_folder, f'{img_id}_cell_mask.npy'),\n",
    "                    batch_cell_masks[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# !python hpa_cell_segment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall time: 4min 45s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Std From Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mean = [0.081018, 0.052349, 0.054012, 0.08106] # rgby\n",
    "total_std = [0.133235, 0.08948, 0.143813, 0.130265]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load Pretrained Model from Bestfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--predict_epoch'], dest='predict_epoch', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='number epoch to predict', metavar=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Protein Classification')\n",
    "parser.add_argument('--out_dir', type=str, help='destination where predicted result should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for predicting (default: 0)')\n",
    "parser.add_argument('--arch', default='class_densenet121_dropout', type=str,\n",
    "                    help='model architecture (default: class_densenet121_dropout)')\n",
    "parser.add_argument('--num_classes', default=28, type=int, help='number of classes (default: 28)')\n",
    "parser.add_argument('--in_channels', default=4, type=int, help='in channels (default: 4)')\n",
    "parser.add_argument('--img_size', default=768, type=int, help='image size (default: 768)')\n",
    "parser.add_argument('--crop_size', default=512, type=int, help='crop size (default: 512)')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='train mini-batch size (default: 32)')\n",
    "parser.add_argument('--workers', default=3, type=int, help='number of data loading workers (default: 3)')\n",
    "parser.add_argument('--fold', default=0, type=int, help='index of fold (default: 0)')\n",
    "parser.add_argument('--augment', default='default', type=str, help='test augmentation (default: default)')\n",
    "parser.add_argument('--seed', default=100, type=int, help='random seed (default: 100)')\n",
    "parser.add_argument('--seeds', default=None, type=str, help='predict seed')\n",
    "parser.add_argument('--predict_epoch', default=None, type=int, help='number epoch to predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(arch='class_densenet121_dropout', augment='default', batch_size=32, crop_size=512, fold=0, gpu_id='0', img_size=768, in_channels=4, num_classes=28, out_dir=None, predict_epoch=None, seed=100, seeds=None, workers=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parser.parse_args([\n",
    "    \"--arch\",\n",
    "    \"class_densenet121_dropout\",\n",
    "    #     \"--img_size\", str(image_size),\n",
    "    \"--crop_size\",\n",
    "    str(crop_size),\n",
    "])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_model(network_path, args, print_model=False):\n",
    "    # setting up the visible GPU\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "    model_params = {}\n",
    "    model_params['architecture'] = args.arch\n",
    "    model_params['num_classes'] = args.num_classes\n",
    "    model_params['in_channels'] = args.in_channels\n",
    "    model_params['pretrained_path'] = f\"{bestfitting_folder}\"\n",
    "    model = init_network(model_params)\n",
    "\n",
    "    checkpoint = torch.load(network_path)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    # moving network to gpu and eval mode\n",
    "    # model = DataParallel(model)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    if print_model:\n",
    "        print(model)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_crop_img(img):\n",
    "    random_crop_size = int(np.random.uniform(self.crop_size, self.img_size))\n",
    "    x = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "    y = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "    crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n",
    "    return crop_img\n",
    "\n",
    "\n",
    "def read_rgby(\n",
    "    img_dir,\n",
    "    img_id,\n",
    "    random_crop=False,\n",
    "):\n",
    "    suffix = '.png'\n",
    "    colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "    flags = cv2.IMREAD_GRAYSCALE\n",
    "    rgby_img = [\n",
    "        cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n",
    "        for color in colors\n",
    "    ]\n",
    "    rgby_img = np.stack(rgby_img, axis=-1)\n",
    "    if random_crop and crop_size > 0:\n",
    "        rgby_img = read_crop_img(rgby_img)\n",
    "\n",
    "    return rgby_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img_id, img_dir):\n",
    "    if img_id not in global_cache:\n",
    "        rgby_img = read_rgby(img_dir, img_id)\n",
    "        if rgby_img[0] is None:\n",
    "            print(self.img_dir, img_id)\n",
    "\n",
    "        h, w = rgby_img.shape[:2]\n",
    "\n",
    "        if crop_size > 0:\n",
    "            if crop_size != h or crop_size != w:\n",
    "                resized_rgby_img = cv2.resize(rgby_img, (crop_size, crop_size),\n",
    "                                              interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        full_mask = np.load(f\"{cell_mask_folder}/{img_id}_cell_mask.npy\")\n",
    "\n",
    "        #         full_mask = cv2.resize(full_mask, (crop_size, crop_size),\n",
    "        #                                interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        cell_masks = [\n",
    "            rle_encoding(full_mask, mask_val=k)\n",
    "            for k in range(1,\n",
    "                           np.max(full_mask) + 1)\n",
    "        ]\n",
    "        if len(cell_masks) == 0:\n",
    "            print(f\"No cell masks found for {img_id}\")\n",
    "\n",
    "        resized_rgby_img = resized_rgby_img / 255.0\n",
    "\n",
    "        global_cache[img_id] = (resized_rgby_img, cell_masks)\n",
    "        return resized_rgby_img, cell_masks\n",
    "    else:\n",
    "        # print(f\"Cache hit for {img_id}!\")\n",
    "        return global_cache[img_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_cells_fn(x):\n",
    "    images = []\n",
    "    # For each full image, extract cell images\n",
    "    image = x[0]\n",
    "    masks = x[1]\n",
    "    transform = x[2]\n",
    "\n",
    "    for rle_string in masks:\n",
    "        cell_mask = rle_to_mask(rle_string, crop_size, crop_size)\n",
    "        # Important: set 255 to 1\n",
    "        cell_mask[cell_mask > 0] = 1\n",
    "\n",
    "        cell_image = np.copy(image)\n",
    "        #         cell_mask = np.tile(cell_mask, reps=(4, 1, 1)).transpose(1, 2, 0)\n",
    "        #         cell_image *= cell_mask\n",
    "        for i in range(4):\n",
    "            cell_image[..., i] = cell_image[..., i] * cell_mask\n",
    "\n",
    "        if transform is not None:\n",
    "            cell_image = transform(cell_image)\n",
    "\n",
    "        cell_image = image_to_tensor(cell_image)\n",
    "        images.append(cell_image.unsqueeze(0))\n",
    "\n",
    "    images = torch.cat(images)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, model, generate_meta=False):\n",
    "    all_probs = []\n",
    "    all_meta = {}\n",
    "\n",
    "    global_processed = 0\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        id = row[\"ID\"]\n",
    "        width = row[\"ImageWidth\"]\n",
    "        height = row[\"ImageHeight\"]\n",
    "\n",
    "        # Data augmentation\n",
    "        aug_probs = None\n",
    "        for augment in augment_list:\n",
    "            transform = eval(f\"augment_{augment}\")\n",
    "            images, masks = process_image(id, test_image_folder)\n",
    "            images, masks = collate_cells_fn((images, masks, transform))\n",
    "\n",
    "            cell_probs = []\n",
    "            processed_count = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_i in range(0, len(masks), batch_size):\n",
    "                    batch_images = images[batch_i:batch_i + batch_size, ...]\n",
    "                    # batch_images = Variable(batch_images.cuda(), volatile=True)\n",
    "                    outputs = model(batch_images.cuda())\n",
    "                    logits = outputs\n",
    "\n",
    "                    probs = F.sigmoid(logits).data\n",
    "                    probs = probs.detach().cpu().numpy().tolist()\n",
    "                    cell_probs += probs\n",
    "\n",
    "                    processed_count += len(probs)\n",
    "\n",
    "            cell_probs = np.array(cell_probs).reshape(processed_count, -1)\n",
    "\n",
    "            if aug_probs is None:\n",
    "                aug_probs = cell_probs / len(augment_list)\n",
    "            else:\n",
    "                aug_probs += cell_probs / len(augment_list)\n",
    "\n",
    "        all_probs.append(aug_probs)\n",
    "\n",
    "        if generate_meta:\n",
    "            masks = np.array(masks)\n",
    "\n",
    "            if masks.shape[0] > 0:\n",
    "                # Generate RLE string for each cell mask\n",
    "                # https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook?scriptVersionId=55714434\n",
    "                submit_strings = []\n",
    "                for i in range(masks.shape[0]):\n",
    "                    mask = masks[i]\n",
    "                    mask = rle_to_mask(mask, crop_size, crop_size)\n",
    "                    # Important: set 255 to 1\n",
    "                    mask[mask > 0] = 1\n",
    "\n",
    "                    # Important: resize to orignal resolution to submit correct mask RLE string\n",
    "                    # https://www.kaggle.com/linshokaku/faster-hpa-cell-segmentation/comments#1251082\n",
    "                    mask = cv2.resize(mask, (width, height),\n",
    "                                      interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                    rle_string = binary_mask_to_ascii(mask, mask_val=1)\n",
    "                    submit_strings.append(rle_string)\n",
    "\n",
    "                if len(submit_strings) > 0:\n",
    "                    all_meta[id] = submit_strings\n",
    "                else:\n",
    "                    all_meta[id] = []\n",
    "            else:\n",
    "                all_meta[id] = []\n",
    "\n",
    "        del images, masks, aug_probs, cell_probs, batch_images\n",
    "        gc.collect()\n",
    "\n",
    "        if debug and global_processed == batch_size - 1:\n",
    "            break\n",
    "\n",
    "        global_processed += 1\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    if generate_meta:\n",
    "        return all_probs, all_meta\n",
    "    else:\n",
    "        return all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch Processing 0]\n",
      "(4, 4)\n",
      "[Fold 0]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:12<00:04,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "[Fold 1]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "[Fold 2]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "[Fold 3]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "[Fold 4]\n",
      ">> Using pre-trained model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferencing with seed 1120 ......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n",
      "CPU times: user 1min 12s, sys: 8.86 s, total: 1min 21s\n",
      "Wall time: 55.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_probs = []\n",
    "final_meta = {}\n",
    "\n",
    "augment_list = [\n",
    "    'default', 'flipud', 'fliplr', 'transpose', 'flipud_lr',\n",
    "    'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose'\n",
    "]\n",
    "# seeds = [0, 1, 2, 3]\n",
    "seeds = [0]\n",
    "kfolds = 5\n",
    "# fold = 0\n",
    "\n",
    "cache_size = batch_size if debug else 250\n",
    "global_cache = {}\n",
    "batch_rounds = 0\n",
    "for batch_i in range(0, submit_df.shape[0], cache_size):\n",
    "    print(f\"[Batch Processing {batch_rounds}]\")\n",
    "    sub_df = submit_df.iloc[batch_i:batch_i + cache_size, :].copy()\n",
    "    print(sub_df.shape)\n",
    "\n",
    "    batch_probs = [0] * sub_df.shape[0]\n",
    "    for fold in range(kfolds):\n",
    "        print(f\"[Fold {fold}]\")\n",
    "        network_path = f\"{bestfitting_folder}/{model_folder}/fold{fold}/final.pth\"\n",
    "        model = load_model(network_path, args)\n",
    "\n",
    "        for i, seed in enumerate(seeds):\n",
    "            print(f\"Inferencing with seed {rand_seed+1000*seed} ......\")\n",
    "            seed_everything(rand_seed + 1000 * seed)\n",
    "\n",
    "            if fold == 0 and i == 0:\n",
    "                seed_probs, meta = predict(sub_df, model, generate_meta=True)\n",
    "                print(len(seed_probs), len(batch_probs), sub_df.shape[0])\n",
    "                for j in range(len(seed_probs)):\n",
    "                    batch_probs[j] = seed_probs[j] / (len(seeds) * kfolds)\n",
    "                final_meta.update(meta)\n",
    "            else:\n",
    "                seed_probs = predict(sub_df, model)\n",
    "                print(len(seed_probs), len(batch_probs), sub_df.shape[0])\n",
    "                for j in range(len(seed_probs)):\n",
    "                    batch_probs[j] += seed_probs[j] / (len(seeds) * kfolds)\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    if batch_probs is not None:\n",
    "        final_probs.extend(batch_probs)\n",
    "\n",
    "    if debug:\n",
    "        break\n",
    "\n",
    "    # Reset cache\n",
    "    del global_cache\n",
    "    gc.collect()\n",
    "    global_cache = {}\n",
    "    batch_rounds += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/559 [00:00<00:02, 257.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33434679 0.04865826 0.25911937 0.18834128 0.09607146 0.22553113\n",
      "  0.11060969 0.20670703 0.00323915 0.0027828  0.00410879 0.09500933\n",
      "  0.08870949 0.05791689 0.06686524 0.00471052 0.1802697  0.07775988\n",
      "  0.1901773  0.21884185 0.02806147 0.23074935 0.13730339 0.18773878\n",
      "  0.0542643  0.46876616 0.08920476 0.00857008]\n",
      " [0.16786988 0.05613488 0.50401652 0.28275432 0.05957497 0.12943275\n",
      "  0.18198858 0.27390415 0.00472906 0.00399137 0.00898763 0.08982923\n",
      "  0.06443159 0.03952967 0.03085783 0.00267168 0.10194304 0.0375218\n",
      "  0.13896492 0.16508048 0.04697486 0.18017078 0.11905039 0.18900523\n",
      "  0.07012391 0.54726522 0.07516516 0.01724261]\n",
      " [0.42305807 0.12024488 0.25443657 0.20892196 0.1179472  0.21593588\n",
      "  0.09086709 0.14906612 0.00137028 0.00083754 0.0011912  0.11714291\n",
      "  0.11238938 0.07202232 0.05819734 0.00652568 0.16251759 0.05282691\n",
      "  0.15146815 0.200137   0.02171619 0.21515143 0.16328235 0.17629771\n",
      "  0.03787746 0.38747682 0.13018555 0.00831233]\n",
      " [0.34821632 0.07957082 0.16102409 0.14789668 0.11053528 0.22291261\n",
      "  0.10903075 0.17541902 0.00193749 0.00095993 0.0014238  0.10324337\n",
      "  0.08376375 0.05981222 0.05437059 0.00240633 0.14502456 0.06404553\n",
      "  0.16428043 0.20119562 0.01520681 0.23355474 0.1371189  0.25461165\n",
      "  0.03002894 0.39300985 0.09891637 0.00687513]\n",
      " [0.2477121  0.02692812 0.17820984 0.15409899 0.07136627 0.17207793\n",
      "  0.12753947 0.19026217 0.00175208 0.00385267 0.00541465 0.09932437\n",
      "  0.07787068 0.05353148 0.07611436 0.00513836 0.22564691 0.16066846\n",
      "  0.2321797  0.1910475  0.02833832 0.21928581 0.10689208 0.16792759\n",
      "  0.03652277 0.57710556 0.13136944 0.02121005]\n",
      " [0.28682398 0.04011868 0.1832708  0.0988432  0.08363953 0.14079054\n",
      "  0.13045322 0.13991593 0.00199465 0.00390998 0.00602622 0.11026328\n",
      "  0.11703952 0.05711204 0.0556581  0.00364122 0.17402208 0.07627199\n",
      "  0.16899286 0.1548434  0.05442872 0.23758366 0.12035159 0.12120687\n",
      "  0.05956927 0.6436662  0.12102076 0.01615901]\n",
      " [0.30124458 0.01968076 0.28767895 0.2013769  0.09466795 0.16335874\n",
      "  0.0921216  0.3493653  0.00298742 0.00818097 0.01400279 0.06245097\n",
      "  0.07519392 0.05090828 0.03791325 0.00290036 0.1471493  0.06367078\n",
      "  0.16801793 0.14577439 0.05045177 0.21669184 0.1114098  0.15815936\n",
      "  0.03909531 0.50199689 0.07019716 0.01233062]\n",
      " [0.391611   0.05955399 0.18703215 0.11295529 0.10733882 0.27525699\n",
      "  0.10208197 0.17186939 0.00391878 0.00340626 0.00575983 0.094131\n",
      "  0.10958036 0.05684308 0.05409    0.00708312 0.16779561 0.06975137\n",
      "  0.20971623 0.33783117 0.03041844 0.22762241 0.13470972 0.18940362\n",
      "  0.07506642 0.47401143 0.13127708 0.02140357]\n",
      " [0.30503159 0.05332771 0.23498312 0.18160314 0.14581295 0.28959046\n",
      "  0.14473809 0.13367838 0.00376195 0.00472516 0.00695617 0.09548933\n",
      "  0.08114678 0.06518799 0.07321341 0.00733402 0.18738973 0.08125392\n",
      "  0.19355092 0.17850644 0.03635649 0.19568313 0.10554917 0.11535097\n",
      "  0.069742   0.60703368 0.10913223 0.01545739]\n",
      " [0.2948675  0.02297108 0.2663491  0.14364561 0.10202494 0.16421322\n",
      "  0.10561453 0.24942631 0.00290154 0.0039198  0.00765367 0.07985671\n",
      "  0.07591847 0.04676418 0.05136893 0.00483762 0.18359013 0.09292236\n",
      "  0.17259183 0.14938816 0.02965017 0.19776012 0.09215145 0.17422119\n",
      "  0.0380507  0.59713648 0.07091725 0.01519012]\n",
      " [0.26940132 0.05442387 0.27313671 0.2776362  0.1496589  0.26214597\n",
      "  0.13252454 0.17396591 0.00329399 0.00262978 0.00415749 0.10888599\n",
      "  0.07585069 0.05882769 0.07473431 0.00406106 0.17421246 0.07011474\n",
      "  0.21408664 0.19825198 0.0343543  0.20344124 0.13656329 0.16892382\n",
      "  0.06336358 0.5094783  0.12006113 0.01168526]\n",
      " [0.31805681 0.03477494 0.28227244 0.25847481 0.12298091 0.402144\n",
      "  0.09112871 0.18985817 0.00311777 0.00309976 0.00492711 0.09588783\n",
      "  0.12043723 0.06221668 0.05776287 0.01974082 0.16932553 0.06142495\n",
      "  0.19480043 0.1940749  0.02047725 0.21445655 0.14305327 0.15489951\n",
      "  0.08195111 0.50437963 0.08741682 0.01961984]\n",
      " [0.25083692 0.04962256 0.32405113 0.17446406 0.09007977 0.1399618\n",
      "  0.15648892 0.11752759 0.00112569 0.00226135 0.0036671  0.07384612\n",
      "  0.10412177 0.06360629 0.04545155 0.00301421 0.14517992 0.04202796\n",
      "  0.1761673  0.17094282 0.03423382 0.22871853 0.14489163 0.14154077\n",
      "  0.05899956 0.58382986 0.11468266 0.01301098]\n",
      " [0.26372639 0.057648   0.37302732 0.31019848 0.1313021  0.16369827\n",
      "  0.12803563 0.18902301 0.00521094 0.0036621  0.00561486 0.08613463\n",
      "  0.07952836 0.05456003 0.06373966 0.00524138 0.16860573 0.06478602\n",
      "  0.16206345 0.18749806 0.03513265 0.21112948 0.11237048 0.15569293\n",
      "  0.037644   0.5240839  0.09176043 0.01148823]\n",
      " [0.28358189 0.05872485 0.17475888 0.14558963 0.07506938 0.15123705\n",
      "  0.13712038 0.14993156 0.00106144 0.0016939  0.00259586 0.14107953\n",
      "  0.08170162 0.05165186 0.09488908 0.00232194 0.20936518 0.10687053\n",
      "  0.19468096 0.17712543 0.02198169 0.21966292 0.11139442 0.14809835\n",
      "  0.05740334 0.59704696 0.14938253 0.01323421]\n",
      " [0.32317902 0.05428317 0.1734791  0.14505665 0.09531609 0.2024984\n",
      "  0.11847776 0.18971763 0.00129287 0.0015378  0.0022675  0.11568357\n",
      "  0.08091555 0.05982192 0.06326126 0.0026973  0.18023886 0.08813669\n",
      "  0.20250423 0.20432277 0.01908205 0.2300456  0.13527293 0.1947635\n",
      "  0.04528509 0.4659081  0.13119149 0.01009443]]\n",
      "[[0.3118357  0.03837184 0.15485068 0.12434932 0.08434634 0.19754081\n",
      "  0.10293688 0.15607281 0.00093455 0.00103395 0.00146422 0.11815142\n",
      "  0.08722464 0.05832249 0.10547322 0.00322031 0.21439904 0.14034919\n",
      "  0.21784228 0.23339335 0.01058697 0.24469334 0.11038314 0.1571165\n",
      "  0.02815022 0.48949347 0.09125919 0.00643005]\n",
      " [0.31349838 0.05864196 0.15993784 0.14563522 0.08785566 0.1853463\n",
      "  0.11904291 0.18391151 0.00114243 0.00220212 0.00301927 0.11195767\n",
      "  0.08842039 0.05783402 0.09417948 0.00324223 0.19086452 0.11069582\n",
      "  0.21695416 0.22411007 0.01460953 0.2400322  0.11304746 0.16669301\n",
      "  0.03203188 0.48807172 0.11537583 0.00986982]\n",
      " [0.21562979 0.10797448 0.15879169 0.0820973  0.05836733 0.1085782\n",
      "  0.12786701 0.16859791 0.00083283 0.00247289 0.00323704 0.14374239\n",
      "  0.10316918 0.04601978 0.2227079  0.00424293 0.25095432 0.16499345\n",
      "  0.22566619 0.28558863 0.01170743 0.23377647 0.09318816 0.10164687\n",
      "  0.02505741 0.59082555 0.09988278 0.0119736 ]\n",
      " [0.21943339 0.08294437 0.12059613 0.07830078 0.05309136 0.12865832\n",
      "  0.12269688 0.13903965 0.00079577 0.00194858 0.00244273 0.12189706\n",
      "  0.10992782 0.05365574 0.11703182 0.00306554 0.2078853  0.14029035\n",
      "  0.19210935 0.23830126 0.01035333 0.28314333 0.097995   0.08813041\n",
      "  0.02197579 0.65661767 0.1173425  0.00901529]\n",
      " [0.24157409 0.06168791 0.15621902 0.10362553 0.07294753 0.16257051\n",
      "  0.1556413  0.14577268 0.00120695 0.00121977 0.00175716 0.11742548\n",
      "  0.08565321 0.05090381 0.17524018 0.00249131 0.24502563 0.17990557\n",
      "  0.24505225 0.28383417 0.01223026 0.23525974 0.10359511 0.15142758\n",
      "  0.02052895 0.52131642 0.08078859 0.00659423]\n",
      " [0.21531883 0.03910906 0.1679636  0.09696802 0.06518092 0.12861467\n",
      "  0.11420445 0.13764856 0.00069585 0.00274852 0.00342289 0.16255516\n",
      "  0.10448272 0.04713865 0.24297477 0.00728551 0.28792759 0.2460798\n",
      "  0.21934542 0.22545887 0.00824914 0.28083653 0.10331656 0.08416361\n",
      "  0.01606831 0.67382009 0.08377972 0.0121888 ]\n",
      " [0.25971027 0.03871997 0.13938964 0.10335317 0.06452694 0.14316233\n",
      "  0.11295895 0.16309196 0.00099716 0.00432558 0.00512981 0.12431318\n",
      "  0.10582894 0.05264344 0.18670964 0.00434414 0.24912716 0.16160244\n",
      "  0.21862312 0.21597778 0.01328453 0.29079369 0.12580249 0.10588572\n",
      "  0.02935423 0.60643786 0.12859514 0.01408668]\n",
      " [0.25899055 0.03894234 0.16426722 0.12417658 0.06832892 0.14708574\n",
      "  0.1158244  0.16440609 0.00111404 0.00326859 0.00425026 0.12129429\n",
      "  0.10970309 0.06295365 0.22081312 0.00632062 0.26729929 0.17210349\n",
      "  0.23860755 0.22534882 0.0125092  0.27732718 0.13247952 0.14382708\n",
      "  0.02575019 0.58108802 0.09781805 0.01346797]\n",
      " [0.17987997 0.05082604 0.20698202 0.09750583 0.06941874 0.13150913\n",
      "  0.15755819 0.1412644  0.00098047 0.00250637 0.00326436 0.10206853\n",
      "  0.09796402 0.05267069 0.14235696 0.00481645 0.26365544 0.18094585\n",
      "  0.18615472 0.23358285 0.01145073 0.25363484 0.09550392 0.09016461\n",
      "  0.0191619  0.67727774 0.08992972 0.00929261]\n",
      " [0.1932533  0.08943034 0.15954968 0.08624268 0.0503607  0.10392763\n",
      "  0.12194967 0.14132245 0.0007183  0.00263596 0.00339678 0.13637521\n",
      "  0.08001815 0.04060355 0.24294456 0.00485718 0.30506106 0.22499075\n",
      "  0.21200145 0.19488867 0.00966688 0.23443366 0.08595526 0.09600597\n",
      "  0.01838394 0.66299105 0.11806983 0.01190539]\n",
      " [0.25091211 0.06166999 0.1521039  0.085628   0.07716861 0.12746248\n",
      "  0.13319675 0.1801498  0.00157663 0.00418313 0.00542302 0.12872773\n",
      "  0.10546263 0.05617621 0.21478066 0.00579787 0.27314714 0.16291746\n",
      "  0.20739837 0.2351435  0.01504425 0.26248848 0.10725836 0.12858905\n",
      "  0.02631571 0.61521626 0.10048617 0.01264426]\n",
      " [0.26234983 0.05364743 0.14752012 0.120996   0.06968161 0.16235781\n",
      "  0.14538321 0.18043328 0.00118097 0.00292614 0.00377697 0.09486607\n",
      "  0.08616679 0.05436413 0.07944607 0.0020222  0.18630447 0.12144429\n",
      "  0.22692413 0.23447712 0.01860901 0.26902126 0.11021384 0.14912732\n",
      "  0.03072508 0.56412669 0.11879907 0.0099862 ]]\n",
      "[[0.22977615 0.02563008 0.13318675 ... 0.31957774 0.05869832 0.00243666]\n",
      " [0.21537614 0.04298169 0.11136874 ... 0.2177813  0.03294675 0.00414591]\n",
      " [0.28987397 0.04032858 0.13855315 ... 0.31618925 0.06440946 0.00513866]\n",
      " ...\n",
      " [0.1891639  0.02442786 0.10318517 ... 0.26153934 0.03256946 0.00182422]\n",
      " [0.29107737 0.04433015 0.1458725  ... 0.37433652 0.08051334 0.00612821]\n",
      " [0.33621269 0.05933724 0.15839924 ... 0.40043061 0.10013892 0.00517475]]\n",
      "[[2.21852936e-01 3.12592511e-02 1.39166152e-01 5.99436082e-02\n",
      "  8.55138500e-02 1.25292909e-01 5.55071427e-02 2.92921418e-01\n",
      "  2.98475734e-04 1.17325559e-03 1.96898961e-03 9.55745256e-02\n",
      "  3.35050635e-02 3.62062112e-02 2.50969168e-02 4.16049989e-04\n",
      "  1.18184269e-01 3.94429885e-02 5.22976249e-01 1.63907674e-01\n",
      "  8.02222431e-03 1.40332720e-01 1.17592655e-01 1.05107849e-01\n",
      "  1.21457563e-01 2.93711946e-01 3.85866348e-02 8.81153349e-03]\n",
      " [2.33457281e-01 2.33174812e-02 9.61734708e-02 6.02830516e-02\n",
      "  1.20966276e-01 1.30654246e-01 7.28030787e-02 4.98622447e-01\n",
      "  1.71401317e-03 1.70820986e-03 3.45907581e-03 8.42172723e-02\n",
      "  6.30198211e-02 6.52215765e-02 2.93457435e-02 1.15198102e-03\n",
      "  1.20781885e-01 4.17171535e-02 1.79085994e-01 1.09158728e-01\n",
      "  2.12145049e-02 2.13738856e-01 2.38830951e-01 2.31859643e-01\n",
      "  4.93093731e-02 2.96646138e-01 1.04760094e-01 9.64402937e-02]\n",
      " [2.52870653e-01 1.89809535e-02 1.13937095e-01 6.05669510e-02\n",
      "  7.86446634e-02 1.13228443e-01 6.58950296e-02 5.95447198e-01\n",
      "  4.18807593e-04 6.48194521e-04 1.47502673e-03 5.85176920e-02\n",
      "  3.16658993e-02 3.02428351e-02 1.57103596e-02 5.55894894e-05\n",
      "  7.71914572e-02 1.57555453e-02 9.92242302e-02 5.98615840e-02\n",
      "  2.61575305e-02 1.47216929e-01 8.58171788e-02 1.39319665e-01\n",
      "  1.26946294e-01 2.63626585e-01 3.12503569e-02 3.96143458e-03]\n",
      " [2.17599192e-01 3.63525965e-02 1.47363715e-01 7.24737789e-02\n",
      "  6.84487110e-02 1.10355546e-01 6.52630227e-02 5.74558658e-01\n",
      "  8.47196124e-04 3.52049370e-04 8.69461010e-04 1.49982219e-01\n",
      "  4.21908306e-02 2.93636467e-02 2.05926120e-02 2.60174006e-04\n",
      "  7.98965824e-02 2.56879103e-02 1.08400299e-01 9.64453883e-02\n",
      "  9.82535350e-03 1.25852991e-01 8.08250880e-02 1.93465097e-01\n",
      "  7.56074186e-02 2.70451608e-01 3.51187951e-02 9.49202453e-03]\n",
      " [2.98198489e-01 5.53235814e-02 1.98931414e-01 1.07706561e-01\n",
      "  1.06488718e-01 3.30979279e-01 6.47113699e-02 1.77691077e-01\n",
      "  6.85635561e-04 8.11418950e-04 1.24513718e-03 1.14134111e-01\n",
      "  6.06111743e-02 5.29273381e-02 6.37572805e-02 4.52232607e-03\n",
      "  1.77662724e-01 8.80118192e-02 3.58841092e-01 2.45035519e-01\n",
      "  8.43453933e-03 1.71075887e-01 1.33047399e-01 1.37997393e-01\n",
      "  5.68487892e-02 3.33837758e-01 7.13083942e-02 8.70033273e-03]\n",
      " [2.28419668e-01 1.74954639e-02 1.70467232e-01 1.17392767e-01\n",
      "  4.67766757e-02 1.09547410e-01 6.33911697e-02 6.31307918e-01\n",
      "  1.53819872e-03 1.26394825e-03 2.50443028e-03 5.83543836e-02\n",
      "  3.60753356e-02 3.92310918e-02 2.72412148e-02 5.02220379e-04\n",
      "  1.28931179e-01 4.71829614e-02 1.26380834e-01 1.22360999e-01\n",
      "  2.06104419e-02 1.45457667e-01 1.08728902e-01 2.36458197e-01\n",
      "  2.02959949e-02 2.64057535e-01 5.04760524e-02 7.49067090e-03]\n",
      " [1.65441799e-01 1.04239374e-02 1.02345727e-01 4.86007161e-02\n",
      "  3.68062571e-02 7.08131098e-02 4.31472977e-02 8.06235209e-01\n",
      "  1.38230820e-04 2.11700640e-04 6.15248730e-04 3.14894512e-02\n",
      "  1.16529830e-02 1.25062287e-02 3.89826219e-03 4.94276112e-06\n",
      "  4.18700451e-02 8.92588987e-03 6.09927334e-02 3.27914787e-02\n",
      "  9.45720611e-03 7.49056691e-02 3.31247642e-02 1.06530203e-01\n",
      "  3.64246206e-02 2.01642919e-01 6.66901741e-03 1.03334043e-03]\n",
      " [2.32568107e-01 1.52610749e-02 1.14372881e-01 5.46491643e-02\n",
      "  6.58539578e-02 1.13027947e-01 6.33046780e-02 5.59504707e-01\n",
      "  2.97937035e-04 4.19363296e-04 1.19130812e-03 5.84958577e-02\n",
      "  2.05283196e-02 1.83662795e-02 1.09294784e-02 3.24433799e-05\n",
      "  7.86076402e-02 1.19649627e-02 1.10203932e-01 4.77412539e-02\n",
      "  3.35538170e-02 1.21555982e-01 7.95494019e-02 1.34078749e-01\n",
      "  1.98368944e-01 2.57497967e-01 2.59180775e-02 3.50284522e-03]\n",
      " [1.59100071e-01 9.23523250e-03 1.25636205e-01 6.49388846e-02\n",
      "  3.60524797e-02 6.99246238e-02 4.14618130e-02 8.18877089e-01\n",
      "  2.20247533e-04 1.48374398e-04 4.10188483e-04 3.76142564e-02\n",
      "  1.48619128e-02 1.67610219e-02 7.57229212e-03 1.77481975e-05\n",
      "  6.34154491e-02 1.63499254e-02 6.08994304e-02 4.06621948e-02\n",
      "  4.97614961e-03 8.65439202e-02 5.05416972e-02 1.21691699e-01\n",
      "  1.62608051e-02 2.15529196e-01 7.77187129e-03 2.14765371e-03]\n",
      " [2.05807126e-01 1.53531485e-02 1.12703229e-01 3.15495943e-02\n",
      "  6.19121145e-02 1.14012390e-01 4.89450403e-02 5.85971361e-01\n",
      "  2.49081602e-04 3.72958668e-04 9.48153964e-04 6.48064025e-02\n",
      "  2.24276770e-02 1.91444248e-02 1.25860090e-02 4.40987007e-05\n",
      "  7.76045953e-02 1.50404699e-02 1.47938812e-01 5.00644376e-02\n",
      "  1.07668575e-02 1.31042457e-01 9.40253327e-02 1.27530198e-01\n",
      "  1.75569548e-01 2.39799313e-01 1.99109646e-02 4.39869406e-03]\n",
      " [2.16592518e-01 1.80359865e-02 1.39974717e-01 5.36749261e-02\n",
      "  3.57574702e-02 9.92338873e-02 5.16995223e-02 7.51010594e-01\n",
      "  3.40946013e-04 1.41246114e-04 4.19317134e-04 5.93261323e-02\n",
      "  1.99507727e-02 2.06461417e-02 1.14590343e-02 4.86852160e-05\n",
      "  6.91204110e-02 2.02767872e-02 6.23346624e-02 5.91811046e-02\n",
      "  6.27232787e-03 9.93516574e-02 3.78507494e-02 1.64677942e-01\n",
      "  3.32537124e-02 2.42386463e-01 1.17376819e-02 2.28392853e-03]\n",
      " [2.03360628e-01 2.52823325e-02 2.00453868e-01 9.05170238e-02\n",
      "  5.72536488e-02 1.19142834e-01 5.95968353e-02 6.17899151e-01\n",
      "  4.66856981e-04 4.40453852e-04 1.12552544e-03 7.56880247e-02\n",
      "  2.41487031e-02 2.77586201e-02 1.81099125e-02 2.99007390e-04\n",
      "  9.87660785e-02 3.00973350e-02 1.18152937e-01 8.29335105e-02\n",
      "  1.46065884e-02 1.22586633e-01 8.61859550e-02 1.81756352e-01\n",
      "  3.42057146e-02 2.51264605e-01 3.51720304e-02 7.40346891e-03]\n",
      " [2.14912781e-01 3.32291051e-02 1.72692597e-01 5.46089495e-02\n",
      "  5.18522615e-02 8.74698343e-02 5.14512702e-02 4.18412981e-01\n",
      "  5.71758087e-04 4.10593196e-04 8.90151043e-04 2.37136776e-01\n",
      "  4.30452039e-02 3.31860767e-02 2.73737298e-02 7.15019303e-04\n",
      "  1.01510102e-01 3.38439850e-02 2.12111694e-01 1.08422231e-01\n",
      "  7.99040252e-03 1.29131697e-01 1.15386733e-01 1.27236641e-01\n",
      "  1.24287391e-01 2.93343773e-01 2.74099237e-02 1.45893188e-02]\n",
      " [1.93187907e-01 1.73472577e-02 1.28679165e-01 6.18640705e-02\n",
      "  3.96374567e-02 8.60728090e-02 6.34279327e-02 6.75382300e-01\n",
      "  3.69982784e-04 2.03826142e-04 5.16403543e-04 8.03000281e-02\n",
      "  2.21861491e-02 2.15583892e-02 1.30722138e-02 3.58218602e-05\n",
      "  6.80390538e-02 2.17482788e-02 7.69992042e-02 6.98156168e-02\n",
      "  8.12228392e-03 1.18989399e-01 5.19327707e-02 1.70845354e-01\n",
      "  5.35687920e-02 2.61375610e-01 1.74855401e-02 3.04535099e-03]\n",
      " [1.33108362e-01 8.62099197e-03 1.55122517e-01 7.36456886e-02\n",
      "  3.92734116e-02 6.27067307e-02 4.80278136e-02 7.85719033e-01\n",
      "  1.42117406e-04 1.26690535e-04 4.48274266e-04 4.13166158e-02\n",
      "  1.08141738e-02 1.09156031e-02 4.70457373e-03 8.12602670e-06\n",
      "  4.51640881e-02 9.62045104e-03 4.77138738e-02 2.69768834e-02\n",
      "  2.12116487e-02 7.05230245e-02 2.96532382e-02 1.13217543e-01\n",
      "  4.17480981e-02 2.19129182e-01 6.69078229e-03 1.01661612e-03]\n",
      " [1.67145737e-01 1.17387153e-02 1.59968998e-01 9.19345807e-02\n",
      "  2.39360211e-02 5.12462426e-02 3.54251467e-02 7.49587032e-01\n",
      "  1.71883802e-04 4.59836175e-05 1.38554810e-04 3.82844958e-02\n",
      "  1.34681236e-02 1.49580072e-02 8.57257316e-03 2.49695665e-05\n",
      "  5.57945302e-02 1.92087990e-02 4.81527328e-02 6.14613428e-02\n",
      "  5.10127172e-03 7.94078522e-02 4.54046622e-02 1.55962869e-01\n",
      "  6.05043826e-03 2.18581638e-01 7.07352375e-03 2.23483618e-03]\n",
      " [2.63516275e-01 2.31379072e-02 1.15941918e-01 7.00083121e-02\n",
      "  6.82019838e-02 1.13603064e-01 6.90689835e-02 5.60338413e-01\n",
      "  5.95760037e-04 1.02529537e-03 2.11239875e-03 1.10964016e-01\n",
      "  3.39176287e-02 3.22335202e-02 2.73186899e-02 2.12987547e-04\n",
      "  1.10054887e-01 3.57123213e-02 1.94313473e-01 1.27735426e-01\n",
      "  1.12525183e-02 1.31583703e-01 8.97838013e-02 1.53773941e-01\n",
      "  8.76807554e-02 2.76769204e-01 3.46077623e-02 6.37928175e-03]\n",
      " [2.12154124e-01 2.99346279e-02 1.91390555e-01 1.20456828e-01\n",
      "  6.64133235e-02 1.25104300e-01 7.37911262e-02 5.04835173e-01\n",
      "  2.17803936e-03 2.18877968e-03 4.73141456e-03 8.77751865e-02\n",
      "  4.69228737e-02 4.63821518e-02 2.91073502e-02 1.16729543e-03\n",
      "  1.16450821e-01 5.72240367e-02 1.84589188e-01 2.44074368e-01\n",
      "  1.74324134e-02 1.30338943e-01 8.38252475e-02 3.24567443e-01\n",
      "  2.47597926e-02 2.88787638e-01 6.11605232e-02 1.47972163e-02]\n",
      " [2.45454092e-01 2.55322766e-02 1.73157077e-01 1.22641915e-01\n",
      "  8.01382978e-02 1.32157717e-01 8.29401451e-02 3.56637426e-01\n",
      "  1.35195979e-03 1.46371755e-03 2.30590211e-03 9.27152004e-02\n",
      "  7.53027097e-02 5.59586052e-02 5.25267089e-02 1.06162269e-03\n",
      "  1.42873265e-01 6.66099685e-02 1.74060203e-01 1.57781180e-01\n",
      "  2.26052446e-02 2.89942382e-01 1.73130853e-01 2.14182665e-01\n",
      "  5.39634595e-02 3.37244429e-01 6.84556746e-02 1.30648258e-02]\n",
      " [2.05392024e-01 2.05043993e-02 1.78534400e-01 1.14337923e-01\n",
      "  5.07542594e-02 1.28558918e-01 6.31760576e-02 6.61844289e-01\n",
      "  7.14056991e-04 3.61162912e-04 8.82512207e-04 7.51436774e-02\n",
      "  2.96314333e-02 3.07709278e-02 2.01707372e-02 2.75901824e-04\n",
      "  1.01991097e-01 3.63759726e-02 1.01448960e-01 9.46755704e-02\n",
      "  8.92377970e-03 1.13255838e-01 5.84590714e-02 2.07743325e-01\n",
      "  2.50691910e-02 2.60817416e-01 2.54943073e-02 4.98613286e-03]\n",
      " [2.58620392e-01 1.77376634e-02 1.37894299e-01 1.03814165e-01\n",
      "  5.78738775e-02 1.10071699e-01 6.06736125e-02 6.30442351e-01\n",
      "  4.81268606e-04 3.29609222e-04 7.62409438e-04 7.18143607e-02\n",
      "  2.60706782e-02 2.74230218e-02 1.58614632e-02 7.69488260e-05\n",
      "  7.90630026e-02 2.07513126e-02 8.21750307e-02 8.28550716e-02\n",
      "  1.64950033e-02 1.15387358e-01 7.04457629e-02 1.94171813e-01\n",
      "  5.66404628e-02 2.48799936e-01 2.91869277e-02 3.90073444e-03]\n",
      " [3.23642155e-01 4.52439025e-02 1.76669135e-01 1.46990304e-01\n",
      "  9.00840844e-02 1.78217083e-01 8.37046049e-02 2.87942391e-01\n",
      "  1.42596517e-03 1.01054095e-03 1.66798813e-03 8.72058027e-02\n",
      "  5.83374715e-02 4.66803434e-02 3.84963091e-02 6.99669426e-04\n",
      "  1.19365440e-01 4.68878519e-02 1.33843401e-01 1.49375862e-01\n",
      "  2.94842568e-02 2.29020524e-01 1.21342749e-01 2.67063889e-01\n",
      "  4.52862538e-02 3.52172966e-01 7.52396671e-02 5.73037953e-03]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "for index, row in tqdm(submit_df.iterrows(), total=submit_df.shape[0]):\n",
    "    id = row[\"ID\"]\n",
    "    width = row[\"ImageWidth\"]\n",
    "    height = row[\"ImageHeight\"]\n",
    "\n",
    "    cell_probs = final_probs[index]\n",
    "    rle_strings = final_meta[id]\n",
    "\n",
    "    new_preds = np.zeros((cell_probs.shape[0], 19))\n",
    "    for i in range(cell_probs.shape[0]):\n",
    "        for j in range(28):\n",
    "            new_class_i = old_class_mappings[j]\n",
    "            # Take maximum prob.\n",
    "            if cell_probs[i, j] > new_preds[i, new_class_i]:\n",
    "                new_preds[i, new_class_i] = cell_probs[i, j]\n",
    "\n",
    "    submit_strings = []\n",
    "    for i in range(new_preds.shape[0]):\n",
    "        confidence = new_preds[i, ...]\n",
    "        rle_string = rle_strings[i]\n",
    "        for l in range(19):\n",
    "            submit_strings.append(f\"{l} {confidence[l]:.6f} {rle_string}\")\n",
    "\n",
    "    if len(submit_strings) > 0:\n",
    "        all_predictions.append(\" \".join(submit_strings))\n",
    "    else:\n",
    "        all_predictions.append(\"\")\n",
    "\n",
    "    if debug and index == batch_size - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ImageWidth</th>\n",
       "      <th>ImageHeight</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0040581b-f1f2-4fbe-b043-b6bfea5404bb</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.334347 eNqtVD2PgzAM/UtJz5wYOtzAwClp5CGCDBk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004a270d-34a2-4d60-bbe4-365fca868193</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.311836 eNoLCAhINIgxTMkzNAADGxcInWDgAGFYeKD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00537262-883c-4b37-a3a1-a4931b6faea5</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.229776 eNqtkb0KAyEMgF8pthkyODjc4CDagoODww0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c9a1c9-2f06-476f-8b0d-6d01032874a2</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 0.221853 eNqlkrkOwjAMQH/Jliw1QweGDhlCQSLQDB0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0173029a-161d-40ef-af28-2342915b22fb</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>fea47298-266a-4cf4-93bd-55d1bcc2fc7d</td>\n",
       "      <td>1728</td>\n",
       "      <td>1728</td>\n",
       "      <td>0 1 eNoLCAjJNgIABNkBkg==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>feb955db-6c07-4717-a98b-92236c8e01d8</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>fefb9bb7-934a-40d1-8d2f-210265857388</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>ff069fa2-d948-408e-91b3-034cfea428d1</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>ff23eea9-4bbe-42af-a8da-9ae16321fc6d</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>559 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ID  ImageWidth  ImageHeight  \\\n",
       "0    0040581b-f1f2-4fbe-b043-b6bfea5404bb        2048         2048   \n",
       "1    004a270d-34a2-4d60-bbe4-365fca868193        2048         2048   \n",
       "2    00537262-883c-4b37-a3a1-a4931b6faea5        2048         2048   \n",
       "3    00c9a1c9-2f06-476f-8b0d-6d01032874a2        2048         2048   \n",
       "4    0173029a-161d-40ef-af28-2342915b22fb        3072         3072   \n",
       "..                                    ...         ...          ...   \n",
       "554  fea47298-266a-4cf4-93bd-55d1bcc2fc7d        1728         1728   \n",
       "555  feb955db-6c07-4717-a98b-92236c8e01d8        2048         2048   \n",
       "556  fefb9bb7-934a-40d1-8d2f-210265857388        2048         2048   \n",
       "557  ff069fa2-d948-408e-91b3-034cfea428d1        3072         3072   \n",
       "558  ff23eea9-4bbe-42af-a8da-9ae16321fc6d        2048         2048   \n",
       "\n",
       "                                      PredictionString  \n",
       "0    0 0.334347 eNqtVD2PgzAM/UtJz5wYOtzAwClp5CGCDBk...  \n",
       "1    0 0.311836 eNoLCAhINIgxTMkzNAADGxcInWDgAGFYeKD...  \n",
       "2    0 0.229776 eNqtkb0KAyEMgF8pthkyODjc4CDagoODww0...  \n",
       "3    0 0.221853 eNqlkrkOwjAMQH/Jliw1QweGDhlCQSLQDB0...  \n",
       "4                             0 1 eNoLCAgIsAQABJ4Beg==  \n",
       "..                                                 ...  \n",
       "554                           0 1 eNoLCAjJNgIABNkBkg==  \n",
       "555                           0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "556                           0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "557                           0 1 eNoLCAgIsAQABJ4Beg==  \n",
       "558                           0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "\n",
       "[559 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if debug:\n",
    "    submit_df.iloc[:batch_size, :][\"PredictionString\"] = all_predictions\n",
    "else:\n",
    "    submit_df[\"PredictionString\"] = all_predictions\n",
    "submit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_result(probs, img_ids, th=0.5):\n",
    "    probs = np.concatenate(probs, axis=0)\n",
    "    predicted_probs = probs.copy()\n",
    "    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n",
    "\n",
    "    pred_list = []\n",
    "    pred_list_new = []\n",
    "    for line in probs:\n",
    "        # Map old classes to new ones\n",
    "        predicted_old_classes = sorted(\n",
    "            list(set([i for i in np.nonzero(line > th)[0]])))\n",
    "        predicted_new_classes = sorted(\n",
    "            list(set([old_class_mappings[i]\n",
    "                      for i in np.nonzero(line > th)[0]])))\n",
    "        # print(predicted_classes)\n",
    "        s = '|'.join([str(i) for i in predicted_old_classes])\n",
    "        s_new = '|'.join([str(i) for i in predicted_new_classes])\n",
    "        pred_list.append(s)\n",
    "        pred_list_new.append(s_new)\n",
    "    result_df = pd.DataFrame({\n",
    "        # \"ID\": img_ids,\n",
    "        \"Predicted\": pred_list,\n",
    "        \"Predicted_New\": pred_list_new\n",
    "    })\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Predicted_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2|25</td>\n",
       "      <td>2|16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Predicted Predicted_New\n",
       "0        25            16\n",
       "1      2|25          2|16\n",
       "2         0             0\n",
       "3        25            16\n",
       "4        25            16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = prob_to_result(final_probs, None, th=confidence_threshold)\n",
    "result_df.to_csv(\"result_comparison.csv\", index=False)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted</th>\n",
       "      <th>Predicted_New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2|25</td>\n",
       "      <td>2|16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Predicted Predicted_New\n",
       "0         25            16\n",
       "1       2|25          2|16\n",
       "2          0             0\n",
       "3         25            16\n",
       "4         25            16\n",
       "5         25            16\n",
       "6         25            16\n",
       "7         25            16\n",
       "8         25            16\n",
       "9         25            16\n",
       "10        25            16\n",
       "11        25            16\n",
       "12        25            16\n",
       "13        25            16\n",
       "14        25            16\n",
       "15        25            16\n",
       "16        25            16\n",
       "17        25            16\n",
       "18        25            16\n",
       "19        25            16\n",
       "20        25            16\n",
       "21        25            16\n",
       "22        25            16\n",
       "23        25            16\n",
       "24        25            16\n",
       "25        25            16\n",
       "26        25            16\n",
       "27        25            16\n",
       "28        23            14\n",
       "29        23            14\n",
       "30        23            14\n",
       "31        23            14\n",
       "32        23            14\n",
       "33        23            14\n",
       "34        23            14\n",
       "35        23            14\n",
       "36         7             7\n",
       "37        23            14\n",
       "38        23            14\n",
       "39        23            14\n",
       "40        25            16\n",
       "41        23            14\n",
       "42        23            14\n",
       "43        23            14\n",
       "44        23            14\n",
       "45        25            16\n",
       "46        23            14\n",
       "47        23            14\n",
       "48        23            14\n",
       "49        23            14\n",
       "50        23            14\n",
       "51        23            14\n",
       "52        23            14\n",
       "53        23            14\n",
       "54        23            14\n",
       "55        23            14\n",
       "56        25            16\n",
       "57        23            14\n",
       "58        23            14\n",
       "59        23            14\n",
       "60        23            14\n",
       "61        23            14\n",
       "62        25            16\n",
       "63        25            16\n",
       "64        18            18\n",
       "65         7             7\n",
       "66         7             7\n",
       "67         7             7\n",
       "68        18            18\n",
       "69         7             7\n",
       "70         7             7\n",
       "71         7             7\n",
       "72         7             7\n",
       "73         7             7\n",
       "74         7             7\n",
       "75         7             7\n",
       "76         7             7\n",
       "77         7             7\n",
       "78         7             7\n",
       "79         7             7\n",
       "80         7             7\n",
       "81         7             7\n",
       "82         7             7\n",
       "83         7             7\n",
       "84         7             7\n",
       "85        25            16"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16      32\n",
       "14      30\n",
       "7       20\n",
       "18       2\n",
       "0        1\n",
       "2|16     1\n",
       "Name: Predicted_New, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df[\"Predicted_New\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset.release_gpu()\n",
    "# del model, test_dataset, test_loader\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm densenet121-a639ec97.pth hpa_cell_segment.py\n",
    "# !rm -rf inference test_cell_masks\n",
    "# !ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

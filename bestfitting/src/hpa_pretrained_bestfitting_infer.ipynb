{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run on 2fb1688be1a5\n",
      "PyTorch Version: 1.6.0+cu101\n",
      "PyTorch Lightning Version: 1.1.1\n"
     ]
    }
   ],
   "source": [
    "kernel_mode = False\n",
    "\n",
    "import sys\n",
    "if kernel_mode:\n",
    "    sys.path.insert(0, \"../input/pytorch-lightning\")\n",
    "    sys.path.insert(0, \"../input/hpa-bestfitting\")\n",
    "    sys.path.insert(0, \"../input/hpa-cell-segmentation\")\n",
    "\n",
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from torch.nn import DataParallel\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from config.config import *\n",
    "from utils.common_util import *\n",
    "from networks.imageclsnet import init_network\n",
    "from datasets.protein_dataset import ProteinDataset\n",
    "from utils.augment_util import *\n",
    "from datasets.tool import *\n",
    "\n",
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.metrics.functional import classification\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "rand_seed = 1120\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning Version: {pl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All label names in the public HPA and their corresponding index.\n",
    "all_locations = dict({\n",
    "    \"Nucleoplasm\": 0,\n",
    "    \"Nuclear membrane\": 1,\n",
    "    \"Nucleoli\": 2,\n",
    "    \"Nucleoli fibrillar center\": 3,\n",
    "    \"Nuclear speckles\": 4,\n",
    "    \"Nuclear bodies\": 5,\n",
    "    \"Endoplasmic reticulum\": 6,\n",
    "    \"Golgi apparatus\": 7,\n",
    "    \"Intermediate filaments\": 8,\n",
    "    \"Actin filaments\": 9,\n",
    "    \"Focal adhesion sites\": 9,\n",
    "    \"Microtubules\": 10,\n",
    "    \"Mitotic spindle\": 11,\n",
    "    \"Centrosome\": 12,\n",
    "    \"Centriolar satellite\": 12,\n",
    "    \"Plasma membrane\": 13,\n",
    "    \"Cell Junctions\": 13,\n",
    "    \"Mitochondria\": 14,\n",
    "    \"Aggresome\": 15,\n",
    "    \"Cytosol\": 16,\n",
    "    \"Vesicles\": 17,\n",
    "    \"Peroxisomes\": 17,\n",
    "    \"Endosomes\": 17,\n",
    "    \"Lysosomes\": 17,\n",
    "    \"Lipid droplets\": 17,\n",
    "    \"Cytoplasmic bodies\": 17,\n",
    "    \"No staining\": 18,\n",
    "    # markpeng\n",
    "    \"Rods & rings\": 18,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n",
    "bestfitting_folder = \"/workspace/Github/HPA-competition-solutions/bestfitting\"\n",
    "test_image_folder = f\"{dataset_folder}/test/\"\n",
    "NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "\n",
    "image_size = 768\n",
    "crop_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference\t       test\t       train\t  train_tfrecords\r\n",
      "sample_submission.csv  test_tfrecords  train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls {dataset_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{dataset_folder}/train.csv\")\n",
    "submit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>8|5|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>14|0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60b57878-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>6|1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>16|10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5b931256-bb99-11e8-b2b9-ac1f6b6435d0</td>\n",
       "      <td>14|0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID  Label\n",
       "0  5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0  8|5|0\n",
       "1  5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0   14|0\n",
       "2  60b57878-bb99-11e8-b2b9-ac1f6b6435d0    6|1\n",
       "3  5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0  16|10\n",
       "4  5b931256-bb99-11e8-b2b9-ac1f6b6435d0   14|0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(559, 4) 1728 3072\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ImageWidth</th>\n",
       "      <th>ImageHeight</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0040581b-f1f2-4fbe-b043-b6bfea5404bb</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004a270d-34a2-4d60-bbe4-365fca868193</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00537262-883c-4b37-a3a1-a4931b6faea5</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00c9a1c9-2f06-476f-8b0d-6d01032874a2</td>\n",
       "      <td>2048</td>\n",
       "      <td>2048</td>\n",
       "      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0173029a-161d-40ef-af28-2342915b22fb</td>\n",
       "      <td>3072</td>\n",
       "      <td>3072</td>\n",
       "      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID  ImageWidth  ImageHeight  \\\n",
       "0  0040581b-f1f2-4fbe-b043-b6bfea5404bb        2048         2048   \n",
       "1  004a270d-34a2-4d60-bbe4-365fca868193        2048         2048   \n",
       "2  00537262-883c-4b37-a3a1-a4931b6faea5        2048         2048   \n",
       "3  00c9a1c9-2f06-476f-8b0d-6d01032874a2        2048         2048   \n",
       "4  0173029a-161d-40ef-af28-2342915b22fb        3072         3072   \n",
       "\n",
       "           PredictionString  \n",
       "0  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "1  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "2  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "3  0 1 eNoLCAgIMAEABJkBdQ==  \n",
       "4  0 1 eNoLCAgIsAQABJ4Beg==  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(submit_df.shape, submit_df.ImageWidth.min(), submit_df.ImageWidth.max())\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook\n",
    "def binary_mask_to_ascii(mask, mask_val=1):\n",
    "    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n",
    "    mask = np.where(mask == mask_val, 1, 0).astype(np.bool)\n",
    "\n",
    "    # check input mask --\n",
    "    if mask.dtype != np.bool:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a binary mask, received dtype == {mask.dtype}\"\n",
    "        )\n",
    "\n",
    "    mask = np.squeeze(mask)\n",
    "    if len(mask.shape) != 2:\n",
    "        raise ValueError(\n",
    "            f\"encode_binary_mask expects a 2d mask, received shape == {mask.shape}\"\n",
    "        )\n",
    "\n",
    "    # convert input mask to expected COCO API input --\n",
    "    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "    mask_to_encode = mask_to_encode.astype(np.uint8)\n",
    "    mask_to_encode = np.asfortranarray(mask_to_encode)\n",
    "\n",
    "    # RLE encode mask --\n",
    "    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n",
    "\n",
    "    # compress and base64 encoding --\n",
    "    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n",
    "    base64_str = base64.b64encode(binary_str)\n",
    "    return base64_str.decode()\n",
    "\n",
    "\n",
    "def rle_encoding(img, mask_val=1):\n",
    "    \"\"\"\n",
    "    Turns our masks into RLE encoding to easily store them\n",
    "    and feed them into models later on\n",
    "    https://en.wikipedia.org/wiki/Run-length_encoding\n",
    "    \n",
    "    Args:\n",
    "        img (np.array): Segmentation array\n",
    "        mask_val (int): Which value to use to create the RLE\n",
    "        \n",
    "    Returns:\n",
    "        RLE string\n",
    "    \n",
    "    \"\"\"\n",
    "    dots = np.where(img.T.flatten() == mask_val)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "\n",
    "    return ' '.join([str(x) for x in run_lengths])\n",
    "\n",
    "\n",
    "def rle_to_mask(rle_string, height, width):\n",
    "    \"\"\" Convert RLE sttring into a binary mask \n",
    "    \n",
    "    Args:\n",
    "        rle_string (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array of the binary segmentation mask for a given cell\n",
    "    \"\"\"\n",
    "    rows, cols = height, width\n",
    "    rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n",
    "    rle_pairs = np.array(rle_numbers).reshape(-1, 2)\n",
    "    img = np.zeros(rows * cols, dtype=np.uint8)\n",
    "    for index, length in rle_pairs:\n",
    "        index -= 1\n",
    "        img[index:index + length] = 255\n",
    "    img = img.reshape(cols, rows)\n",
    "    img = img.T\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_segmentation_maps(list_of_image_lists, segmentator, batch_size=8):\n",
    "    \"\"\" Function to generate segmentation maps using CellSegmentator tool \n",
    "    \n",
    "    Args:\n",
    "        list_of_image_lists (list of lists):\n",
    "            - [[micro-tubules(red)], [endoplasmic-reticulum(yellow)], [nucleus(blue)]]\n",
    "        batch_size (int): Batch size to use in generating the segmentation masks\n",
    "        \n",
    "    Returns:\n",
    "        List of lists containing RLEs for all the cells in all images\n",
    "    \"\"\"\n",
    "\n",
    "    all_mask_rles = {}\n",
    "    for i in tqdm(range(0, len(list_of_image_lists[0]), batch_size),\n",
    "                  total=len(list_of_image_lists[0]) // batch_size):\n",
    "\n",
    "        # Get batch of images\n",
    "        sub_images = [\n",
    "            img_channel_list[i:i + batch_size]\n",
    "            for img_channel_list in list_of_image_lists\n",
    "        ]  # 0.000001 seconds\n",
    "\n",
    "        # Do segmentation\n",
    "        cell_segmentations = segmentator.pred_cells(sub_images)\n",
    "        nuc_segmentations = segmentator.pred_nuclei(sub_images[2])\n",
    "\n",
    "        # post-processing\n",
    "        for j, path in enumerate(sub_images[0]):\n",
    "            img_id = path.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1]\n",
    "            nuc_mask, cell_mask = label_cell(nuc_segmentations[j],\n",
    "                                             cell_segmentations[j])\n",
    "            new_name = os.path.basename(path).replace('red', 'mask')\n",
    "            all_mask_rles[img_id] = [\n",
    "                rle_encoding(cell_mask, mask_val=k)\n",
    "                for k in range(1,\n",
    "                               np.max(cell_mask) + 1)\n",
    "            ]\n",
    "    return all_mask_rles\n",
    "\n",
    "\n",
    "def get_img_list(img_dir, return_ids=False, sub_n=None):\n",
    "    \"\"\" Get image list in the format expected by the CellSegmentator tool \"\"\"\n",
    "    if sub_n is None:\n",
    "        sub_n = len(glob(img_dir + '/' + f'*_red.png'))\n",
    "    if return_ids:\n",
    "        images = [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "        return [\n",
    "            x.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1] for x in images[0]\n",
    "        ], images\n",
    "    else:\n",
    "        return [\n",
    "            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n",
    "            for c in [\"red\", \"yellow\", \"blue\"]\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_rle(\n",
    "    rle,\n",
    "    width,\n",
    "    height,\n",
    "    return_mask=True,\n",
    "):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        rle (rle_string): Run length encoding containing \n",
    "            segmentation mask information\n",
    "        height (int): Height of the original image the map comes from\n",
    "        width (int): Width of the original image the map comes from\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    mask = rle_to_mask(rle, height, width).copy()\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    x, y, w, h = cv2.boundingRect(cnts[0])\n",
    "\n",
    "    if return_mask:\n",
    "        return (x, y, x + w, y + h), mask\n",
    "    else:\n",
    "        return (x, y, x + w, y + h)\n",
    "\n",
    "\n",
    "def get_contour_bbox_from_raw(raw_mask):\n",
    "    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n",
    "    \n",
    "    Args:\n",
    "        raw_mask (nparray): Numpy array containing segmentation mask information\n",
    "    \n",
    "    Returns:\n",
    "        Numpy array for a cell bounding box coordinates\n",
    "    \"\"\"\n",
    "    cnts = grab_contours(\n",
    "        cv2.findContours(raw_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n",
    "    xywhs = [cv2.boundingRect(cnt) for cnt in cnts]\n",
    "    xys = [(xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3])\n",
    "           for xywh in xywhs]\n",
    "    return sorted(xys, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "\n",
    "def pad_to_square(a):\n",
    "    \"\"\" Pad an array `a` evenly until it is a square \"\"\"\n",
    "    if a.shape[1] > a.shape[0]:  # pad height\n",
    "        n_to_add = a.shape[1] - a.shape[0]\n",
    "        top_pad = n_to_add // 2\n",
    "        bottom_pad = n_to_add - top_pad\n",
    "        a = np.pad(a, [(top_pad, bottom_pad), (0, 0), (0, 0)], mode='constant')\n",
    "\n",
    "    elif a.shape[0] > a.shape[1]:  # pad width\n",
    "        n_to_add = a.shape[0] - a.shape[1]\n",
    "        left_pad = n_to_add // 2\n",
    "        right_pad = n_to_add - left_pad\n",
    "        a = np.pad(a, [(0, 0), (left_pad, right_pad), (0, 0)], mode='constant')\n",
    "    else:\n",
    "        pass\n",
    "    return a\n",
    "\n",
    "\n",
    "def cut_out_cells(rgby,\n",
    "                  rles,\n",
    "                  resize_to=(256, 256),\n",
    "                  square_off=True,\n",
    "                  return_masks=False,\n",
    "                  from_raw=True):\n",
    "    \"\"\" Cut out the cells as padded square images \n",
    "    \n",
    "    Args:\n",
    "        rgby (np.array): 4 Channel image to be cut into tiles\n",
    "        rles (list of RLE strings): List of run length encoding containing \n",
    "            segmentation mask information\n",
    "        resize_to (tuple of ints, optional): The square dimension to resize the image to\n",
    "        square_off (bool, optional): Whether to pad the image to a square or not\n",
    "        \n",
    "    Returns:\n",
    "        list of square arrays representing squared off cell images\n",
    "    \"\"\"\n",
    "    w, h = rgby.shape[:2]\n",
    "    contour_bboxes = [\n",
    "        get_contour_bbox(rle, w, h, return_mask=return_masks) for rle in rles\n",
    "    ]\n",
    "    if return_masks:\n",
    "        masks = [x[-1] for x in contour_bboxes]\n",
    "        contour_bboxes = [x[:-1] for x in contour_bboxes]\n",
    "\n",
    "    arrs = [\n",
    "        rgby[bbox[1]:bbox[3], bbox[0]:bbox[2], ...] for bbox in contour_bboxes\n",
    "    ]\n",
    "    if square_off:\n",
    "        arrs = [pad_to_square(arr) for arr in arrs]\n",
    "\n",
    "    if resize_to is not None:\n",
    "        arrs = [\n",
    "            cv2.resize(pad_to_square(arr).astype(np.float32),\n",
    "                       resize_to,\n",
    "                       interpolation=cv2.INTER_CUBIC) \\\n",
    "            for arr in arrs\n",
    "        ]\n",
    "    if return_masks:\n",
    "        return arrs, masks\n",
    "    else:\n",
    "        return arrs\n",
    "\n",
    "\n",
    "def grab_contours(cnts):\n",
    "    # if the length the contours tuple returned by cv2.findContours\n",
    "    # is '2' then we are using either OpenCV v2.4, v4-beta, or\n",
    "    # v4-official\n",
    "    if len(cnts) == 2:\n",
    "        cnts = cnts[0]\n",
    "\n",
    "    # if the length of the contours tuple is '3' then we are using\n",
    "    # either OpenCV v3, v4-pre, or v4-alpha\n",
    "    elif len(cnts) == 3:\n",
    "        cnts = cnts[1]\n",
    "\n",
    "    # otherwise OpenCV has changed their cv2.findContours return\n",
    "    # signature yet again and I have no idea WTH is going on\n",
    "    else:\n",
    "        raise Exception(\n",
    "            (\"Contours tuple must have length 2 or 3, \"\n",
    "             \"otherwise OpenCV changed their cv2.findContours return \"\n",
    "             \"signature yet again. Refer to OpenCV's documentation \"\n",
    "             \"in that case\"))\n",
    "\n",
    "    # return the actual contours array\n",
    "    return cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/72534\n",
    "def generate_hash(img_dir,\n",
    "                  colors,\n",
    "                  dataset='train',\n",
    "                  imread_func=None,\n",
    "                  is_update=False):\n",
    "    meta = meta.copy()\n",
    "    hash_maps = {}\n",
    "    for color in colors:\n",
    "        hash_maps[color] = []\n",
    "        for idx in tqdm(range(len(meta)), desc='train %s' % color):\n",
    "            img = imread_func(img_dir, meta.iloc[idx][ID], color)\n",
    "            hash = imagehash.phash(img)\n",
    "            hash_maps[color].append(hash)\n",
    "\n",
    "    for color in colors:\n",
    "        meta[color] = hash_maps[color]\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def calc_hash(params):\n",
    "    color, threshold, base_test_hash1, base_test_hash2, test_ids1, test_ids2 = params\n",
    "\n",
    "    test_hash1 = base_test_hash1.reshape(1, -1)  # 1*m\n",
    "\n",
    "    test_idxes_list1 = []\n",
    "    test_idxes_list2 = []\n",
    "    hash_list = []\n",
    "\n",
    "    step = 5\n",
    "    for test_idx in tqdm(range(0, len(base_test_hash2), step), desc=color):\n",
    "        test_hash2 = base_test_hash2[test_idx:test_idx + step].reshape(\n",
    "            -1, 1)  # n*1\n",
    "        hash = test_hash2 - test_hash1  # n*m\n",
    "        test_idxes2, test_idxes1 = np.where(hash <= threshold)\n",
    "        hash = hash[test_idxes2, test_idxes1]\n",
    "\n",
    "        test_idxes2 = test_idxes2 + test_idx\n",
    "\n",
    "        test_idxes_list1.extend(test_idxes1.tolist())\n",
    "        test_idxes_list2.extend(test_idxes2.tolist())\n",
    "        hash_list.extend(hash.tolist())\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Test1': test_ids1[test_idxes_list1],\n",
    "        'Test2': test_ids2[test_idxes_list2],\n",
    "        'Sim%s' % color[:1].upper(): hash_list\n",
    "    })\n",
    "    df = df[df['Test1'] != df['Test2']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "559\n"
     ]
    }
   ],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\"]\n",
    "\n",
    "# Extract unique image IDs\n",
    "test_ids = [\n",
    "    f.split(\"/\")[-1].replace(\"_red.png\", \"\")\n",
    "    for f in glob.glob(f\"{test_image_folder}/*_red.png\")\n",
    "]\n",
    "print(len(test_ids))\n",
    "\n",
    "# Estimated number of private test images (RGBY): 2236 x 2.3 ~= 5143 (for 9 hours we have 6.2 secs per image)\n",
    "# Estimated number of private test images: 559 x 2.3 ~= 1286 (for 9 hours we have 25.2 secs per image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Resize and Save All Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!python {bestfitting_folder}/src/data_process/s2_resize_png_image.py \\\n",
    "    --source {dataset_folder} \\\n",
    "    --dest {dataset_folder} \\\n",
    "    --dataset test \\\n",
    "    --size {image_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def convert_image(image_folder, image_id, size):\n",
    "#     print(f\"Convering image {image_id}\")\n",
    "#     rgby = []\n",
    "#     for c in colors:\n",
    "#         image = np.array(Image.open(\n",
    "#             os.path.join(image_folder, image_id + f\"_{c}.png\")),\n",
    "#                          dtype=np.float32)\n",
    "\n",
    "#         # Resize\n",
    "#         image = cv2.resize(image, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "#         rgby.append(image)\n",
    "\n",
    "#     return np.stack(rgby, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# test_images = [convert_image(test_image_folder, id, image_size) for id in test_ids]\n",
    "# print(f\"Time spent on loading test images: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# print(f\"Resized test images: {sys.getsizeof(test_images)/1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Find Similar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# similarity_threshold = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# colors = ['red', 'green', 'blue']\n",
    "# test_meta = generate_hash(test_img_dir,\n",
    "#                           test_meta,\n",
    "#                           colors,\n",
    "#                           dataset='test',\n",
    "#                           imread_func=test_imread,\n",
    "#                           is_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting\n",
    "class ProteinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir=None,\n",
    "        img_size=512,\n",
    "        transform=None,\n",
    "        return_label=True,\n",
    "        in_channels=4,\n",
    "        crop_size=0,\n",
    "        random_crop=False,\n",
    "    ):\n",
    "        self.img_size = img_size\n",
    "        self.return_label = return_label\n",
    "        self.in_channels = in_channels\n",
    "        self.transform = transform\n",
    "        self.crop_size = crop_size\n",
    "        self.random_crop = random_crop\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "        self.img_ids = test_ids\n",
    "        self.num = len(self.img_ids)\n",
    "\n",
    "    def read_crop_img(self, img):\n",
    "        random_crop_size = int(np.random.uniform(self.crop_size,\n",
    "                                                 self.img_size))\n",
    "        x = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "        y = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "        crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n",
    "        return crop_img\n",
    "\n",
    "    def read_rgby(self, img_dir, img_id, index):\n",
    "        suffix = '.png'\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "        flags = cv2.IMREAD_GRAYSCALE\n",
    "        img = [\n",
    "            cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n",
    "            for color in colors\n",
    "        ]\n",
    "        img = np.stack(img, axis=-1)\n",
    "        if self.random_crop and self.crop_size > 0:\n",
    "            img = self.read_crop_img(img)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index]\n",
    "        image = self.read_rgby(self.img_dir, img_id, index)\n",
    "        if image[0] is None:\n",
    "            print(img_dir, img_id)\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if self.crop_size > 0:\n",
    "            if self.crop_size != h or self.crop_size != w:\n",
    "                image = cv2.resize(image, (self.crop_size, self.crop_size),\n",
    "                                   interpolation=cv2.INTER_LINEAR)\n",
    "        else:\n",
    "            if self.img_size != h or self.img_size != w:\n",
    "                image = cv2.resize(image, (self.img_size, self.img_size),\n",
    "                                   interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        image = image / 255.0\n",
    "        image = image_to_tensor(image)\n",
    "\n",
    "        if self.return_label:\n",
    "            label = self.labels[index]\n",
    "            return image, label, index\n",
    "        else:\n",
    "            return image, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512, 512]) 0\n"
     ]
    }
   ],
   "source": [
    "tmp_dataset = ProteinDataset(\n",
    "    img_dir=\"/workspace/Kaggle/HPA/hpa_2020/inference/test/images_768\",\n",
    "    img_size=768,\n",
    "    return_label=False,\n",
    "    in_channels=4,\n",
    "    transform=None,\n",
    "    crop_size=512,\n",
    "    random_crop=True,\n",
    ")\n",
    "tmpiter = iter(tmp_dataset)\n",
    "tmp_img, tmp_index = next(tmpiter)\n",
    "print(tmp_img.shape, tmp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei\n",
    "\n",
    "\n",
    "# Based on https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting\n",
    "class SegmentedProteinDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir=None,\n",
    "        img_size=512,\n",
    "        transform=None,\n",
    "        return_label=True,\n",
    "        in_channels=4,\n",
    "        crop_size=0,\n",
    "        random_crop=False,\n",
    "    ):\n",
    "        self.img_size = img_size\n",
    "        self.return_label = return_label\n",
    "        self.in_channels = in_channels\n",
    "        self.transform = transform\n",
    "        self.crop_size = crop_size\n",
    "        self.random_crop = random_crop\n",
    "\n",
    "        self.segmentator = cellsegmentator.CellSegmentator(\n",
    "            NUC_MODEL,\n",
    "            CELL_MODEL,\n",
    "            scale_factor=0.25,\n",
    "            device=\"cuda\",\n",
    "            padding=True,\n",
    "            multi_channel_model=True,\n",
    "        )\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.img_ids = test_ids\n",
    "        self.num = len(self.img_ids)\n",
    "\n",
    "    def read_crop_img(self, img):\n",
    "        random_crop_size = int(np.random.uniform(self.crop_size,\n",
    "                                                 self.img_size))\n",
    "        x = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "        y = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "        crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n",
    "        return crop_img\n",
    "\n",
    "    def read_rgby(self, img_dir, img_id, index):\n",
    "        suffix = '.png'\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "        flags = cv2.IMREAD_GRAYSCALE\n",
    "        rgby_img = [\n",
    "            cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n",
    "            for color in colors\n",
    "        ]\n",
    "        rgby_img = np.stack(rgby_img, axis=-1)\n",
    "        if self.random_crop and self.crop_size > 0:\n",
    "            rgby_img = self.read_crop_img(rgby_img)\n",
    "\n",
    "        return rgby_img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.img_ids[index]\n",
    "        rgby_img = self.read_rgby(self.img_dir, img_id, index)\n",
    "        if rgby_img[0] is None:\n",
    "            print(img_dir, img_id)\n",
    "\n",
    "        h, w = rgby_img.shape[:2]\n",
    "        if self.crop_size > 0:\n",
    "            if self.crop_size != h or self.crop_size != w:\n",
    "                rgby_img = cv2.resize(rgby_img,\n",
    "                                      (self.crop_size, self.crop_size),\n",
    "                                      interpolation=cv2.INTER_LINEAR)\n",
    "        else:\n",
    "            if self.img_size != h or self.img_size != w:\n",
    "                rgby_img = cv2.resize(rgby_img, (self.img_size, self.img_size),\n",
    "                                      interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            rgby_img = self.transform(rgby_img)\n",
    "        rgby_img = rgby_img / 255.0\n",
    "\n",
    "        # For nuclei\n",
    "        nuc_segmentations = self.segmentator.pred_nuclei([rgby_img[2, :, :]])\n",
    "\n",
    "        # For full cells\n",
    "        cell_segmentations = self.segmentator.pred_cells([[rgby_img[:, :, 0]]\n",
    "                                                          for img in [0, 3, 2]\n",
    "                                                          ])\n",
    "        print(len(cell_segmentations), cell_segmentations[0].shape)\n",
    "\n",
    "        rgby_img = image_to_tensor(rgby_img)\n",
    "\n",
    "        if self.return_label:\n",
    "            label = self.labels[index]\n",
    "            return rgby_img, label, index\n",
    "        else:\n",
    "            return rgby_img, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 (512, 512, 3)\n",
      "torch.Size([4, 512, 512]) 0\n"
     ]
    }
   ],
   "source": [
    "tmp_dataset = SegmentedProteinDataset(\n",
    "    img_dir=\"/workspace/Kaggle/HPA/hpa_2020/inference/test/images_768\",\n",
    "    img_size=768,\n",
    "    return_label=False,\n",
    "    in_channels=4,\n",
    "    transform=None,\n",
    "    crop_size=512,\n",
    "    random_crop=True,\n",
    ")\n",
    "tmpiter = iter(tmp_dataset)\n",
    "tmp_img, tmp_index = next(tmpiter)\n",
    "print(tmp_img.shape, tmp_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting\n",
    "\n",
    "\n",
    "class HPATestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 image_folder,\n",
    "                 img_size=768,\n",
    "                 crop_size=512,\n",
    "                 transform=None):\n",
    "        self.img_size = img_size\n",
    "        self.crop_size = crop_size\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"{len(self.image_files)} images for inference\")\n",
    "\n",
    "        mt = glob.glob(save_dir + '/' + '*_red.png')\n",
    "        er = [f.replace('red', 'yellow') for f in mt]\n",
    "        nu = [f.replace('red', 'blue') for f in mt]\n",
    "        protein = [f.replace('red', 'green') for f in mt]\n",
    "        self.image_files = [mt, er, nu]\n",
    "\n",
    "        # Extract cell masks from each image\n",
    "        self.images = []\n",
    "        for mt, er, nu in self.image_files:\n",
    "            red_image = self.convert(image_path, self.img_size)\n",
    "            yellow_image = self.convert(image_path, self.img_size)\n",
    "            blue_image = self.convert(image_path, self.img_size)\n",
    "            green_image = self.convert(image_path, self.img_size)\n",
    "\n",
    "        # NUC_MODEL = \"/workspace/Kaggle/HPA/nuclei-model.pth\"\n",
    "        # CELL_MODEL = \"/workspace/Kaggle/HPA/cell-model.pth\"\n",
    "        NUC_MODEL = \"./nuclei-model.pth\"\n",
    "        CELL_MODEL = \"./cell-model.pth\"\n",
    "        segmentator = cellsegmentator.CellSegmentator(\n",
    "            NUC_MODEL,\n",
    "            CELL_MODEL,\n",
    "            scale_factor=0.25,\n",
    "            device=\"cuda\",\n",
    "            padding=False,\n",
    "            multi_channel_model=True,\n",
    "        )\n",
    "\n",
    "        # For nuclei\n",
    "        nuc_segmentations = segmentator.pred_nuclei([images[2, :, :]])\n",
    "\n",
    "        # For full cells\n",
    "        cell_segmentations = segmentator.pred_cells(images)\n",
    "\n",
    "        # post-processing\n",
    "        for i, pred in enumerate(cell_segmentations):\n",
    "            nuclei_mask, cell_mask = label_cell(nuc_segmentations[i],\n",
    "                                                cell_segmentations[i])\n",
    "\n",
    "    def convert(self, image_path, size):\n",
    "        image = np.array(Image.open(image_path), dtype=np.float32)\n",
    "        print(image.shape)\n",
    "\n",
    "        # Resize\n",
    "        image = cv2.resize(image, (size, size), interpolation=cv2.INTER_LINEAR)\n",
    "        return image\n",
    "\n",
    "    def read_crop_img(self, img):\n",
    "        random_crop_size = int(np.random.uniform(self.crop_size,\n",
    "                                                 self.img_size))\n",
    "        x = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "        y = int(np.random.uniform(0, self.img_size - random_crop_size))\n",
    "        crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n",
    "        return crop_img\n",
    "\n",
    "    def read_rgby(self, img_dir, img_id, index):\n",
    "        if self.is_external[index]:\n",
    "            img_is_external = True\n",
    "        else:\n",
    "            img_is_external = False\n",
    "\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "        flags = cv2.IMREAD_GRAYSCALE\n",
    "        img = [\n",
    "            cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n",
    "            for color in colors\n",
    "        ]\n",
    "        img = np.stack(img, axis=-1)\n",
    "        if self.random_crop and self.crop_size > 0:\n",
    "            img = self.read_crop_img(img)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_files[index]\n",
    "\n",
    "        image = self.convert(image_path, self.img_size)\n",
    "        return {\"x\": image, \"y\": -1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp = HPATestDataset(test_image_folder)\n",
    "next(iter(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model from Bestfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = ['test', 'val']\n",
    "split_names = ['random_ext_folds5', 'random_ext_noleak_clean_folds5']\n",
    "augment_list = ['default', 'flipud', 'fliplr','transpose', 'flipud_lr',\n",
    "                'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Protein Classification')\n",
    "parser.add_argument('--out_dir', type=str, help='destination where predicted result should be saved')\n",
    "parser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for predicting (default: 0)')\n",
    "parser.add_argument('--arch', default='class_densenet121_dropout', type=str,\n",
    "                    help='model architecture (default: class_densenet121_dropout)')\n",
    "parser.add_argument('--num_classes', default=28, type=int, help='number of classes (default: 28)')\n",
    "parser.add_argument('--in_channels', default=4, type=int, help='in channels (default: 4)')\n",
    "parser.add_argument('--img_size', default=768, type=int, help='image size (default: 768)')\n",
    "parser.add_argument('--crop_size', default=512, type=int, help='crop size (default: 512)')\n",
    "parser.add_argument('--batch_size', default=32, type=int, help='train mini-batch size (default: 32)')\n",
    "parser.add_argument('--workers', default=3, type=int, help='number of data loading workers (default: 3)')\n",
    "parser.add_argument('--fold', default=0, type=int, help='index of fold (default: 0)')\n",
    "parser.add_argument('--augment', default='default', type=str, help='test augmentation (default: default)')\n",
    "parser.add_argument('--seed', default=100, type=int, help='random seed (default: 100)')\n",
    "parser.add_argument('--seeds', default=None, type=str, help='predict seed')\n",
    "parser.add_argument('--dataset', default='test', type=str, choices=datasets_names,\n",
    "                    help='dataset options: ' + ' | '.join(datasets_names) + ' (default: test)')\n",
    "parser.add_argument('--split_name', default='random_ext_folds5', type=str, choices=split_names,\n",
    "                    help='split name options: ' + ' | '.join(split_names) + ' (default: random_ext_folds5)')\n",
    "parser.add_argument('--predict_epoch', default=None, type=int, help='number epoch to predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = \"/workspace/Kaggle/HPA/hpa_2020/test\"\n",
    "args = parser.parse_args([\n",
    "    \"--out_dir\", \"external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds\",\n",
    "    \"--gpu_id\", \"0\",\n",
    "    \"--arch\", \"class_densenet121_dropout\",\n",
    "    \"--img_size\", \"768\",\n",
    "    \"--crop_size\", \"512\",\n",
    "    \"--seeds\", \"1120\",\n",
    "    \"--batch_size\", \"8\",\n",
    "    \"--fold\", \"0\",\n",
    "    \"--dataset\", \"test\"\n",
    "])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.predict_epoch = 'final' if args.predict_epoch is None else '%03d' % args.predict_epoch\n",
    "# network_path = opj(RESULT_DIR, 'models', args.out_dir, 'fold%d' % args.fold,\n",
    "#                    '%s.pth' % args.predict_epoch)\n",
    "network_path = f\"{bestfitting_folder}/external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds/fold0/final.pth\"\n",
    "\n",
    "# setting up the visible GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "\n",
    "model_params = {}\n",
    "model_params['architecture'] = args.arch\n",
    "model_params['num_classes'] = args.num_classes\n",
    "model_params['in_channels'] = args.in_channels\n",
    "model = init_network(model_params)\n",
    "\n",
    "# log.write(\">> Loading network:\\n>>>> '{}'\\n\".format(network_path))\n",
    "checkpoint = torch.load(network_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "# log.write(\">>>> loaded network:\\n>>>> epoch {}\\n\".format(checkpoint['epoch']))\n",
    "\n",
    "# moving network to gpu and eval mode\n",
    "# model = DataParallel(model)\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader, model, submit_out_dir, dataset):\n",
    "    all_probs = []\n",
    "    img_ids = np.array(test_loader.dataset.img_ids)\n",
    "    for it, iter_data in tqdm(enumerate(test_loader, 0),\n",
    "                              total=len(test_loader)):\n",
    "        images, indices = iter_data\n",
    "        images = Variable(images.cuda(), volatile=True)\n",
    "        outputs = model(images)\n",
    "        logits = outputs\n",
    "\n",
    "        probs = F.sigmoid(logits).data\n",
    "        all_probs += probs.cpu().numpy().tolist()\n",
    "    img_ids = img_ids[:len(all_probs)]\n",
    "    all_probs = np.array(all_probs).reshape(len(img_ids), -1)\n",
    "\n",
    "    np.save(opj(submit_out_dir, 'prob_%s.npy' % dataset), all_probs)\n",
    "\n",
    "    result_df = prob_to_result(all_probs, img_ids)\n",
    "    result_df.to_csv(opj(submit_out_dir, 'results_%s.csv.gz' % dataset),\n",
    "                     index=False,\n",
    "                     compression='gzip')\n",
    "\n",
    "\n",
    "def prob_to_result(probs, img_ids, th=0.5):\n",
    "    probs = probs.copy()\n",
    "    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n",
    "\n",
    "    pred_list = []\n",
    "    for line in probs:\n",
    "        s = ' '.join(list([str(i) for i in np.nonzero(line > th)[0]]))\n",
    "        pred_list.append(s)\n",
    "    result_df = pd.DataFrame({ID: img_ids, PREDICTED: pred_list})\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading code\n",
    "dataset = args.dataset\n",
    "# if dataset == 'test':\n",
    "#     test_split_file = opj(DATA_DIR, 'split', 'test_11702.csv')\n",
    "# elif dataset == 'val':\n",
    "#     test_split_file = opj(DATA_DIR, 'split', args.split_name, 'random_valid_cv%d.csv' % args.fold)\n",
    "# else:\n",
    "#     raise ValueError('Unsupported or unknown dataset: {}!'.format(dataset))\n",
    "test_split_file = \"/workspace/Kaggle/HPA/hpa_2020/sample_submission.csv\"\n",
    "\n",
    "\n",
    "test_dataset = ProteinDataset(\n",
    "    test_split_file,\n",
    "    img_size=args.img_size,\n",
    "    is_trainset=(dataset != 'test'),\n",
    "    return_label=False,\n",
    "    in_channels=args.in_channels,\n",
    "    transform=None,\n",
    "    crop_size=args.crop_size,\n",
    "    random_crop=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=args.batch_size,\n",
    "    drop_last=False,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [args.seed] if args.seeds is None else [int(i) for i in args.seeds.split(',')]\n",
    "for seed in seeds:\n",
    "    test_dataset.random_crop = (seed != 0)\n",
    "    for augment in args.augment:\n",
    "        test_loader.dataset.transform = eval('augment_%s' % augment)\n",
    "        if args.crop_size > 0:\n",
    "            sub_submit_out_dir = opj(submit_out_dir, '%s_seed%d' % (augment, seed))\n",
    "        else:\n",
    "            sub_submit_out_dir = opj(submit_out_dir, augment)\n",
    "        if not ope(sub_submit_out_dir):\n",
    "            os.makedirs(sub_submit_out_dir)\n",
    "        with torch.no_grad():\n",
    "            predict(test_loader, model, sub_submit_out_dir, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /workspace/Kaggle/HPA/hpa_2020/inference/test/images_768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei\n",
    "import glob\n",
    "\n",
    "mt = glob.glob(test_image_folder + '/' + '*_red.png')\n",
    "er = [f.replace('red', 'yellow') for f in mt]\n",
    "nu = [f.replace('red', 'blue') for f in mt]\n",
    "images = [mt, er, nu]\n",
    "\n",
    "NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n",
    "CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n",
    "# NUC_MODEL = \"/workspace/Kaggle/HPA/nuclei-model.pth\"\n",
    "# CELL_MODEL = \"/workspace/Kaggle/HPA/cell-model.pth\"\n",
    "# NUC_MODEL = \"./nuclei-model.pth\"\n",
    "# CELL_MODEL = \"./cell-model.pth\"\n",
    "segmentator = cellsegmentator.CellSegmentator(\n",
    "    NUC_MODEL,\n",
    "    CELL_MODEL,\n",
    "    scale_factor=0.25,\n",
    "    device=\"cuda\",\n",
    "    padding=True,\n",
    "    multi_channel_model=True,\n",
    ")\n",
    "\n",
    "# For nuclei\n",
    "nuc_segmentations = segmentator.pred_nuclei(images[2])\n",
    "\n",
    "# For full cells\n",
    "cell_segmentations = segmentator.pred_cells(images)\n",
    "\n",
    "# post-processing\n",
    "for i, pred in enumerate(cell_segmentations):\n",
    "    nuclei_mask, cell_mask = label_cell(nuc_segmentations[i],\n",
    "                                        cell_segmentations[i])\n",
    "    FOVname = os.path.basename(mt[i]).replace('red', 'predictedmask')\n",
    "    imageio.imwrite(os.path.join(save_dir, FOVname), cell_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

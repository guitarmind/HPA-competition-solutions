{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n[V1]\n* Resolution: Resized to 512x512 from 768x768\n* Extract cell masks and create individual cell images (512x512)\n* No random crop\n* No TTA\n* Update normalization mean and std with 2021 training and test sets\n\"\"\"\n\nkernel_mode = True\ndebug = False\n\nimport sys\nif kernel_mode:\n    sys.path.insert(0, \"../input/hpa-bestfitting-solution/src\")\n    sys.path.insert(0, \"../input/hpa-cell-segmentation\")","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!ls ../input/hpa-bestfitting-solution/","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"LICENSE\nREADME.md\ndensenet121-a639ec97.pth\nexternal_crop1024_focal_slov_hardlog_clean_class_densenet121_large_dropout_i1536_aug2_5folds\nexternal_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds\nface_all_class_resnet50_dropout_i768_aug2_5folds\nmlcrate-0.2.0-py3-none-any.whl\nrequirements.txt\nsrc\n","output_type":"stream"}]},{"cell_type":"code","source":"# !cp -r ../input/hpa-bestfitting-solution/src/* .\n!cp ../input/hpa-bestfitting-solution/densenet121-a639ec97.pth .\n# !cp -r ../input/hpa-bestfitting-solution/external_* .\n!ls -la","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"total 31600\ndrwxr-xr-x 2 root root     4096 Apr 11 13:20 .\ndrwxr-xr-x 5 root root     4096 Apr 11 13:19 ..\n---------- 1 root root      263 Apr 11 13:19 __notebook_source__.ipynb\n-rw-r--r-- 1 root root 32342954 Apr 11 13:20 densenet121-a639ec97.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install mlcrate-0.2.0-py3-none-any.whl\n!pip install -q \"../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl\"\n!pip install -q \"../input/hpapytorchzoozip/pytorch_zoo-master\"","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import sys\nimport argparse\nfrom tqdm import tqdm\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport math\nimport pickle\nfrom pickle import dump, load\nimport glob\nimport time\nimport collections\n\nimport torch\nimport torch.optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SequentialSampler\nfrom torch.nn import DataParallel\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom config.config import *\nfrom utils.common_util import *\nfrom networks.imageclsnet import init_network\nfrom datasets.protein_dataset import ProteinDataset\nfrom utils.augment_util import *\nfrom datasets.tool import *\n\nimport hpacellseg.cellsegmentator as cellsegmentator\nfrom hpacellseg.utils import label_cell, label_nuclei\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning.metrics.functional import classification\n\nfrom pycocotools import _mask as coco_mask\nimport typing as t\nimport base64\nimport zlib\n\nimport cv2\nfrom PIL import Image\nimport imagehash\n\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nimport gc\ngc.enable()\n\nrand_seed = 1120\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nprint(f\"PyTorch Lightning Version: {pl.__version__}\")","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"run on ff8ce0ffaf4e\nPyTorch Version: 1.7.0\nPyTorch Lightning Version: 1.2.5\n","output_type":"stream"}]},{"cell_type":"code","source":"if kernel_mode:\n    dataset_folder = \"/kaggle/input/hpa-single-cell-image-classification\"\n    bestfitting_folder = \"/kaggle/input/hpa-bestfitting-solution\"\n    test_image_folder = f\"{dataset_folder}/test/\"\n    NUC_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_nuclei_v1.pth\"\n    CELL_MODEL = \"/kaggle/input/hpa-cell-segmentation/dpn_unet_cell_3ch_v1.pth\"\nelse:\n    dataset_folder = \"/workspace/Kaggle/HPA/hpa_2020\"\n    bestfitting_folder = \"/workspace/Github/HPA-competition-solutions/bestfitting\"\n    test_image_folder = f\"{dataset_folder}/test/\"\n    NUC_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_nuclei_v1.pth\"\n    CELL_MODEL = \"/workspace/Github/HPA-Cell-Segmentation/dpn_unet_cell_3ch_v1.pth\"\n\n# image_size = 2048\nimage_size = 768\ncrop_size = 512\n\nbatch_size = 8 if kernel_mode else 6\n# batch_size = 4\nnum_workers = 2 if kernel_mode else 3\n\nconfidence_threshold = 0.5","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"old_classes = {\n    0: 'Nucleoplasm',\n    1: 'Nuclear membrane',\n    2: 'Nucleoli',\n    3: 'Nucleoli fibrillar center',\n    4: 'Nuclear speckles',\n    5: 'Nuclear bodies',\n    6: 'Endoplasmic reticulum',\n    7: 'Golgi apparatus',\n    8: 'Peroxisomes',\n    9: 'Endosomes',\n    10: 'Lysosomes',\n    11: 'Intermediate filaments',\n    12: 'Actin filaments',\n    13: 'Focal adhesion sites',\n    14: 'Microtubules',\n    15: 'Microtubule ends',\n    16: 'Cytokinetic bridge',\n    17: 'Mitotic spindle',\n    18: 'Microtubule organizing center',\n    19: 'Centrosome',\n    20: 'Lipid droplets',\n    21: 'Plasma membrane',\n    22: 'Cell junctions',\n    23: 'Mitochondria',\n    24: 'Aggresome',\n    25: 'Cytosol',\n    26: 'Cytoplasmic bodies',\n    27: 'Rods & rings'\n}\nold_class_indices = {v: k for k, v in old_classes.items()}\n\n# All label names in the public HPA and their corresponding index.\nall_locations = dict({\n    \"Nucleoplasm\": 0,\n    \"Nuclear membrane\": 1,\n    \"Nucleoli\": 2,\n    \"Nucleoli fibrillar center\": 3,\n    \"Nuclear speckles\": 4,\n    \"Nuclear bodies\": 5,\n    \"Endoplasmic reticulum\": 6,\n    \"Golgi apparatus\": 7,\n    \"Intermediate filaments\": 8,\n    \"Actin filaments\": 9,\n    \"Focal adhesion sites\": 9,\n    \"Microtubules\": 10,\n    \"Mitotic spindle\": 11,\n    \"Centrosome\": 12,\n    \"Centriolar satellite\": 12,\n    \"Plasma membrane\": 13,\n    \"Cell Junctions\": 13,\n    \"Mitochondria\": 14,\n    \"Aggresome\": 15,\n    \"Cytosol\": 16,\n    \"Vesicles\": 17,\n    \"Peroxisomes\": 17,\n    \"Endosomes\": 17,\n    \"Lysosomes\": 17,\n    \"Lipid droplets\": 17,\n    \"Cytoplasmic bodies\": 17,\n    \"Rods & rings\": 18,\n    # markpeng\n    \"No staining\": 18,\n})\n\nold_class_mappings = {}\nfor i, (k, v) in enumerate(old_class_indices.items()):\n    if k in all_locations:\n        old_class_mappings[v] = all_locations[k]\n    else:\n        # No staining\n        old_class_mappings[v] = 18\nassert len(old_class_mappings.values()) == len(old_classes.values())\nprint(old_class_mappings)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 17, 9: 17, 10: 17, 11: 8, 12: 9, 13: 9, 14: 10, 15: 18, 16: 18, 17: 11, 18: 18, 19: 12, 20: 17, 21: 13, 22: 18, 23: 14, 24: 15, 25: 16, 26: 17, 27: 18}\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls {dataset_folder}","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"sample_submission.csv  test  test_tfrecords  train  train.csv  train_tfrecords\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv(f\"{dataset_folder}/train.csv\")\nsubmit_df = pd.read_csv(f\"{dataset_folder}/sample_submission.csv\")","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(21806, 2)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                     ID  Label\n0  5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0  8|5|0\n1  5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0   14|0\n2  60b57878-bb99-11e8-b2b9-ac1f6b6435d0    6|1\n3  5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0  16|10\n4  5b931256-bb99-11e8-b2b9-ac1f6b6435d0   14|0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5c27f04c-bb99-11e8-b2b9-ac1f6b6435d0</td>\n      <td>8|5|0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5fb643ee-bb99-11e8-b2b9-ac1f6b6435d0</td>\n      <td>14|0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60b57878-bb99-11e8-b2b9-ac1f6b6435d0</td>\n      <td>6|1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5c1a898e-bb99-11e8-b2b9-ac1f6b6435d0</td>\n      <td>16|10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5b931256-bb99-11e8-b2b9-ac1f6b6435d0</td>\n      <td>14|0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(submit_df.shape, submit_df.ImageWidth.min(), submit_df.ImageWidth.max())\nsubmit_df.head()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(559, 4) 1728 3072\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                     ID  ImageWidth  ImageHeight  \\\n0  0040581b-f1f2-4fbe-b043-b6bfea5404bb        2048         2048   \n1  004a270d-34a2-4d60-bbe4-365fca868193        2048         2048   \n2  00537262-883c-4b37-a3a1-a4931b6faea5        2048         2048   \n3  00c9a1c9-2f06-476f-8b0d-6d01032874a2        2048         2048   \n4  0173029a-161d-40ef-af28-2342915b22fb        3072         3072   \n\n           PredictionString  \n0  0 1 eNoLCAgIMAEABJkBdQ==  \n1  0 1 eNoLCAgIMAEABJkBdQ==  \n2  0 1 eNoLCAgIMAEABJkBdQ==  \n3  0 1 eNoLCAgIMAEABJkBdQ==  \n4  0 1 eNoLCAgIsAQABJ4Beg==  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>ImageWidth</th>\n      <th>ImageHeight</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0040581b-f1f2-4fbe-b043-b6bfea5404bb</td>\n      <td>2048</td>\n      <td>2048</td>\n      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>004a270d-34a2-4d60-bbe4-365fca868193</td>\n      <td>2048</td>\n      <td>2048</td>\n      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00537262-883c-4b37-a3a1-a4931b6faea5</td>\n      <td>2048</td>\n      <td>2048</td>\n      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00c9a1c9-2f06-476f-8b0d-6d01032874a2</td>\n      <td>2048</td>\n      <td>2048</td>\n      <td>0 1 eNoLCAgIMAEABJkBdQ==</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0173029a-161d-40ef-af28-2342915b22fb</td>\n      <td>3072</td>\n      <td>3072</td>\n      <td>0 1 eNoLCAgIsAQABJ4Beg==</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"colors = [\"red\", \"green\", \"blue\", \"yellow\"]\n\n# Extract unique image IDs\n# test_ids = [\n#     f.split(\"/\")[-1].replace(\"_red.png\", \"\")\n#     for f in glob.glob(f\"{test_image_folder}/*_red.png\")\n# ]\n# print(len(test_ids))\ntest_ids = submit_df[\"ID\"].values.tolist()\nprint(len(test_ids))\n\n# Estimated number of private test images (RGBY): 2236 x 2.3 ~= 5143 (for 9 hours we have 6.2 secs per image)\n# Estimated number of private test images: 559 x 2.3 ~= 1286 (for 9 hours we have 25.2 secs per image)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"559\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Utility Functions","metadata":{"heading_collapsed":true}},{"cell_type":"code","source":"# Reference: https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook\ndef binary_mask_to_ascii(mask, mask_val=1):\n    \"\"\"Converts a binary mask into OID challenge encoding ascii text.\"\"\"\n    mask = np.where(mask == mask_val, 1, 0).astype(np.bool)\n\n    # check input mask --\n    if mask.dtype != np.bool:\n        raise ValueError(\n            f\"encode_binary_mask expects a binary mask, received dtype == {mask.dtype}\"\n        )\n\n    mask = np.squeeze(mask)\n    if len(mask.shape) != 2:\n        raise ValueError(\n            f\"encode_binary_mask expects a 2d mask, received shape == {mask.shape}\"\n        )\n\n    # convert input mask to expected COCO API input --\n    mask_to_encode = mask.reshape(mask.shape[0], mask.shape[1], 1)\n    mask_to_encode = mask_to_encode.astype(np.uint8)\n    mask_to_encode = np.asfortranarray(mask_to_encode)\n\n    # RLE encode mask --\n    encoded_mask = coco_mask.encode(mask_to_encode)[0][\"counts\"]\n\n    # compress and base64 encoding --\n    binary_str = zlib.compress(encoded_mask, zlib.Z_BEST_COMPRESSION)\n    base64_str = base64.b64encode(binary_str)\n    return base64_str.decode()\n\n\ndef rle_encoding(img, mask_val=1):\n    \"\"\"\n    Turns our masks into RLE encoding to easily store them\n    and feed them into models later on\n    https://en.wikipedia.org/wiki/Run-length_encoding\n    \n    Args:\n        img (np.array): Segmentation array\n        mask_val (int): Which value to use to create the RLE\n        \n    Returns:\n        RLE string\n    \n    \"\"\"\n    dots = np.where(img.T.flatten() == mask_val)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev + 1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n\n    return ' '.join([str(x) for x in run_lengths])\n\n\ndef rle_to_mask(rle_string, height, width):\n    \"\"\" Convert RLE sttring into a binary mask \n    \n    Args:\n        rle_string (rle_string): Run length encoding containing \n            segmentation mask information\n        height (int): Height of the original image the map comes from\n        width (int): Width of the original image the map comes from\n    \n    Returns:\n        Numpy array of the binary segmentation mask for a given cell\n    \"\"\"\n    rows, cols = height, width\n    rle_numbers = [int(num_string) for num_string in rle_string.split(' ')]\n    rle_pairs = np.array(rle_numbers).reshape(-1, 2)\n    img = np.zeros(rows * cols, dtype=np.uint8)\n    for index, length in rle_pairs:\n        index -= 1\n        img[index:index + length] = 255\n    img = img.reshape(cols, rows)\n    img = img.T\n    return img\n\n\ndef create_segmentation_maps(list_of_image_lists, segmentator, batch_size=8):\n    \"\"\" Function to generate segmentation maps using CellSegmentator tool \n    \n    Args:\n        list_of_image_lists (list of lists):\n            - [[micro-tubules(red)], [endoplasmic-reticulum(yellow)], [nucleus(blue)]]\n        batch_size (int): Batch size to use in generating the segmentation masks\n        \n    Returns:\n        List of lists containing RLEs for all the cells in all images\n    \"\"\"\n\n    all_mask_rles = {}\n    for i in tqdm(range(0, len(list_of_image_lists[0]), batch_size),\n                  total=len(list_of_image_lists[0]) // batch_size):\n\n        # Get batch of images\n        sub_images = [\n            img_channel_list[i:i + batch_size]\n            for img_channel_list in list_of_image_lists\n        ]  # 0.000001 seconds\n\n        # Do segmentation\n        cell_segmentations = segmentator.pred_cells(sub_images)\n        nuc_segmentations = segmentator.pred_nuclei(sub_images[2])\n\n        # post-processing\n        for j, path in enumerate(sub_images[0]):\n            img_id = path.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1]\n            nuc_mask, cell_mask = label_cell(nuc_segmentations[j],\n                                             cell_segmentations[j])\n            new_name = os.path.basename(path).replace('red', 'mask')\n            all_mask_rles[img_id] = [\n                rle_encoding(cell_mask, mask_val=k)\n                for k in range(1,\n                               np.max(cell_mask) + 1)\n            ]\n    return all_mask_rles\n\n\ndef get_img_list(img_dir, return_ids=False, sub_n=None):\n    \"\"\" Get image list in the format expected by the CellSegmentator tool \"\"\"\n    if sub_n is None:\n        sub_n = len(glob(img_dir + '/' + f'*_red.png'))\n    if return_ids:\n        images = [\n            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n            for c in [\"red\", \"yellow\", \"blue\"]\n        ]\n        return [\n            x.replace(\"_red.png\", \"\").rsplit(\"/\", 1)[1] for x in images[0]\n        ], images\n    else:\n        return [\n            sorted(glob(img_dir + '/' + f'*_{c}.png'))[:sub_n]\n            for c in [\"red\", \"yellow\", \"blue\"]\n        ]\n\n\ndef get_contour_bbox_from_rle(\n    rle,\n    width,\n    height,\n    return_mask=True,\n):\n    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n    \n    Args:\n        rle (rle_string): Run length encoding containing \n            segmentation mask information\n        height (int): Height of the original image the map comes from\n        width (int): Width of the original image the map comes from\n    \n    Returns:\n        Numpy array for a cell bounding box coordinates\n    \"\"\"\n    mask = rle_to_mask(rle, height, width).copy()\n    cnts = grab_contours(\n        cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n    x, y, w, h = cv2.boundingRect(cnts[0])\n\n    if return_mask:\n        return (x, y, x + w, y + h), mask\n    else:\n        return (x, y, x + w, y + h)\n\n\ndef get_contour_bbox_from_raw(raw_mask):\n    \"\"\" Get bbox of contour as `xmin ymin xmax ymax`\n    \n    Args:\n        raw_mask (nparray): Numpy array containing segmentation mask information\n    \n    Returns:\n        Numpy array for a cell bounding box coordinates\n    \"\"\"\n    cnts = grab_contours(\n        cv2.findContours(raw_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE))\n    xywhs = [cv2.boundingRect(cnt) for cnt in cnts]\n    xys = [(xywh[0], xywh[1], xywh[0] + xywh[2], xywh[1] + xywh[3])\n           for xywh in xywhs]\n    return sorted(xys, key=lambda x: (x[1], x[0]))\n\n\ndef pad_to_square(a):\n    \"\"\" Pad an array `a` evenly until it is a square \"\"\"\n    if a.shape[1] > a.shape[0]:  # pad height\n        n_to_add = a.shape[1] - a.shape[0]\n        top_pad = n_to_add // 2\n        bottom_pad = n_to_add - top_pad\n        a = np.pad(a, [(top_pad, bottom_pad), (0, 0), (0, 0)], mode='constant')\n\n    elif a.shape[0] > a.shape[1]:  # pad width\n        n_to_add = a.shape[0] - a.shape[1]\n        left_pad = n_to_add // 2\n        right_pad = n_to_add - left_pad\n        a = np.pad(a, [(0, 0), (left_pad, right_pad), (0, 0)], mode='constant')\n    else:\n        pass\n    return a\n\n\ndef cut_out_cells(rgby,\n                  rles,\n                  resize_to=(256, 256),\n                  square_off=True,\n                  return_masks=False,\n                  from_raw=True):\n    \"\"\" Cut out the cells as padded square images \n    \n    Args:\n        rgby (np.array): 4 Channel image to be cut into tiles\n        rles (list of RLE strings): List of run length encoding containing \n            segmentation mask information\n        resize_to (tuple of ints, optional): The square dimension to resize the image to\n        square_off (bool, optional): Whether to pad the image to a square or not\n        \n    Returns:\n        list of square arrays representing squared off cell images\n    \"\"\"\n    w, h = rgby.shape[:2]\n    contour_bboxes = [\n        get_contour_bbox(rle, w, h, return_mask=return_masks) for rle in rles\n    ]\n    if return_masks:\n        masks = [x[-1] for x in contour_bboxes]\n        contour_bboxes = [x[:-1] for x in contour_bboxes]\n\n    arrs = [\n        rgby[bbox[1]:bbox[3], bbox[0]:bbox[2], ...] for bbox in contour_bboxes\n    ]\n    if square_off:\n        arrs = [pad_to_square(arr) for arr in arrs]\n\n    if resize_to is not None:\n        arrs = [\n            cv2.resize(pad_to_square(arr).astype(np.float32),\n                       resize_to,\n                       interpolation=cv2.INTER_CUBIC) \\\n            for arr in arrs\n        ]\n    if return_masks:\n        return arrs, masks\n    else:\n        return arrs\n\n\ndef grab_contours(cnts):\n    # if the length the contours tuple returned by cv2.findContours\n    # is '2' then we are using either OpenCV v2.4, v4-beta, or\n    # v4-official\n    if len(cnts) == 2:\n        cnts = cnts[0]\n\n    # if the length of the contours tuple is '3' then we are using\n    # either OpenCV v3, v4-pre, or v4-alpha\n    elif len(cnts) == 3:\n        cnts = cnts[1]\n\n    # otherwise OpenCV has changed their cv2.findContours return\n    # signature yet again and I have no idea WTH is going on\n    else:\n        raise Exception(\n            (\"Contours tuple must have length 2 or 3, \"\n             \"otherwise OpenCV changed their cv2.findContours return \"\n             \"signature yet again. Refer to OpenCV's documentation \"\n             \"in that case\"))\n\n    # return the actual contours array\n    return cnts","metadata":{"hidden":true,"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/72534\ndef generate_hash(img_dir,\n                  colors,\n                  dataset='train',\n                  imread_func=None,\n                  is_update=False):\n    meta = meta.copy()\n    hash_maps = {}\n    for color in colors:\n        hash_maps[color] = []\n        for idx in tqdm(range(len(meta)), desc='train %s' % color):\n            img = imread_func(img_dir, meta.iloc[idx][ID], color)\n            hash = imagehash.phash(img)\n            hash_maps[color].append(hash)\n\n    for color in colors:\n        meta[color] = hash_maps[color]\n\n    return meta\n\n\ndef calc_hash(params):\n    color, threshold, base_test_hash1, base_test_hash2, test_ids1, test_ids2 = params\n\n    test_hash1 = base_test_hash1.reshape(1, -1)  # 1*m\n\n    test_idxes_list1 = []\n    test_idxes_list2 = []\n    hash_list = []\n\n    step = 5\n    for test_idx in tqdm(range(0, len(base_test_hash2), step), desc=color):\n        test_hash2 = base_test_hash2[test_idx:test_idx + step].reshape(\n            -1, 1)  # n*1\n        hash = test_hash2 - test_hash1  # n*m\n        test_idxes2, test_idxes1 = np.where(hash <= threshold)\n        hash = hash[test_idxes2, test_idxes1]\n\n        test_idxes2 = test_idxes2 + test_idx\n\n        test_idxes_list1.extend(test_idxes1.tolist())\n        test_idxes_list2.extend(test_idxes2.tolist())\n        hash_list.extend(hash.tolist())\n\n    df = pd.DataFrame({\n        'Test1': test_ids1[test_idxes_list1],\n        'Test2': test_ids2[test_idxes_list2],\n        'Sim%s' % color[:1].upper(): hash_list\n    })\n    df = df[df['Test1'] != df['Test2']]\n    return df","metadata":{"hidden":true,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"heading_collapsed":true}},{"cell_type":"code","source":"","metadata":{"hidden":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculate Mean and Std From Training and Test Sets","metadata":{"hidden":true}},{"cell_type":"code","source":"# !python {bestfitting_folder}/src/data_process/s8_generate_images_mean_std.py \\\n#     --source /workspace/Kaggle/HPA/hpa_2020","metadata":{"hidden":true,"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"total_mean = [0.081018, 0.052349, 0.054012, 0.08106] # rgby\ntotal_std = [0.133235, 0.08948, 0.143813, 0.130265]","metadata":{"hidden":true,"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Resize and Save All Images","metadata":{"heading_collapsed":true,"hidden":true}},{"cell_type":"code","source":"script_path = \"../input/hpa-bestfitting-solution/src/data_process\" if kernel_mode else \\\n    \"/workspace/Github/HPA-competition-solutions/bestfitting/src\"","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# !ls -la ../input/hpa-bestfitting-solution/src/data_process","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!cd {script_path} &&\\\n    python s2_resize_png_image.py \\\n    --source {dataset_folder} \\\n    --dest /kaggle/working \\\n    --dataset test \\\n    --size {image_size}","metadata":{"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"run on ff8ce0ffaf4e\ns2_resize_png_image.py: calling main function ... \n[mlcrate] 3 CPUs - resize test image:  39%|▊ | 882/2236 [01:47<03:20,  6.74it/s]","output_type":"stream"}]},{"cell_type":"code","source":"# !python {bestfitting_folder}/src/data_process/s2_resize_png_image.py \\\n#     --source {dataset_folder} \\\n#     --dest {dataset_folder} \\\n#     --dataset test \\\n#     --size {image_size}","metadata":{"hidden":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset","metadata":{"heading_collapsed":true}},{"cell_type":"code","source":"# Based on https://github.com/CellProfiling/HPA-competition-solutions/tree/master/bestfitting\nclass SegmentedProteinDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        img_dir=None,\n        img_size=512,\n        transform=None,\n        return_label=True,\n        in_channels=4,\n        crop_size=0,\n        random_crop=False,\n    ):\n        self.img_size = img_size\n        self.return_label = return_label\n        self.in_channels = in_channels\n        self.transform = transform\n        self.crop_size = crop_size\n        self.random_crop = random_crop\n\n        self.segmentator = cellsegmentator.CellSegmentator(\n            NUC_MODEL,\n            CELL_MODEL,\n            scale_factor=0.25,\n            device=\"cuda\",\n            padding=True,\n            multi_channel_model=True,\n        )\n\n        self.img_dir = img_dir\n        self.img_ids = test_ids\n        self.num = len(self.img_ids)\n\n    def read_crop_img(self, img):\n        random_crop_size = int(np.random.uniform(self.crop_size,\n                                                 self.img_size))\n        x = int(np.random.uniform(0, self.img_size - random_crop_size))\n        y = int(np.random.uniform(0, self.img_size - random_crop_size))\n        crop_img = img[x:x + random_crop_size, y:y + random_crop_size]\n        return crop_img\n\n    def read_rgby(self, img_dir, img_id, index):\n        suffix = '.png'\n        colors = ['red', 'green', 'blue', 'yellow']\n\n        flags = cv2.IMREAD_GRAYSCALE\n        rgby_img = [\n            cv2.imread(opj(img_dir, img_id + '_' + color + suffix), flags)\n            for color in colors\n        ]\n        rgby_img = np.stack(rgby_img, axis=-1)\n        if self.random_crop and self.crop_size > 0:\n            rgby_img = self.read_crop_img(rgby_img)\n\n        return rgby_img\n\n    def release_gpu(self):\n        # Not work\n        del self.segmentator\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    def __getitem__(self, index):\n        img_id = self.img_ids[index]\n        rgby_img = self.read_rgby(self.img_dir, img_id, index)\n        if rgby_img[0] is None:\n            print(self.img_dir, img_id)\n\n        # print(rgby_img.shape)\n\n        h, w = rgby_img.shape[:2]\n        if self.crop_size > 0:\n            if self.crop_size != h or self.crop_size != w:\n                rgby_img = cv2.resize(rgby_img,\n                                      (self.crop_size, self.crop_size),\n                                      interpolation=cv2.INTER_LINEAR)\n        else:\n            if self.img_size != h or self.img_size != w:\n                rgby_img = cv2.resize(rgby_img, (self.img_size, self.img_size),\n                                      interpolation=cv2.INTER_LINEAR)\n\n        if self.transform is not None:\n            rgby_img = self.transform(rgby_img)\n        # rgby_img = rgby_img / 255.0\n\n        # For nuclei\n        nuc_segmentations = self.segmentator.pred_nuclei([rgby_img[:, :, 2]])\n\n        # For full cells\n        cell_segmentations = self.segmentator.pred_cells([[rgby_img[:, :, i]]\n                                                          for i in [0, 3, 2]])\n\n        # Extract cell masks\n        _, full_mask = label_cell(nuc_segmentations[0], cell_segmentations[0])\n        # print(np.min(full_mask), np.max(full_mask))\n        cell_masks = [\n            rle_encoding(full_mask, mask_val=k)\n            for k in range(1,\n                           np.max(full_mask) + 1)\n        ]\n        if len(cell_masks) == 0:\n            print(f\"No cell masks found for {img_id}\")\n\n        rgby_img = rgby_img / 255.0\n        rgby_img = image_to_tensor(rgby_img)\n\n        if self.return_label:\n            label = self.labels[index]\n            return rgby_img, cell_masks, label, img_id\n        else:\n            return rgby_img, cell_masks, img_id\n\n    def __len__(self):\n        return self.num","metadata":{"hidden":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tmp_dataset = SegmentedProteinDataset(\n#     img_dir=\"/workspace/Kaggle/HPA/hpa_2020/inference/test/images_768\",\n#     img_size=768,\n#     return_label=False,\n#     in_channels=4,\n#     transform=None,\n#     crop_size=512,\n#     random_crop=False,\n# )\n# tmpiter = iter(tmp_dataset)\n# tmp_img, tmp_mask, tmp_index = next(tmpiter)\n# print(tmp_img.shape, len(tmp_mask), tmp_index)","metadata":{"hidden":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(1, len(tmp_mask), figsize=(10, 15))\n# for i in range(len(tmp_mask)):\n#     tmp = np.copy(rle_to_mask(tmp_mask[i], 512, 512))\n#     ax[i].imshow(tmp)\n# plt.show()","metadata":{"hidden":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"hidden":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Pretrained Model from Bestfitting","metadata":{}},{"cell_type":"code","source":"datasets_names = ['test', 'val']\nsplit_names = ['random_ext_folds5', 'random_ext_noleak_clean_folds5']\naugment_list = ['default', 'flipud', 'fliplr','transpose', 'flipud_lr',\n                'flipud_transpose', 'fliplr_transpose', 'flipud_lr_transpose']\n\nparser = argparse.ArgumentParser(description='PyTorch Protein Classification')\nparser.add_argument('--out_dir', type=str, help='destination where predicted result should be saved')\nparser.add_argument('--gpu_id', default='0', type=str, help='gpu id used for predicting (default: 0)')\nparser.add_argument('--arch', default='class_densenet121_dropout', type=str,\n                    help='model architecture (default: class_densenet121_dropout)')\nparser.add_argument('--num_classes', default=28, type=int, help='number of classes (default: 28)')\nparser.add_argument('--in_channels', default=4, type=int, help='in channels (default: 4)')\nparser.add_argument('--img_size', default=768, type=int, help='image size (default: 768)')\nparser.add_argument('--crop_size', default=512, type=int, help='crop size (default: 512)')\nparser.add_argument('--batch_size', default=32, type=int, help='train mini-batch size (default: 32)')\nparser.add_argument('--workers', default=3, type=int, help='number of data loading workers (default: 3)')\nparser.add_argument('--fold', default=0, type=int, help='index of fold (default: 0)')\nparser.add_argument('--augment', default='default', type=str, help='test augmentation (default: default)')\nparser.add_argument('--seed', default=100, type=int, help='random seed (default: 100)')\nparser.add_argument('--seeds', default=None, type=str, help='predict seed')\nparser.add_argument('--dataset', default='test', type=str, choices=datasets_names,\n                    help='dataset options: ' + ' | '.join(datasets_names) + ' (default: test)')\nparser.add_argument('--split_name', default='random_ext_folds5', type=str, choices=split_names,\n                    help='split name options: ' + ' | '.join(split_names) + ' (default: random_ext_folds5)')\nparser.add_argument('--predict_epoch', default=None, type=int, help='number epoch to predict')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = parser.parse_args([\n    \"--out_dir\", \"external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds\",\n    \"--gpu_id\", \"0\",\n    \"--arch\", \"class_densenet121_dropout\",\n    \"--img_size\", str(image_size),\n    \"--crop_size\", str(crop_size),\n    \"--seeds\", str(rand_seed),\n    \"--batch_size\", str(batch_size),\n    \"--fold\", \"0\",\n    \"--augment\", \"default,flipud,fliplr,transpose,flipud_lr,flipud_transpose,fliplr_transpose,flipud_lr_transpose\",\n    \"--dataset\", \"test\"\n])\nargs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# args.predict_epoch = 'final' if args.predict_epoch is None else '%03d' % args.predict_epoch\n# network_path = opj(RESULT_DIR, 'models', args.out_dir, 'fold%d' % args.fold,\n#                    '%s.pth' % args.predict_epoch)\nnetwork_path = f\"{bestfitting_folder}/external_crop512_focal_slov_hardlog_class_densenet121_dropout_i768_aug2_5folds/fold0/final.pth\"\n\n# setting up the visible GPU\nos.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n\nmodel_params = {}\nmodel_params['architecture'] = args.arch\nmodel_params['num_classes'] = args.num_classes\nmodel_params['in_channels'] = args.in_channels\nmodel_params['pretrained_path'] = f\"{bestfitting_folder}\"\nmodel = init_network(model_params)\n\n# log.write(\">> Loading network:\\n>>>> '{}'\\n\".format(network_path))\ncheckpoint = torch.load(network_path)\nmodel.load_state_dict(checkpoint['state_dict'])\n# log.write(\">>>> loaded network:\\n>>>> epoch {}\\n\".format(checkpoint['epoch']))\n\n# moving network to gpu and eval mode\n# model = DataParallel(model)\nmodel.cuda()\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"test_dataset = SegmentedProteinDataset(\n    # img_dir=\"/workspace/Kaggle/HPA/hpa_2020/test\",\n    img_dir= \"/kaggle/working/inference/test/images_768\" if kernel_mode \\\n        else \"/workspace/Kaggle/HPA/hpa_2020/inference/test/images_768\",\n    img_size=768,\n    return_label=False,\n    in_channels=4,\n    transform=None,\n    crop_size=512,\n    random_crop=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(x):\n    images, masks, indices = [], [], []\n    for single in x:\n        images.append(single[0].unsqueeze(0))\n        masks.append(single[1])\n        indices.append(single[2])\n    images = torch.cat(images)\n    return images, masks, indices\n\n\n# def collate_cells_fn(x):\n#     images, masks, img_ids = [], [], []\n#     # For each full image, extract cell images\n#     for single in x:\n#         for rle_string in single[1]:\n#             cell_mask = rle_to_mask(rle_string, crop_size, crop_size)\n#             # Important: set 255 to 1\n#             cell_mask[cell_mask > 0] = 1\n#             cell_image = single[0] * cell_mask\n#             # print(single[0][0, :], cell_mask[0, :], cell_image[0, :])\n#             images.append(cell_image.unsqueeze(0))\n#             masks.append(cell_mask)\n#             img_ids.append(single[2])\n#     images = torch.cat(images)\n#     return images, np.array(masks), np.array(img_ids)\n\n\ndef collate_cells_fn(x):\n    images, masks, img_id_meta = [], [], {}\n    # For each full image, extract cell images\n    offset = 0\n    for single in x:\n        image = single[0]\n        image_id = single[2]\n        \n        #         if image_id == \"0173029a-161d-40ef-af28-2342915b22fb\":\n        #             print(image_id, len(single[1]))\n        \n        for rle_string in single[1]:\n            cell_mask = rle_to_mask(rle_string, crop_size, crop_size)\n            # Important: set 255 to 1\n            cell_mask[cell_mask > 0] = 1\n            cell_image = image * cell_mask\n            # print(single[0][0, :], cell_mask[0, :], cell_image[0, :])\n            images.append(cell_image.unsqueeze(0))\n            masks.append(cell_mask)\n\n        img_id_meta[image_id] = (offset, offset + len(single[1]))\n        offset += len(single[1])\n\n    images = torch.cat(images)\n    return images, np.array(masks), img_id_meta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = DataLoader(\n    test_dataset,\n    sampler=SequentialSampler(test_dataset),\n    batch_size=batch_size,\n    drop_last=False,\n    num_workers=0,\n    pin_memory=True,\n    collate_fn=collate_cells_fn,\n    # collate_fn=collate_fn,\n    # collate_fn=lambda x: x,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_cell_img_meta = {}\nall_cell_masks = []\nall_probs = []\nprocessed_count = 0\nfor it, iter_data in tqdm(enumerate(test_loader, 0), total=len(test_loader)):\n    images, masks, img_id_meta = iter_data\n    for batch_i in range(0, len(masks), batch_size):\n        batch_images = images[batch_i:batch_i + batch_size, ...]\n        batch_masks = masks[batch_i:batch_i + batch_size]\n        # print(batch_images.size(), len(batch_masks))\n\n        batch_images = Variable(batch_images.cuda(), volatile=True)\n        outputs = model(batch_images)\n        logits = outputs\n\n        probs = F.sigmoid(logits).data\n        probs = probs.cpu().numpy().tolist()\n        all_probs += probs\n\n        processed_count += len(probs)\n\n    all_cell_img_meta.update(img_id_meta)\n    all_cell_masks.append(masks)\n\n    if debug:\n        break\n\nall_cell_masks = np.concatenate(all_cell_masks, axis=0)\nall_probs = np.array(all_probs).reshape(processed_count, -1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_cell_img_meta, all_cell_masks.shape, all_probs.shape\nlen(all_cell_img_meta), all_cell_masks.shape, all_probs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_predictions = []\nfor id in submit_df[\"ID\"].values:\n    if id in all_cell_img_meta:\n        (start, end) = all_cell_img_meta[id]\n        masks = all_cell_masks[start:end, ...]\n        preds = all_probs[start:end, ...]\n        # print(start, end, masks.shape, preds.shape)\n        if masks.shape[0] > 0:\n            new_preds = np.zeros((preds.shape[0], 19))\n            for i in range(preds.shape[0]):\n                for j in range(28):\n                    new_class_i = old_class_mappings[j]\n                    # Take maximum prob.\n                    if preds[i, j] > new_preds[i, new_class_i]:\n                        new_preds[i, new_class_i] = preds[i, j]\n            print(new_preds.shape)\n\n            # Generate RLE string for each cell mask\n            # https://www.kaggle.com/dschettler8845/hpa-cellwise-classification-inference/notebook?scriptVersionId=55714434\n            submit_strings = []\n            for i in range(masks.shape[0]):\n                preds = new_preds[i, ...]\n                mask = masks[i, ...]\n                rle_string = binary_mask_to_ascii(mask, mask_val=1)\n                # print(rle_string)\n\n                labels = sorted(\n                    np.nonzero(preds > confidence_threshold)[0].tolist())\n                # print(labels)\n                for l in labels:\n                    submit_strings.append(f\"{l} {preds[l]:.6f} {rle_string}\")\n\n            if len(submit_strings) > 0:\n                all_predictions.append(\" \".join(submit_strings))\n            else:\n                all_predictions.append(\"\")\n        else:\n            all_predictions.append(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(all_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if debug:\n    submit_df.iloc[:batch_size, :][\"PredictionString\"] = all_predictions\nelse:\n    submit_df[\"PredictionString\"] = all_predictions\nsubmit_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prob_to_result(probs, img_ids, th=0.5):\n    predicted_probs = probs.copy()\n    probs[np.arange(len(probs)), np.argmax(probs, axis=1)] = 1\n\n    pred_list = []\n    pred_list_new = []\n    for line in probs:\n        # Map old classes to new ones\n        predicted_old_classes = sorted(\n            list(set([i for i in np.nonzero(line > th)[0]])))\n        predicted_new_classes = sorted(\n            list(set([old_class_mappings[i]\n                      for i in np.nonzero(line > th)[0]])))\n        # print(predicted_classes)\n        s = '|'.join([str(i) for i in predicted_old_classes])\n        s_new = '|'.join([str(i) for i in predicted_new_classes])\n        pred_list.append(s)\n        pred_list_new.append(s_new)\n    result_df = pd.DataFrame({\n        # \"ID\": img_ids,\n        \"Predicted\": pred_list,\n        \"Predicted_New\": pred_list_new\n    })\n    return result_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = prob_to_result(all_probs, None, th=confidence_threshold)\nresult_df.to_csv(\"result_comparison.csv\", index=False)\nresult_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset.release_gpu()\n# del model, test_dataset, test_loader\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm densenet121-a639ec97.pth\n!rm -rf inference\n!ls -la","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### EOF","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}